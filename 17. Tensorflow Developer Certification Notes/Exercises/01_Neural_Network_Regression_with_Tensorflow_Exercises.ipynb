{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Neural Network Regression with TensorFlow Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create your own regression dataset (or make the one we created in \"Create data to view and fit\" bigger) and build fit a model to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "X = tf.cast(tf.constant(np.hstack([np.random.randn(2000, 7), np.random.randint(500, size = (2000, 7)), np.random.randint(2, size=(2000, 1)), np.random.randint(4, size=(2000,2))])), dtype=tf.float32)\n",
    "multiplicand = tf.random.uniform(shape = (17,1), minval=1, maxval=10, dtype=tf.float32)\n",
    "y = tf.matmul(X, multiplicand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"min_max_scaler\", MinMaxScaler(), slice(0,14)),\n",
    "        (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"ignore\"), slice(14,17))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X.numpy(), y.numpy(), test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_prepared = tf.convert_to_tensor(full_pipeline.fit_transform(X_train))\n",
    "X_test_prepared = tf.convert_to_tensor(full_pipeline.transform(X_test))\n",
    "\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "50/50 [==============================] - 1s 7ms/step - loss: 8645.0479 - mse: 79237824.0000\n",
      "Epoch 2/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8620.6592 - mse: 78814472.0000\n",
      "Epoch 3/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 8524.6533 - mse: 77140416.0000\n",
      "Epoch 4/100\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 8285.1729 - mse: 73064344.0000\n",
      "Epoch 5/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7821.2119 - mse: 65521684.0000\n",
      "Epoch 6/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7047.1470 - mse: 53813560.0000\n",
      "Epoch 7/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 5878.2720 - mse: 38588604.0000\n",
      "Epoch 8/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4241.4404 - mse: 21582772.0000\n",
      "Epoch 9/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2385.2200 - mse: 8100473.5000\n",
      "Epoch 10/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1422.1555 - mse: 3066157.0000\n",
      "Epoch 11/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1310.4652 - mse: 2601169.0000\n",
      "Epoch 12/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1299.0339 - mse: 2558499.2500\n",
      "Epoch 13/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1287.2190 - mse: 2513808.2500\n",
      "Epoch 14/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1274.9491 - mse: 2465186.5000\n",
      "Epoch 15/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1261.6587 - mse: 2415240.7500\n",
      "Epoch 16/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1249.7275 - mse: 2370949.5000\n",
      "Epoch 17/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1236.9541 - mse: 2324825.5000\n",
      "Epoch 18/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1222.9669 - mse: 2274085.5000\n",
      "Epoch 19/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1208.6154 - mse: 2218942.7500\n",
      "Epoch 20/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1194.7802 - mse: 2170988.2500\n",
      "Epoch 21/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1179.9948 - mse: 2119670.0000\n",
      "Epoch 22/100\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1165.1135 - mse: 2068000.5000\n",
      "Epoch 23/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1150.1204 - mse: 2015671.5000\n",
      "Epoch 24/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1135.3204 - mse: 1961977.6250\n",
      "Epoch 25/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1119.5620 - mse: 1910813.2500\n",
      "Epoch 26/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1103.5969 - mse: 1859849.6250\n",
      "Epoch 27/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1086.4468 - mse: 1802149.5000\n",
      "Epoch 28/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1070.5149 - mse: 1748041.0000\n",
      "Epoch 29/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1052.8943 - mse: 1693750.3750\n",
      "Epoch 30/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1034.7352 - mse: 1634366.1250\n",
      "Epoch 31/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 1017.2800 - mse: 1582847.2500\n",
      "Epoch 32/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 997.4168 - mse: 1522342.1250\n",
      "Epoch 33/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 978.4190 - mse: 1465642.5000\n",
      "Epoch 34/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 959.1204 - mse: 1408475.6250\n",
      "Epoch 35/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 938.7026 - mse: 1350041.6250\n",
      "Epoch 36/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 915.8552 - mse: 1286023.0000\n",
      "Epoch 37/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 895.3619 - mse: 1230822.0000\n",
      "Epoch 38/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 872.3477 - mse: 1168726.8750\n",
      "Epoch 39/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 848.8229 - mse: 1106455.0000\n",
      "Epoch 40/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 824.2569 - mse: 1045797.1250\n",
      "Epoch 41/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 799.2015 - mse: 983402.5625\n",
      "Epoch 42/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 773.0903 - mse: 918352.9375\n",
      "Epoch 43/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 744.5190 - mse: 854569.7500\n",
      "Epoch 44/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 715.2557 - mse: 789795.1875\n",
      "Epoch 45/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 688.1183 - mse: 729525.0625\n",
      "Epoch 46/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 655.0883 - mse: 662096.5000\n",
      "Epoch 47/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 622.0959 - mse: 598236.3750\n",
      "Epoch 48/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 589.6888 - mse: 537409.2500\n",
      "Epoch 49/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 554.0640 - mse: 473012.2812\n",
      "Epoch 50/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 516.0228 - mse: 411688.4688\n",
      "Epoch 51/100\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 478.0633 - mse: 355923.1250\n",
      "Epoch 52/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 437.7131 - mse: 298185.8438\n",
      "Epoch 53/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 397.3566 - mse: 244828.3438\n",
      "Epoch 54/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 353.1227 - mse: 194107.0781\n",
      "Epoch 55/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 307.7960 - mse: 147826.9844\n",
      "Epoch 56/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 262.0024 - mse: 107794.6875\n",
      "Epoch 57/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 216.6922 - mse: 73196.2266\n",
      "Epoch 58/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 172.4724 - mse: 47030.7695\n",
      "Epoch 59/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 132.8511 - mse: 27932.3379\n",
      "Epoch 60/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 103.2780 - mse: 17045.2207\n",
      "Epoch 61/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 86.4018 - mse: 11694.3789\n",
      "Epoch 62/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 74.2957 - mse: 8727.6338\n",
      "Epoch 63/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 66.6388 - mse: 7026.3901\n",
      "Epoch 64/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 61.6149 - mse: 6007.9761\n",
      "Epoch 65/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 56.1847 - mse: 5008.3735\n",
      "Epoch 66/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 51.7020 - mse: 4249.6299\n",
      "Epoch 67/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 47.2472 - mse: 3503.8069\n",
      "Epoch 68/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 42.3220 - mse: 2874.5544\n",
      "Epoch 69/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 37.7130 - mse: 2278.7451\n",
      "Epoch 70/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 34.0156 - mse: 1879.4200\n",
      "Epoch 71/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 29.5449 - mse: 1419.3047\n",
      "Epoch 72/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 25.3498 - mse: 1042.0046\n",
      "Epoch 73/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 21.4832 - mse: 752.4291\n",
      "Epoch 74/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 16.7418 - mse: 477.1265\n",
      "Epoch 75/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 14.0856 - mse: 339.7888\n",
      "Epoch 76/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 9.8179 - mse: 188.0596\n",
      "Epoch 77/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.6009 - mse: 102.8916\n",
      "Epoch 78/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.6650 - mse: 97.7203\n",
      "Epoch 79/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 7.1896 - mse: 100.7654\n",
      "Epoch 80/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.7176 - mse: 60.4941\n",
      "Epoch 81/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.2525 - mse: 57.7233\n",
      "Epoch 82/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.3037 - mse: 55.9666\n",
      "Epoch 83/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.9112 - mse: 38.7894\n",
      "Epoch 84/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.9855 - mse: 50.3712\n",
      "Epoch 85/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.8498 - mse: 36.9712\n",
      "Epoch 86/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.6331 - mse: 35.6402\n",
      "Epoch 87/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8394 - mse: 46.0519\n",
      "Epoch 88/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.5971 - mse: 33.8005\n",
      "Epoch 89/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.7337 - mse: 32.9081\n",
      "Epoch 90/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5240 - mse: 42.6594\n",
      "Epoch 91/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.7035 - mse: 33.0394\n",
      "Epoch 92/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.8985 - mse: 44.6471\n",
      "Epoch 93/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.5487 - mse: 38.3516\n",
      "Epoch 94/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.4526 - mse: 29.8775\n",
      "Epoch 95/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.4954 - mse: 36.3544\n",
      "Epoch 96/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2823 - mse: 35.6457\n",
      "Epoch 97/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 4.1096 - mse: 41.9433\n",
      "Epoch 98/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2632 - mse: 34.3610\n",
      "Epoch 99/100\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.1335 - mse: 25.8862\n",
      "Epoch 100/100\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 2.4418 - mse: 26.7050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c812299510>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create the model\n",
    "model_1 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (24,), name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(64, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = \"model_1\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_1.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    metrics = [\"mse\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model_1.fit(X_train_prepared, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 6ms/step - loss: 3.5971 - mse: 16.5424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.59709095954895, 16.542400360107422]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(X_test_prepared, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! This seems to be performing really well! I don't think we should be touching this in any case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Try building a neural network with 4 Dense layers and fitting it to your own regression dataset, how does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50/50 [==============================] - 1s 9ms/step - loss: 8647.2422 - mae: 8647.2422\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 8628.7607 - mae: 8628.7607\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8452.9258 - mae: 8452.9258\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7639.4673 - mae: 7639.4673\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5283.7642 - mae: 5283.7642\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1780.0177 - mae: 1780.0177\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1298.4364 - mae: 1298.4364\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1272.0494 - mae: 1272.0494\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1250.3136 - mae: 1250.3136\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 1226.5458 - mae: 1226.5458\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1198.8097 - mae: 1198.8097\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1171.3352 - mae: 1171.3352\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1143.3618 - mae: 1143.3618\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1117.8844 - mae: 1117.8844\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1086.9988 - mae: 1086.9988\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1056.7245 - mae: 1056.7245\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 1027.4054 - mae: 1027.4054\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 991.7911 - mae: 991.7911\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 957.8777 - mae: 957.8777\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 921.5208 - mae: 921.5208\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 880.5759 - mae: 880.5759\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 837.8514 - mae: 837.8514\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 792.6970 - mae: 792.6970\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 745.9966 - mae: 745.9966\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 691.5895 - mae: 691.5895\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 635.2333 - mae: 635.2333\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 571.4512 - mae: 571.4512\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 503.3712 - mae: 503.3712\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 425.0063 - mae: 425.0063\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 341.9664 - mae: 341.9664\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 252.6627 - mae: 252.6627\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 167.8781 - mae: 167.8781\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 107.8670 - mae: 107.8670\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 80.9262 - mae: 80.9262\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 70.2122 - mae: 70.2122\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 59.6304 - mae: 59.6304\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 51.7316 - mae: 51.7316\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 42.3980 - mae: 42.3980\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 36.0507 - mae: 36.0507\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 28.5126 - mae: 28.5126\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 23.9864 - mae: 23.9864\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 17.5201 - mae: 17.5201\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 12.0976 - mae: 12.0976\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 9.6613 - mae: 9.6613\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 12.6715 - mae: 12.6715\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.6103 - mae: 7.6103\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.4574 - mae: 8.4574\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 11.9996 - mae: 11.9996\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 6.8286 - mae: 6.8286\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.5603 - mae: 7.5603\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.4315 - mae: 6.4315\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.7474 - mae: 5.7474\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.1330 - mae: 6.1330\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.8652 - mae: 5.8652\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.3177 - mae: 4.3177\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.0637 - mae: 6.0637\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.3736 - mae: 3.3736\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.5986 - mae: 6.5986\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.4464 - mae: 5.4464\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 8.4618 - mae: 8.4618\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.3941 - mae: 6.3941\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.1270 - mae: 5.1270\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.2276 - mae: 5.2276\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.0121 - mae: 3.0121\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 6.3931 - mae: 6.3931\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6364 - mae: 4.6364\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.4549 - mae: 7.4549\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.0962 - mae: 3.0962\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.3200 - mae: 5.3200\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.2544 - mae: 6.2544\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 6.5754 - mae: 6.5754\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6662 - mae: 4.6662\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 11.1551 - mae: 11.1551\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.5511 - mae: 4.5511\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 7.2840 - mae: 7.2840\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.1630 - mae: 3.1630\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.2959 - mae: 6.2959\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.2158 - mae: 4.2158\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.3122 - mae: 5.3122\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.2778 - mae: 3.2778\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.5928 - mae: 2.5928\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.0914 - mae: 4.0914\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.4920 - mae: 3.4920\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6721 - mae: 4.6721\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.1360 - mae: 7.1360\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.0261 - mae: 5.0261\n",
      "Epoch 87/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.3719 - mae: 3.3719\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.2649 - mae: 3.2649\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.0345 - mae: 5.0345\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.3282 - mae: 4.3282\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.0961 - mae: 3.0961\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.9638 - mae: 7.9638\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.4264 - mae: 3.4264\n",
      "Epoch 94/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.9164 - mae: 2.9164\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.8835 - mae: 4.8835\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.2710 - mae: 7.2710\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.5909 - mae: 3.5909\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.6662 - mae: 2.6662\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.6596 - mae: 3.6596\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.3851 - mae: 5.3851\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.0810 - mae: 4.0810\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.3653 - mae: 4.3653\n",
      "Epoch 103/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.7482 - mae: 3.7482\n",
      "Epoch 104/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.9830 - mae: 5.9830\n",
      "Epoch 105/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.7223 - mae: 4.7223\n",
      "Epoch 106/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.3798 - mae: 4.3798\n",
      "Epoch 107/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.1143 - mae: 3.1143\n",
      "Epoch 108/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.5278 - mae: 4.5278\n",
      "Epoch 109/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.6806 - mae: 2.6806\n",
      "Epoch 110/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.2474 - mae: 4.2474\n",
      "Epoch 111/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.2712 - mae: 3.2712\n",
      "Epoch 112/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.1116 - mae: 6.1116\n",
      "Epoch 113/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.5114 - mae: 4.5114\n",
      "Epoch 114/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.8683 - mae: 3.8683\n",
      "Epoch 115/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.8180 - mae: 3.8180\n",
      "Epoch 116/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.0047 - mae: 4.0047\n",
      "Epoch 117/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.7060 - mae: 4.7060\n",
      "Epoch 118/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.8603 - mae: 4.8603\n",
      "Epoch 119/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.0368 - mae: 4.0368\n",
      "Epoch 120/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.0548 - mae: 4.0548\n",
      "Epoch 121/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.5878 - mae: 4.5878\n",
      "Epoch 122/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.9800 - mae: 3.9800\n",
      "Epoch 123/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.8094 - mae: 4.8094\n",
      "Epoch 124/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.4715 - mae: 5.4715\n",
      "Epoch 125/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.7251 - mae: 4.7251\n",
      "Epoch 126/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.0484 - mae: 5.0484\n",
      "Epoch 127/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6003 - mae: 4.6003\n",
      "Epoch 128/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.0330 - mae: 2.0330\n",
      "Epoch 129/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 7.5699 - mae: 7.5699\n",
      "Epoch 130/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.7619 - mae: 5.7619\n",
      "Epoch 131/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.2564 - mae: 5.2564\n",
      "Epoch 132/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.2819 - mae: 3.2819\n",
      "Epoch 133/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.6159 - mae: 3.6159\n",
      "Epoch 134/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.9933 - mae: 3.9933\n",
      "Epoch 135/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.1421 - mae: 4.1421\n",
      "Epoch 136/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.1054 - mae: 6.1054\n",
      "Epoch 137/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.1661 - mae: 3.1661\n",
      "Epoch 138/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.5020 - mae: 5.5020\n",
      "Epoch 139/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.0277 - mae: 4.0277\n",
      "Epoch 140/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.0784 - mae: 4.0784\n",
      "Epoch 141/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.1130 - mae: 5.1130\n",
      "Epoch 142/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.3070 - mae: 5.3070\n",
      "Epoch 143/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.2730 - mae: 3.2730\n",
      "Epoch 144/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.7874 - mae: 3.7874\n",
      "Epoch 145/200\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 3.7265 - mae: 3.7265\n",
      "Epoch 146/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.7566 - mae: 4.7566\n",
      "Epoch 147/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 2.8553 - mae: 2.8553\n",
      "Epoch 148/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.0469 - mae: 5.0469\n",
      "Epoch 149/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.5172 - mae: 5.5172\n",
      "Epoch 150/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.5650 - mae: 4.5650\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.7574 - mae: 3.7574\n",
      "Epoch 152/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.8259 - mae: 3.8259\n",
      "Epoch 153/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.6305 - mae: 4.6305\n",
      "Epoch 154/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.2390 - mae: 5.2390\n",
      "Epoch 155/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 6.2041 - mae: 6.2041\n",
      "Epoch 156/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.5111 - mae: 3.5111\n",
      "Epoch 157/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.9110 - mae: 3.9110\n",
      "Epoch 158/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 4.7295 - mae: 4.7295\n",
      "Epoch 159/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.9523 - mae: 3.9523\n",
      "Epoch 160/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 5.1852 - mae: 5.1852\n",
      "Epoch 161/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.2153 - mae: 3.2153\n",
      "Epoch 162/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.5310 - mae: 3.5310\n",
      "Epoch 163/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.1952 - mae: 3.1952\n",
      "Epoch 164/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.5356 - mae: 3.5356\n",
      "Epoch 165/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.4412 - mae: 4.4412\n",
      "Epoch 166/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.5523 - mae: 3.5523\n",
      "Epoch 167/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.8581 - mae: 3.8581\n",
      "Epoch 168/200\n",
      "50/50 [==============================] - 0s 5ms/step - loss: 3.1023 - mae: 3.1023\n",
      "Epoch 169/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.8635 - mae: 3.8635\n",
      "Epoch 170/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 6.9217 - mae: 6.9217\n",
      "Epoch 171/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 3.1838 - mae: 3.1838\n",
      "Epoch 172/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 5.1788 - mae: 5.1788\n",
      "Epoch 173/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 6.5169 - mae: 6.5169\n",
      "Epoch 174/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 4.5391 - mae: 4.5391\n",
      "Epoch 175/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 7.3430 - mae: 7.3430\n",
      "Epoch 176/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 4.2556 - mae: 4.2556\n",
      "Epoch 177/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.4157 - mae: 5.4157\n",
      "Epoch 178/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.6741 - mae: 4.6741\n",
      "Epoch 179/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.3118 - mae: 5.3118\n",
      "Epoch 180/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.1177 - mae: 4.1177\n",
      "Epoch 181/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 7.2852 - mae: 7.2852\n",
      "Epoch 182/200\n",
      "50/50 [==============================] - 0s 8ms/step - loss: 3.6574 - mae: 3.6574\n",
      "Epoch 183/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.2765 - mae: 5.2765\n",
      "Epoch 184/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.6293 - mae: 4.6293\n",
      "Epoch 185/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.0023 - mae: 5.0023\n",
      "Epoch 186/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.6582 - mae: 5.6582\n",
      "Epoch 187/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.5001 - mae: 5.5001\n",
      "Epoch 188/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.4455 - mae: 3.4455\n",
      "Epoch 189/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.3287 - mae: 3.3287\n",
      "Epoch 190/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.7816 - mae: 3.7816\n",
      "Epoch 191/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.0415 - mae: 5.0415\n",
      "Epoch 192/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 4.2459 - mae: 4.2459\n",
      "Epoch 193/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.4610 - mae: 5.4610\n",
      "Epoch 194/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 4.0271 - mae: 4.0271\n",
      "Epoch 195/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 5.4207 - mae: 5.4207\n",
      "Epoch 196/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.5945 - mae: 3.5945\n",
      "Epoch 197/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 5.3678 - mae: 5.3678\n",
      "Epoch 198/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.4227 - mae: 3.4227\n",
      "Epoch 199/200\n",
      "50/50 [==============================] - 0s 7ms/step - loss: 2.5476 - mae: 2.5476\n",
      "Epoch 200/200\n",
      "50/50 [==============================] - 0s 6ms/step - loss: 3.8007 - mae: 3.8007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c81366c6d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create the model\n",
    "model_final = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (24,), name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (24,), name = \"hidden_layer_2\"),\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (24,), name = \"hidden_layer_3\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = \"model_final\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_final.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model_final.fit(X_train_prepared, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 7ms/step - loss: 10.6923 - mae: 10.6923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[10.692336082458496, 10.692336082458496]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.evaluate(X_test_prepared, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the deeper model doesn't improve over the previous model. We were indeed right in guessing that the previous model was indeed optimal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Try and improve the results we got on the insurance dataset, some things you might want to try include:\n",
    "* Building a larger model (how does one with 4 dense layers go?).\n",
    "* Increasing the number of units in each layer.\n",
    "* Lookup the documentation of Adam and find out what the first parameter is, what happens if you increase it by 10x?\n",
    "* What happens if you train for longer (say 300 epochs instead of 200)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the insurance dataset\n",
    "insurance_df = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n",
    "insurance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = insurance_df.iloc[:, :-1], insurance_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"sex\", \"smoker\", \"region\"]\n",
    "numerical_columns = [\"age\", \"bmi\", \"children\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "numerical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"min_max_scaler\", MinMaxScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"one_hot_encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"number_transform\", numerical_pipeline, numerical_columns),\n",
    "        (\"categorical_transform\", categorical_pipeline, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)\n",
    "X_test_prepared = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "34/34 [==============================] - 1s 8ms/step - loss: 13342.8271 - mse: 322361792.0000\n",
      "Epoch 2/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 13296.2480 - mse: 321082976.0000\n",
      "Epoch 3/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 12943.9180 - mse: 311594080.0000\n",
      "Epoch 4/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 11450.5088 - mse: 273180704.0000\n",
      "Epoch 5/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 8880.4160 - mse: 198929488.0000\n",
      "Epoch 6/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 7854.2783 - mse: 151981056.0000\n",
      "Epoch 7/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 7641.4985 - mse: 141221440.0000\n",
      "Epoch 8/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 7407.1343 - mse: 133665664.0000\n",
      "Epoch 9/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 7152.8311 - mse: 125089392.0000\n",
      "Epoch 10/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 6853.1494 - mse: 114892600.0000\n",
      "Epoch 11/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 6495.8105 - mse: 104300376.0000\n",
      "Epoch 12/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 6042.4312 - mse: 93720704.0000\n",
      "Epoch 13/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 5503.4751 - mse: 79597368.0000\n",
      "Epoch 14/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4919.4927 - mse: 69628328.0000\n",
      "Epoch 15/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4477.0420 - mse: 61079608.0000\n",
      "Epoch 16/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 4138.2109 - mse: 55097736.0000\n",
      "Epoch 17/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3877.5483 - mse: 49765996.0000\n",
      "Epoch 18/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3655.0383 - mse: 45994396.0000\n",
      "Epoch 19/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3512.9465 - mse: 43366604.0000\n",
      "Epoch 20/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3445.6589 - mse: 42048656.0000\n",
      "Epoch 21/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3392.3279 - mse: 41508064.0000\n",
      "Epoch 22/300\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 3354.0898 - mse: 41269756.0000\n",
      "Epoch 23/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3317.0210 - mse: 41790252.0000\n",
      "Epoch 24/300\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 3280.5969 - mse: 42474568.0000\n",
      "Epoch 25/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3264.7563 - mse: 43225584.0000\n",
      "Epoch 26/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3242.5647 - mse: 44041428.0000\n",
      "Epoch 27/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3231.9202 - mse: 45603136.0000\n",
      "Epoch 28/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3211.6606 - mse: 46910824.0000\n",
      "Epoch 29/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3199.2556 - mse: 46725368.0000\n",
      "Epoch 30/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3194.3530 - mse: 47070284.0000\n",
      "Epoch 31/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3196.9541 - mse: 48056912.0000\n",
      "Epoch 32/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3196.9751 - mse: 47184044.0000\n",
      "Epoch 33/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3197.7522 - mse: 47309472.0000\n",
      "Epoch 34/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3198.2739 - mse: 46574936.0000\n",
      "Epoch 35/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3184.9497 - mse: 46814920.0000\n",
      "Epoch 36/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3177.9961 - mse: 46909244.0000\n",
      "Epoch 37/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3176.8425 - mse: 46853388.0000\n",
      "Epoch 38/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3174.3943 - mse: 46832200.0000\n",
      "Epoch 39/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3177.5435 - mse: 46998940.0000\n",
      "Epoch 40/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3173.0010 - mse: 46746004.0000\n",
      "Epoch 41/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3181.0081 - mse: 46757196.0000\n",
      "Epoch 42/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3170.6338 - mse: 46497940.0000\n",
      "Epoch 43/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3160.9072 - mse: 46823316.0000\n",
      "Epoch 44/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3159.2747 - mse: 46163016.0000\n",
      "Epoch 45/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3157.5073 - mse: 46109076.0000\n",
      "Epoch 46/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3157.1462 - mse: 46127036.0000\n",
      "Epoch 47/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3155.9343 - mse: 46770220.0000\n",
      "Epoch 48/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 3146.6001 - mse: 46006884.0000\n",
      "Epoch 49/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3148.0752 - mse: 45480672.0000\n",
      "Epoch 50/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3146.3235 - mse: 46359248.0000\n",
      "Epoch 51/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3146.7158 - mse: 45790556.0000\n",
      "Epoch 52/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3133.5378 - mse: 45688188.0000\n",
      "Epoch 53/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3130.9402 - mse: 45253716.0000\n",
      "Epoch 54/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3130.6201 - mse: 45735444.0000\n",
      "Epoch 55/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3143.8242 - mse: 44879496.0000\n",
      "Epoch 56/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3126.5305 - mse: 45158716.0000\n",
      "Epoch 57/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3115.8491 - mse: 45190676.0000\n",
      "Epoch 58/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3117.5928 - mse: 45092520.0000\n",
      "Epoch 59/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3113.2612 - mse: 45398144.0000\n",
      "Epoch 60/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3109.0637 - mse: 44638400.0000\n",
      "Epoch 61/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3115.7783 - mse: 44664432.0000\n",
      "Epoch 62/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3098.3447 - mse: 44266744.0000\n",
      "Epoch 63/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3093.1138 - mse: 44459316.0000\n",
      "Epoch 64/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3089.2639 - mse: 44217148.0000\n",
      "Epoch 65/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3082.9905 - mse: 44211228.0000\n",
      "Epoch 66/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3078.6379 - mse: 43709700.0000\n",
      "Epoch 67/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3069.3792 - mse: 44040504.0000\n",
      "Epoch 68/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3068.8276 - mse: 43745100.0000\n",
      "Epoch 69/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3059.7954 - mse: 43524300.0000\n",
      "Epoch 70/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3056.1458 - mse: 43463324.0000\n",
      "Epoch 71/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3052.8101 - mse: 43431460.0000\n",
      "Epoch 72/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3047.2449 - mse: 42661704.0000\n",
      "Epoch 73/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3040.9104 - mse: 43278160.0000\n",
      "Epoch 74/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3031.0791 - mse: 42580152.0000\n",
      "Epoch 75/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 3025.7852 - mse: 42467060.0000\n",
      "Epoch 76/300\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 3020.9539 - mse: 42009296.0000\n",
      "Epoch 77/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3018.8188 - mse: 42112536.0000\n",
      "Epoch 78/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3015.5459 - mse: 41420460.0000\n",
      "Epoch 79/300\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 3007.7700 - mse: 41358340.0000\n",
      "Epoch 80/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2998.5771 - mse: 41779252.0000\n",
      "Epoch 81/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 3007.5156 - mse: 41498704.0000\n",
      "Epoch 82/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2995.5466 - mse: 41742112.0000\n",
      "Epoch 83/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2979.8770 - mse: 40743600.0000\n",
      "Epoch 84/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2976.8330 - mse: 40958960.0000\n",
      "Epoch 85/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2967.1519 - mse: 40130888.0000\n",
      "Epoch 86/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2956.5952 - mse: 40437004.0000\n",
      "Epoch 87/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2950.8645 - mse: 40018052.0000\n",
      "Epoch 88/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2941.2104 - mse: 40000008.0000\n",
      "Epoch 89/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2937.4312 - mse: 39857608.0000\n",
      "Epoch 90/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2928.8064 - mse: 39611152.0000\n",
      "Epoch 91/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2922.8679 - mse: 39465580.0000\n",
      "Epoch 92/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2920.1069 - mse: 38942620.0000\n",
      "Epoch 93/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2916.3257 - mse: 39020332.0000\n",
      "Epoch 94/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2924.9583 - mse: 39387476.0000\n",
      "Epoch 95/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2897.8650 - mse: 38946468.0000\n",
      "Epoch 96/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2893.2637 - mse: 39007468.0000\n",
      "Epoch 97/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2884.7646 - mse: 38366248.0000\n",
      "Epoch 98/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2874.2500 - mse: 38127988.0000\n",
      "Epoch 99/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2870.4302 - mse: 38158572.0000\n",
      "Epoch 100/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2858.3167 - mse: 38168428.0000\n",
      "Epoch 101/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2852.3940 - mse: 37961612.0000\n",
      "Epoch 102/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2846.8752 - mse: 37617476.0000\n",
      "Epoch 103/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2836.4106 - mse: 37390100.0000\n",
      "Epoch 104/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2827.7239 - mse: 37282964.0000\n",
      "Epoch 105/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2822.7280 - mse: 37012588.0000\n",
      "Epoch 106/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2813.6799 - mse: 37140280.0000\n",
      "Epoch 107/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2803.3762 - mse: 36955236.0000\n",
      "Epoch 108/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2791.7295 - mse: 36622808.0000\n",
      "Epoch 109/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2783.9548 - mse: 36531304.0000\n",
      "Epoch 110/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2779.9114 - mse: 36650548.0000\n",
      "Epoch 111/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2775.1772 - mse: 36075932.0000\n",
      "Epoch 112/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2759.8586 - mse: 36304908.0000\n",
      "Epoch 113/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2774.0376 - mse: 35901332.0000\n",
      "Epoch 114/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2753.1628 - mse: 35867616.0000\n",
      "Epoch 115/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2741.0669 - mse: 35644584.0000\n",
      "Epoch 116/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2740.8860 - mse: 35593964.0000\n",
      "Epoch 117/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2719.7185 - mse: 35522860.0000\n",
      "Epoch 118/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2711.3069 - mse: 35399900.0000\n",
      "Epoch 119/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2694.2070 - mse: 35121476.0000\n",
      "Epoch 120/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2680.2148 - mse: 34955248.0000\n",
      "Epoch 121/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2670.6931 - mse: 34999800.0000\n",
      "Epoch 122/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2663.4253 - mse: 34648204.0000\n",
      "Epoch 123/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2655.6536 - mse: 34559844.0000\n",
      "Epoch 124/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2646.9202 - mse: 34439036.0000\n",
      "Epoch 125/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2628.3635 - mse: 34292108.0000\n",
      "Epoch 126/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2619.9102 - mse: 34159680.0000\n",
      "Epoch 127/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2602.7834 - mse: 34107876.0000\n",
      "Epoch 128/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2594.6199 - mse: 33860864.0000\n",
      "Epoch 129/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2583.6829 - mse: 33983192.0000\n",
      "Epoch 130/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2567.9053 - mse: 33730444.0000\n",
      "Epoch 131/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2554.8203 - mse: 33709192.0000\n",
      "Epoch 132/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2545.8555 - mse: 33566496.0000\n",
      "Epoch 133/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2533.4309 - mse: 33495818.0000\n",
      "Epoch 134/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2529.8533 - mse: 33401744.0000\n",
      "Epoch 135/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2506.5962 - mse: 33282610.0000\n",
      "Epoch 136/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2497.0640 - mse: 33232586.0000\n",
      "Epoch 137/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2487.5078 - mse: 33147998.0000\n",
      "Epoch 138/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2471.2974 - mse: 33113412.0000\n",
      "Epoch 139/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2459.6387 - mse: 33059494.0000\n",
      "Epoch 140/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2454.0337 - mse: 32941100.0000\n",
      "Epoch 141/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2453.5098 - mse: 33021206.0000\n",
      "Epoch 142/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2434.8340 - mse: 32819966.0000\n",
      "Epoch 143/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2427.7092 - mse: 32732774.0000\n",
      "Epoch 144/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2415.4766 - mse: 32577872.0000\n",
      "Epoch 145/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2404.2292 - mse: 32494662.0000\n",
      "Epoch 146/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2389.7610 - mse: 32371778.0000\n",
      "Epoch 147/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2385.1038 - mse: 32300910.0000\n",
      "Epoch 148/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2381.0039 - mse: 32235616.0000\n",
      "Epoch 149/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2364.9143 - mse: 32034510.0000\n",
      "Epoch 150/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2359.8696 - mse: 31979026.0000\n",
      "Epoch 151/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2354.3777 - mse: 31850992.0000\n",
      "Epoch 152/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2352.0352 - mse: 31652772.0000\n",
      "Epoch 153/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2345.7581 - mse: 31764494.0000\n",
      "Epoch 154/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2329.8223 - mse: 31569318.0000\n",
      "Epoch 155/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2326.3149 - mse: 31479732.0000\n",
      "Epoch 156/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2327.6562 - mse: 31306346.0000\n",
      "Epoch 157/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2312.0889 - mse: 31224634.0000\n",
      "Epoch 158/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2306.6172 - mse: 31043640.0000\n",
      "Epoch 159/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2295.4868 - mse: 30948710.0000\n",
      "Epoch 160/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2292.2598 - mse: 30869142.0000\n",
      "Epoch 161/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2285.8936 - mse: 30587854.0000\n",
      "Epoch 162/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2280.9873 - mse: 30572026.0000\n",
      "Epoch 163/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2269.0544 - mse: 30447664.0000\n",
      "Epoch 164/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2270.9126 - mse: 30269684.0000\n",
      "Epoch 165/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2270.7600 - mse: 30064640.0000\n",
      "Epoch 166/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2260.0759 - mse: 30072484.0000\n",
      "Epoch 167/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2260.9690 - mse: 29840852.0000\n",
      "Epoch 168/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2245.6821 - mse: 29779350.0000\n",
      "Epoch 169/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2241.1619 - mse: 29620004.0000\n",
      "Epoch 170/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2234.0959 - mse: 29462838.0000\n",
      "Epoch 171/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2231.2644 - mse: 29342558.0000\n",
      "Epoch 172/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2224.7422 - mse: 29079084.0000\n",
      "Epoch 173/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2220.2791 - mse: 29097562.0000\n",
      "Epoch 174/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2214.5681 - mse: 29021028.0000\n",
      "Epoch 175/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2212.3066 - mse: 28767302.0000\n",
      "Epoch 176/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2207.0906 - mse: 28670400.0000\n",
      "Epoch 177/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2199.2744 - mse: 28587272.0000\n",
      "Epoch 178/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2192.8394 - mse: 28565424.0000\n",
      "Epoch 179/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2190.5571 - mse: 28313356.0000\n",
      "Epoch 180/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2184.7568 - mse: 28189878.0000\n",
      "Epoch 181/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2172.4053 - mse: 28105918.0000\n",
      "Epoch 182/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2169.9578 - mse: 27883724.0000\n",
      "Epoch 183/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2160.8521 - mse: 27866812.0000\n",
      "Epoch 184/300\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 2162.5554 - mse: 27805190.0000\n",
      "Epoch 185/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2150.1348 - mse: 27548246.0000\n",
      "Epoch 186/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2148.7195 - mse: 27489186.0000\n",
      "Epoch 187/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2137.6970 - mse: 27307202.0000\n",
      "Epoch 188/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2136.7881 - mse: 27329076.0000\n",
      "Epoch 189/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2133.6819 - mse: 27272086.0000\n",
      "Epoch 190/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2123.4651 - mse: 27103082.0000\n",
      "Epoch 191/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2115.6917 - mse: 26939706.0000\n",
      "Epoch 192/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2114.5879 - mse: 26825756.0000\n",
      "Epoch 193/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2110.3254 - mse: 26753666.0000\n",
      "Epoch 194/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2103.7212 - mse: 26730686.0000\n",
      "Epoch 195/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2094.6431 - mse: 26611252.0000\n",
      "Epoch 196/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2090.7446 - mse: 26615266.0000\n",
      "Epoch 197/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2084.1633 - mse: 26472272.0000\n",
      "Epoch 198/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2088.6140 - mse: 26455920.0000\n",
      "Epoch 199/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2082.2222 - mse: 26323314.0000\n",
      "Epoch 200/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2072.3928 - mse: 26191306.0000\n",
      "Epoch 201/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2075.1724 - mse: 26297492.0000\n",
      "Epoch 202/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2077.0457 - mse: 26182456.0000\n",
      "Epoch 203/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2067.8525 - mse: 26172436.0000\n",
      "Epoch 204/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2075.6145 - mse: 26113532.0000\n",
      "Epoch 205/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2065.1995 - mse: 25889840.0000\n",
      "Epoch 206/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2054.1145 - mse: 25958374.0000\n",
      "Epoch 207/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2054.5850 - mse: 25869564.0000\n",
      "Epoch 208/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2048.2310 - mse: 25926088.0000\n",
      "Epoch 209/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2043.7252 - mse: 25832604.0000\n",
      "Epoch 210/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2034.5325 - mse: 25718996.0000\n",
      "Epoch 211/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2035.8906 - mse: 25692944.0000\n",
      "Epoch 212/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2029.9575 - mse: 25652926.0000\n",
      "Epoch 213/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2028.7250 - mse: 25671780.0000\n",
      "Epoch 214/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2033.4918 - mse: 25629022.0000\n",
      "Epoch 215/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2029.0668 - mse: 25648382.0000\n",
      "Epoch 216/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2021.2563 - mse: 25563112.0000\n",
      "Epoch 217/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2017.8876 - mse: 25560862.0000\n",
      "Epoch 218/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2019.4603 - mse: 25541428.0000\n",
      "Epoch 219/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2023.2869 - mse: 25536538.0000\n",
      "Epoch 220/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2019.2806 - mse: 25527204.0000\n",
      "Epoch 221/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2013.8729 - mse: 25497532.0000\n",
      "Epoch 222/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2010.9568 - mse: 25578832.0000\n",
      "Epoch 223/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2009.0225 - mse: 25602946.0000\n",
      "Epoch 224/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2006.7727 - mse: 25573650.0000\n",
      "Epoch 225/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2005.4519 - mse: 25486406.0000\n",
      "Epoch 226/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2003.6722 - mse: 25500834.0000\n",
      "Epoch 227/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2008.7061 - mse: 25509748.0000\n",
      "Epoch 228/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1997.8965 - mse: 25391126.0000\n",
      "Epoch 229/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2003.8252 - mse: 25400636.0000\n",
      "Epoch 230/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2004.3762 - mse: 25465120.0000\n",
      "Epoch 231/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2002.2979 - mse: 25428216.0000\n",
      "Epoch 232/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 2002.1292 - mse: 25611734.0000\n",
      "Epoch 233/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2003.2811 - mse: 25361364.0000\n",
      "Epoch 234/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2000.4180 - mse: 25469930.0000\n",
      "Epoch 235/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1997.7953 - mse: 25387918.0000\n",
      "Epoch 236/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1995.7979 - mse: 25422854.0000\n",
      "Epoch 237/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 2001.1155 - mse: 25480256.0000\n",
      "Epoch 238/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2001.8893 - mse: 25525564.0000\n",
      "Epoch 239/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1994.5535 - mse: 25379138.0000\n",
      "Epoch 240/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 2000.0468 - mse: 25444636.0000\n",
      "Epoch 241/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1996.0215 - mse: 25427348.0000\n",
      "Epoch 242/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1990.2622 - mse: 25345116.0000\n",
      "Epoch 243/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1993.0582 - mse: 25429026.0000\n",
      "Epoch 244/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1987.6549 - mse: 25386458.0000\n",
      "Epoch 245/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1988.3091 - mse: 25523936.0000\n",
      "Epoch 246/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1989.1071 - mse: 25419134.0000\n",
      "Epoch 247/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1996.1348 - mse: 25386492.0000\n",
      "Epoch 248/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1987.0575 - mse: 25408632.0000\n",
      "Epoch 249/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1989.0442 - mse: 25371938.0000\n",
      "Epoch 250/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1989.0529 - mse: 25453118.0000\n",
      "Epoch 251/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1989.9456 - mse: 25440250.0000\n",
      "Epoch 252/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1987.5054 - mse: 25473082.0000\n",
      "Epoch 253/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1991.0320 - mse: 25597880.0000\n",
      "Epoch 254/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1979.3722 - mse: 25367602.0000\n",
      "Epoch 255/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1983.2671 - mse: 25345536.0000\n",
      "Epoch 256/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1987.5649 - mse: 25435192.0000\n",
      "Epoch 257/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1980.9136 - mse: 25356382.0000\n",
      "Epoch 258/300\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1978.3203 - mse: 25380138.0000\n",
      "Epoch 259/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1978.6458 - mse: 25430632.0000\n",
      "Epoch 260/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1980.9500 - mse: 25532372.0000\n",
      "Epoch 261/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1977.4005 - mse: 25355888.0000\n",
      "Epoch 262/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1979.6761 - mse: 25403672.0000\n",
      "Epoch 263/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1981.3014 - mse: 25425414.0000\n",
      "Epoch 264/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1975.0540 - mse: 25372266.0000\n",
      "Epoch 265/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1973.7551 - mse: 25391418.0000\n",
      "Epoch 266/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1979.7150 - mse: 25332590.0000\n",
      "Epoch 267/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1982.0687 - mse: 25397788.0000\n",
      "Epoch 268/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1975.0033 - mse: 25361518.0000\n",
      "Epoch 269/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1981.2231 - mse: 25321366.0000\n",
      "Epoch 270/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1988.1647 - mse: 25437526.0000\n",
      "Epoch 271/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1974.0836 - mse: 25362688.0000\n",
      "Epoch 272/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1981.6362 - mse: 25343462.0000\n",
      "Epoch 273/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1982.9261 - mse: 25502386.0000\n",
      "Epoch 274/300\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 1978.1393 - mse: 25369446.0000\n",
      "Epoch 275/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1972.7500 - mse: 25374134.0000\n",
      "Epoch 276/300\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1979.9609 - mse: 25374746.0000\n",
      "Epoch 277/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1992.5642 - mse: 25633466.0000\n",
      "Epoch 278/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1970.1079 - mse: 25347944.0000\n",
      "Epoch 279/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1974.5112 - mse: 25396262.0000\n",
      "Epoch 280/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1983.0044 - mse: 25366586.0000\n",
      "Epoch 281/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1972.9100 - mse: 25347838.0000\n",
      "Epoch 282/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1968.3820 - mse: 25375890.0000\n",
      "Epoch 283/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1965.7556 - mse: 25329854.0000\n",
      "Epoch 284/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1975.1283 - mse: 25272110.0000\n",
      "Epoch 285/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1967.7112 - mse: 25382894.0000\n",
      "Epoch 286/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1970.1685 - mse: 25369120.0000\n",
      "Epoch 287/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1965.2610 - mse: 25392952.0000\n",
      "Epoch 288/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1970.7488 - mse: 25383084.0000\n",
      "Epoch 289/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1965.4003 - mse: 25409322.0000\n",
      "Epoch 290/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1962.6753 - mse: 25349276.0000\n",
      "Epoch 291/300\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 1973.3070 - mse: 25372924.0000\n",
      "Epoch 292/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1969.5887 - mse: 25378412.0000\n",
      "Epoch 293/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1963.6907 - mse: 25406528.0000\n",
      "Epoch 294/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1960.6672 - mse: 25339442.0000\n",
      "Epoch 295/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1958.5049 - mse: 25369018.0000\n",
      "Epoch 296/300\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 1969.9348 - mse: 25407056.0000\n",
      "Epoch 297/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1966.2203 - mse: 25345404.0000\n",
      "Epoch 298/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1963.2656 - mse: 25282064.0000\n",
      "Epoch 299/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1969.4578 - mse: 25352252.0000\n",
      "Epoch 300/300\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 1962.2418 - mse: 25375342.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c814731660>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create the model\n",
    "model_1 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (11, ), name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", name = \"hidden_layer_3\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layers\")\n",
    "    ], name = \"model_1\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_1.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    metrics = ['mse']\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model_1.fit(X_train_prepared, y_train, epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 14ms/step - loss: 1755.3727 - mse: 21411860.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1755.3726806640625, 21411860.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(X_test_prepared, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better than the models we trained earlier. Maybe a greater number of hidden layers and neurons in each layer does indeed help in improving accuracy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import the Boston pricing dataset from TensorFlow tf.keras.datasets and model it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(path=\"boston_housing.npz\", test_split = 0.2, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((404, 13), (404,), (102, 13), (102,))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "13/13 [==============================] - 0s 11ms/step - loss: 9.6160 - mae: 9.6160\n",
      "Epoch 2/200\n",
      "13/13 [==============================] - 0s 13ms/step - loss: 6.2460 - mae: 6.2460\n",
      "Epoch 3/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.5509 - mae: 5.5509\n",
      "Epoch 4/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.1553 - mae: 5.1553\n",
      "Epoch 5/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.2057 - mae: 5.2057\n",
      "Epoch 6/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.7782 - mae: 5.7782\n",
      "Epoch 7/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.9084 - mae: 5.9084\n",
      "Epoch 8/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.5967 - mae: 5.5967\n",
      "Epoch 9/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.0983 - mae: 5.0983\n",
      "Epoch 10/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.7975 - mae: 4.7975\n",
      "Epoch 11/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.7047 - mae: 4.7047\n",
      "Epoch 12/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.0271 - mae: 5.0271\n",
      "Epoch 13/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.6831 - mae: 4.6831\n",
      "Epoch 14/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.8718 - mae: 4.8718\n",
      "Epoch 15/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 5.0203 - mae: 5.0203\n",
      "Epoch 16/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.9488 - mae: 4.9488\n",
      "Epoch 17/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.4379 - mae: 4.4379\n",
      "Epoch 18/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.6767 - mae: 4.6767\n",
      "Epoch 19/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.4967 - mae: 4.4967\n",
      "Epoch 20/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.3102 - mae: 4.3102\n",
      "Epoch 21/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.6523 - mae: 4.6523\n",
      "Epoch 22/200\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.8120 - mae: 4.8120\n",
      "Epoch 23/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.6618 - mae: 4.6618\n",
      "Epoch 24/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.3037 - mae: 4.3037\n",
      "Epoch 25/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.5974 - mae: 4.5974\n",
      "Epoch 26/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.2769 - mae: 4.2769\n",
      "Epoch 27/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.3596 - mae: 4.3596\n",
      "Epoch 28/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 4.1540 - mae: 4.1540\n",
      "Epoch 29/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.2860 - mae: 4.2860\n",
      "Epoch 30/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.3928 - mae: 4.3928\n",
      "Epoch 31/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.7061 - mae: 4.7061\n",
      "Epoch 32/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.3278 - mae: 4.3278\n",
      "Epoch 33/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.0234 - mae: 4.0234\n",
      "Epoch 34/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.8836 - mae: 3.8836\n",
      "Epoch 35/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.7894 - mae: 3.7894\n",
      "Epoch 36/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.8935 - mae: 3.8935\n",
      "Epoch 37/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.2129 - mae: 4.2129\n",
      "Epoch 38/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.9641 - mae: 3.9641\n",
      "Epoch 39/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.8037 - mae: 3.8037\n",
      "Epoch 40/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.6993 - mae: 3.6993\n",
      "Epoch 41/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.9516 - mae: 3.9516\n",
      "Epoch 42/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.7737 - mae: 3.7737\n",
      "Epoch 43/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.9304 - mae: 3.9304\n",
      "Epoch 44/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.2730 - mae: 4.2730\n",
      "Epoch 45/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.7465 - mae: 3.7465\n",
      "Epoch 46/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.8430 - mae: 3.8430\n",
      "Epoch 47/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.7947 - mae: 4.7947\n",
      "Epoch 48/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.0774 - mae: 4.0774\n",
      "Epoch 49/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.7370 - mae: 3.7370\n",
      "Epoch 50/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.6267 - mae: 3.6267\n",
      "Epoch 51/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.6725 - mae: 3.6725\n",
      "Epoch 52/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.6291 - mae: 3.6291\n",
      "Epoch 53/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.5216 - mae: 3.5216\n",
      "Epoch 54/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.4734 - mae: 3.4734\n",
      "Epoch 55/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.7481 - mae: 3.7481\n",
      "Epoch 56/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.8346 - mae: 3.8346\n",
      "Epoch 57/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 4.2743 - mae: 4.2743\n",
      "Epoch 58/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.9020 - mae: 3.9020\n",
      "Epoch 59/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.6756 - mae: 3.6756\n",
      "Epoch 60/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.4163 - mae: 3.4163\n",
      "Epoch 61/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.7177 - mae: 3.7177\n",
      "Epoch 62/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.3261 - mae: 3.3261\n",
      "Epoch 63/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3.4121 - mae: 3.4121\n",
      "Epoch 64/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.5444 - mae: 3.5444\n",
      "Epoch 65/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.3477 - mae: 3.3477\n",
      "Epoch 66/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.4246 - mae: 3.4246\n",
      "Epoch 67/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.3705 - mae: 3.3705\n",
      "Epoch 68/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1676 - mae: 3.1676\n",
      "Epoch 69/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.3588 - mae: 3.3588\n",
      "Epoch 70/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.6571 - mae: 3.6571\n",
      "Epoch 71/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.3683 - mae: 3.3683\n",
      "Epoch 72/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.1041 - mae: 3.1041\n",
      "Epoch 73/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.2426 - mae: 3.2426\n",
      "Epoch 74/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.9522 - mae: 2.9522\n",
      "Epoch 75/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1199 - mae: 3.1199\n",
      "Epoch 76/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.2241 - mae: 3.2241\n",
      "Epoch 77/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3.7510 - mae: 3.7510\n",
      "Epoch 78/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 3.8279 - mae: 3.8279\n",
      "Epoch 79/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.6818 - mae: 3.6818\n",
      "Epoch 80/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.2125 - mae: 3.2125\n",
      "Epoch 81/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.1990 - mae: 3.1990\n",
      "Epoch 82/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.0351 - mae: 3.0351\n",
      "Epoch 83/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.4523 - mae: 3.4523\n",
      "Epoch 84/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.4035 - mae: 3.4035\n",
      "Epoch 85/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.3626 - mae: 3.3626\n",
      "Epoch 86/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.4373 - mae: 3.4373\n",
      "Epoch 87/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.0867 - mae: 3.0867\n",
      "Epoch 88/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.0340 - mae: 3.0340\n",
      "Epoch 89/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.9574 - mae: 2.9574\n",
      "Epoch 90/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.9760 - mae: 2.9760\n",
      "Epoch 91/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1112 - mae: 3.1112\n",
      "Epoch 92/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1307 - mae: 3.1307\n",
      "Epoch 93/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.9791 - mae: 2.9791\n",
      "Epoch 94/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.0054 - mae: 3.0054\n",
      "Epoch 95/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.1312 - mae: 3.1312\n",
      "Epoch 96/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.6991 - mae: 3.6991\n",
      "Epoch 97/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.0567 - mae: 3.0567\n",
      "Epoch 98/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8222 - mae: 2.8222\n",
      "Epoch 99/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.9215 - mae: 2.9215\n",
      "Epoch 100/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7819 - mae: 2.7819\n",
      "Epoch 101/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7660 - mae: 2.7660\n",
      "Epoch 102/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.8646 - mae: 2.8646\n",
      "Epoch 103/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.9777 - mae: 2.9777\n",
      "Epoch 104/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8017 - mae: 2.8017\n",
      "Epoch 105/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8898 - mae: 2.8898\n",
      "Epoch 106/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8769 - mae: 2.8769\n",
      "Epoch 107/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.9807 - mae: 2.9807\n",
      "Epoch 108/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.1185 - mae: 3.1185\n",
      "Epoch 109/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.9847 - mae: 2.9847\n",
      "Epoch 110/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 3.5178 - mae: 3.5178\n",
      "Epoch 111/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8660 - mae: 2.8660\n",
      "Epoch 112/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6596 - mae: 2.6596\n",
      "Epoch 113/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.7376 - mae: 2.7376\n",
      "Epoch 114/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.7663 - mae: 2.7663\n",
      "Epoch 115/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.7446 - mae: 2.7446\n",
      "Epoch 116/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8510 - mae: 2.8510\n",
      "Epoch 117/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6765 - mae: 2.6765\n",
      "Epoch 118/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6185 - mae: 2.6185\n",
      "Epoch 119/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8405 - mae: 2.8405\n",
      "Epoch 120/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.7455 - mae: 2.7455\n",
      "Epoch 121/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7939 - mae: 2.7939\n",
      "Epoch 122/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8226 - mae: 2.8226\n",
      "Epoch 123/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.9959 - mae: 2.9959\n",
      "Epoch 124/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.7313 - mae: 2.7313\n",
      "Epoch 125/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.6140 - mae: 2.6140\n",
      "Epoch 126/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 3.0919 - mae: 3.0919\n",
      "Epoch 127/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.9990 - mae: 2.9990\n",
      "Epoch 128/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6496 - mae: 2.6496\n",
      "Epoch 129/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.6287 - mae: 2.6287\n",
      "Epoch 130/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.6908 - mae: 2.6908\n",
      "Epoch 131/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8460 - mae: 2.8460\n",
      "Epoch 132/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5645 - mae: 2.5645\n",
      "Epoch 133/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.6355 - mae: 2.6355\n",
      "Epoch 134/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5086 - mae: 2.5086\n",
      "Epoch 135/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5808 - mae: 2.5808\n",
      "Epoch 136/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6398 - mae: 2.6398\n",
      "Epoch 137/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.9845 - mae: 2.9845\n",
      "Epoch 138/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.0247 - mae: 3.0247\n",
      "Epoch 139/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 3.0298 - mae: 3.0298\n",
      "Epoch 140/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.6929 - mae: 2.6929\n",
      "Epoch 141/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7803 - mae: 2.7803\n",
      "Epoch 142/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5846 - mae: 2.5846\n",
      "Epoch 143/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.7606 - mae: 2.7606\n",
      "Epoch 144/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4797 - mae: 2.4797\n",
      "Epoch 145/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.9045 - mae: 2.9045\n",
      "Epoch 146/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5250 - mae: 2.5250\n",
      "Epoch 147/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4611 - mae: 2.4611\n",
      "Epoch 148/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6185 - mae: 2.6185\n",
      "Epoch 149/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5264 - mae: 2.5264\n",
      "Epoch 150/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4745 - mae: 2.4745\n",
      "Epoch 151/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5290 - mae: 2.5290\n",
      "Epoch 152/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4947 - mae: 2.4947\n",
      "Epoch 153/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.8440 - mae: 2.8440\n",
      "Epoch 154/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4674 - mae: 2.4674\n",
      "Epoch 155/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5385 - mae: 2.5385\n",
      "Epoch 156/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6197 - mae: 2.6197\n",
      "Epoch 157/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4778 - mae: 2.4778\n",
      "Epoch 158/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.7134 - mae: 2.7134\n",
      "Epoch 159/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8281 - mae: 2.8281\n",
      "Epoch 160/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4581 - mae: 2.4581\n",
      "Epoch 161/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4384 - mae: 2.4384\n",
      "Epoch 162/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.6812 - mae: 2.6812\n",
      "Epoch 163/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 3.0148 - mae: 3.0148\n",
      "Epoch 164/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5567 - mae: 2.5567\n",
      "Epoch 165/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5763 - mae: 2.5763\n",
      "Epoch 166/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4513 - mae: 2.4513\n",
      "Epoch 167/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5090 - mae: 2.5090\n",
      "Epoch 168/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6848 - mae: 2.6848\n",
      "Epoch 169/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5549 - mae: 2.5549\n",
      "Epoch 170/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.8953 - mae: 2.8953\n",
      "Epoch 171/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4192 - mae: 2.4192\n",
      "Epoch 172/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6036 - mae: 2.6036\n",
      "Epoch 173/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5385 - mae: 2.5385\n",
      "Epoch 174/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5341 - mae: 2.5341\n",
      "Epoch 175/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.3857 - mae: 2.3857\n",
      "Epoch 176/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.3780 - mae: 2.3780\n",
      "Epoch 177/200\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 2.4074 - mae: 2.4074\n",
      "Epoch 178/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.3973 - mae: 2.3973\n",
      "Epoch 179/200\n",
      "13/13 [==============================] - 0s 8ms/step - loss: 2.4736 - mae: 2.4736\n",
      "Epoch 180/200\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 2.3781 - mae: 2.3781\n",
      "Epoch 181/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4054 - mae: 2.4054\n",
      "Epoch 182/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4886 - mae: 2.4886\n",
      "Epoch 183/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5213 - mae: 2.5213\n",
      "Epoch 184/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.5643 - mae: 2.5643\n",
      "Epoch 185/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.3429 - mae: 2.3429\n",
      "Epoch 186/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3747 - mae: 2.3747\n",
      "Epoch 187/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4240 - mae: 2.4240\n",
      "Epoch 188/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4444 - mae: 2.4444\n",
      "Epoch 189/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3917 - mae: 2.3917\n",
      "Epoch 190/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.3357 - mae: 2.3357\n",
      "Epoch 191/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3988 - mae: 2.3988\n",
      "Epoch 192/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4321 - mae: 2.4321\n",
      "Epoch 193/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3663 - mae: 2.3663\n",
      "Epoch 194/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2800 - mae: 2.2800\n",
      "Epoch 195/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4046 - mae: 2.4046\n",
      "Epoch 196/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.4171 - mae: 2.4171\n",
      "Epoch 197/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.6683 - mae: 2.6683\n",
      "Epoch 198/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3573 - mae: 2.3573\n",
      "Epoch 199/200\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4629 - mae: 2.4629\n",
      "Epoch 200/200\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 2.3239 - mae: 2.3239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c8120050f0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create the model\n",
    "model_boston = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", input_shape = (13, ), name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(64, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", name = \"hidden_layer_3\"),\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", name = \"hidden_layer_4\"),\n",
    "        tf.keras.layers.Dense(1, activation = \"relu\", name = \"output_layer\")\n",
    "    ], name = \"model_boston\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_boston.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model_boston.fit(x_train, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 8ms/step - loss: 2.8430 - mae: 2.8430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.843005895614624, 2.843005895614624]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_boston.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this was easy!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98ba7f9ca0a7d3f9faf83a09faac2df1e0ca0e1c9a5db868ca666908d95c6454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
