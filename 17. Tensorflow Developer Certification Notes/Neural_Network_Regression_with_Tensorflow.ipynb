{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 20 09:41:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.96       Driver Version: 512.96       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   55C    P8     5W /  N/A |   4446MiB /  6144MiB |      7%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1928    C+G   ...e\\Current\\LogiOverlay.exe    N/A      |\n",
      "|    0   N/A  N/A      6732    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A     15316    C+G   ...mmandCenterBackground.exe    N/A      |\n",
      "|    0   N/A  N/A     20752      C   ...thon\\Python310\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll cover:\n",
    "* Architecture of a neural network regression model\n",
    "* Input shapes and output shapes a regression model (features and labels)\n",
    "* Creating custom data to view and fit\n",
    "* Steps in modelling\n",
    "* Creating a model, compiling a model, fitting a model, evaluating a model\n",
    "* Different evaluation methods\n",
    "* Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression inputs and outputs\n",
    "\n",
    "`Inputs` -> `Machine Learning Model` -> `Outputs`\n",
    "\n",
    "We're essentially trying to figure out the relationship between inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of Neural Networks\n",
    "\n",
    "`Input` -> `Input Layer` -> `Hidden Layer(s)` -> `Output Layer`-> `Output`\n",
    "\n",
    "|Hyperparameter|Typical Value|\n",
    "|--------------|-------------|\n",
    "|Input Layer Shape|Same shape as number of features (eg 3 for # bedrooms, # car spaces in housing price prediction)|\n",
    "|Hidden Layer(s)|Problem specific, minimum = 1, maximum = $\\infty$|\n",
    "|Neurons per hidden layer|Problem specific; generally 10 to 100|\n",
    "|Output layer shape|Same shape as desired prediction shape (eg 1 for house price)|\n",
    "|Hidden activation|Using ReLU (rectified linear unit)|\n",
    "|Output activation|None, ReLU, logistic/tanh|\n",
    "|Loss function|MSE(mean square error) or MAE (mean absolute error)/Huber (combination of MAE/MSE) if outliers|\n",
    "|Optimizer|SGD (stochastic gradient descent), Adam|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model specimen\n",
    "model = tf.keras.Sequential(<br>\n",
    "    [<br>\n",
    "        tf.keras.Input(shape = (3,)),<br>\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),<br>\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),<br>\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),<br>\n",
    "        tf.keras.layers.Dense(1, activation = None)<br>\n",
    "    ]<br>\n",
    ")<br>\n",
    "#### Compiling the model\n",
    "model.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(lr = 1e-3), metrics = [\"mae\"])\n",
    "#### Fit the model\n",
    "model.fit(X_train, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOQElEQVR4nO3df2jc933H8ddrigZHGlCCVWNpMR4lHIRBrU6EQctIadfL8o+Vf8LyR/FYwPmjgY6Vg6j/NDAGYdcf/2wUHBriQZtRqKKEUXrNTJkpjDG5MpXT7EgpNsvJsR26oxl8YYr63h/6npFcS/dDd/refe75AKG7z33le/NFeeb8/X7P54gQACAdv1f0AACAwSLsAJAYwg4AiSHsAJAYwg4AiSHsAJCYjmG3/bDtn9j+he23bX85X3/RdtP2lfzryeGPCwDoxJ2uY7d9QtKJiPiZ7QckXZa0JOlpSf8bEV8f+pQAgK7d12mDiLgh6UZ++0Pb70iaH/ZgAID+dHzFvmdj+5SkS5L+SNLfSPpLSb+RtCbpKxHxPwf9/LFjx+LUqVN9jgoAk+ny5csfRMRst9t3HXbbH5P0b5L+LiJWbB+X9IGkkPS32jlc81f3+Llzks5J0smTJ//4+vXr3c4GAJBk+3JELHa7fVdXxdielvQDSd+NiBVJioibEbEdEb+V9LKkx+71sxFxPiIWI2Jxdrbr/+EAAPrUzVUxlvQdSe9ExDd3rZ/YtdlTkq4OfjwAQK86njyV9GlJX5S0YftKvvZVSc/YPq2dQzHXJD03hPkAAD3q5qqYn0ryPR764eDHAQAcFu88BYDEdHMoBgDQp9X1pmr1hjZbmeZmSqpWylpaGO5bgQg7AAzJ6npTyysbyra2JUnNVqbllQ1JGmrcORQDAENSqzfuRL0t29pWrd4Y6vMSdgAYks1W1tP6oBB2ABiSuZlST+uDQtgBYEiqlbJK01N71krTU6pWykN9Xk6eAsCQtE+QclUMACRkaWF+6CG/G4diACAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEsOHWQMYK6vrTdXqDW22Ms3NlFStlI/8w6JHHWEHMDZW15taXtlQtrUtSWq2Mi2vbEgScd+FQzEAxkat3rgT9bZsa1u1eqOgiUYTYQcwNjZbWU/rk4qwAxgbczOlntYnFWEHMDaqlbJK01N71krTU6pWygVNNJo4eQpgbLRPkHJVzMEIO4CxsrQwT8g74FAMACSmY9htP2z7J7Z/Yftt21/O1x+y/Zbtd/PvDw5/XABAJ928Yv9I0lci4lFJfyLpS7YflfSCpIsR8Yiki/l9AEDBOoY9Im5ExM/y2x9KekfSvKQzki7km12QtDSkGQEAPejpGLvtU5IWJP2HpOMRcSN/6H1Jxwc7GgCgH12H3fbHJP1A0l9HxG92PxYRISn2+blzttdsr92+fftQwwIAOusq7LantRP170bESr580/aJ/PETkm7d62cj4nxELEbE4uzs7CBmBgAcoJurYizpO5LeiYhv7nroTUln89tnJb0x+PEAAL3q5g1Kn5b0RUkbtq/ka1+V9JKk79t+VtJ1SU8PZUIAQE86hj0ifirJ+zz8ucGOAwA4LN55CgCJIewAkBjCDgCJIewAkBjCDgCJIewAkBjCDgCJIewAkBjCDgCJIewAkBjCDgCJ6eYfAQOQuNX1pmr1hjZbmeZmSqpWylpamC96LPSJsAMTbnW9qeWVDWVb25KkZivT8sqGJBH3McWhGGDC1eqNO1Fvy7a2Vas3CpoIh0XYgQm32cp6WsfoI+zAhJubKfW0jtFH2IEJV62UVZqe2rNWmp5StVIuaCIcFidPgQnXPkHKVTHpIOwAtLQwT8gTwqEYAEgMYQeAxBB2AEgMYQeAxBB2AEgMYQeAxBB2AEgMYQeAxBB2AEgMYQeAxBB2AEgMYQeAxBB2AEgMYQeAxBB2AEgMYQeAxBB2AEgMYQeAxBB2AEhMx7DbfsX2LdtXd629aLtp+0r+9eRwxwQAdKubD7N+VdI/SPqnu9a/FRFfH/hEQAJW15uq1RvabGWamympWinzYdE4Mh3DHhGXbJ86glmAJKyuN7W8sqFsa1uS1GxlWl7ZkCTijiNxmGPsz9v+eX6o5sGBTQSMuVq9cSfqbdnWtmr1RkETYdL0G/ZvS/qEpNOSbkj6xn4b2j5ne8322u3bt/t8OmB8bLayntaBQesr7BFxMyK2I+K3kl6W9NgB256PiMWIWJydne13TmBszM2UeloHBq2vsNs+sevuU5Ku7rctMGmqlbJK01N71krTU6pWygVNhEnT8eSp7dckPS7pmO33JH1N0uO2T0sKSdckPTe8EYHx0j5BylUxKIoj4siebHFxMdbW1o7s+QAgBbYvR8Rit9vzzlMASAxhB4DEEHYASAxhB4DEEHYASAxhB4DEEHYASAxhB4DEEHYASAxhB4DEEHYASAxhB4DEEHYASAxhB4DEEHYASAxhB4DEEHYASAxhB4DEEHYASAxhB4DE3Ff0AEC3VtebqtUb2mxlmpspqVopa2lhvuixgJFD2DEWVtebWl7ZULa1LUlqtjItr2xIEnEH7sKhGIyFWr1xJ+pt2da2avVGQRMBo4uwYyxstrKe1oFJRtgxFuZmSj2tA5OMsGMsVCtllaan9qyVpqdUrZQLmggYXZw8xVhonyDlqhigM8KOsbG0ME/IgS5wKAYAEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxHcNu+xXbt2xf3bX2kO23bL+bf39wuGMCALrVzSv2VyU9cdfaC5IuRsQjki7m9wEAI6Bj2CPikqRf37V8RtKF/PYFSUuDHQsA0K9+j7Efj4gb+e33JR0f0DwAgEM69MnTiAhJsd/jts/ZXrO9dvv27cM+HQCgg37DftP2CUnKv9/ab8OIOB8RixGxODs72+fTAQC61W/Y35R0Nr99VtIbgxkHAHBY3Vzu+Jqkf5dUtv2e7WclvSTpz2y/K+nz+X0AwAjo+NF4EfHMPg99bsCzAAAGgHeeAkBi+DDrCba63lSt3tBmK9PcTEnVSpkPiwYSQNgn1Op6U8srG8q2tiVJzVam5ZUNSSLuwJjjUMyEqtUbd6Lelm1tq1ZvFDQRgEEh7BNqs5X1tA5gfBD2CTU3U+ppHcD4IOwTqlopqzQ9tWetND2laqVc0EQABoWTpxOqfYKUq2KA9BD2Cba0ME/IgQRxKAYAEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEkPYASAxhB0AEnNf0QOkZnW9qVq9oc1WprmZkqqVspYW5oseC8AEIewDtLre1PLKhrKtbUlSs5VpeWVDkog7gCPDoZgBqtUbd6Lelm1tq1ZvFDQRgElE2Ados5X1tA4Aw0DYB2huptTTOgAMA2EfoGqlrNL01J610vSUqpVyQRMBmEScPB2g9glSrooBUCTCPmBLC/OEHEChDhV229ckfShpW9JHEbE4iKEAAP0bxCv2z0bEBwP4cwAAA8DJUwBIzGHDHpJ+bPuy7XODGAgAcDiHPRTzmYho2v64pLds/1dEXNq9QR78c5J08uTJQz4dAKCTQ71ij4hm/v2WpNclPXaPbc5HxGJELM7Ozh7m6QAAXeg77Lbvt/1A+7akL0i6OqjBAAD9OcyhmOOSXrfd/nO+FxE/GshUAIC+9R32iPiVpE8OcBYAwABwuSMAJIawA0BiCDsAJIawA0BiCDsAJIawA0BiCDsAJIawA0BiCDsAJIawA0BiCDsAJGbkP8x6db2pWr2hzVamuZmSqpUyHxYNAAcY6bCvrje1vLKhbGtbktRsZVpe2ZAk4g4A+xjpQzG1euNO1NuyrW3V6o2CJgKA0TfSYd9sZT2tAwBGPOxzM6We1gEAIx72aqWs0vTUnrXS9JSqlXJBEwHA6Bvpk6ftE6RcFQMA3RvpsEs7cSfkANC9kT4UAwDoHWEHgMQQdgBIDGEHgMQQdgBIjCPi6J7Mvi3p+pE94eEdk/RB0UOMOPbRwdg/nbGPDnZM0v0RMdvtDxxp2MeN7bWIWCx6jlHGPjoY+6cz9tHB+tk/HIoBgMQQdgBIDGE/2PmiBxgD7KODsX86Yx8drOf9wzF2AEgMr9gBIDGEvQPbL9pu2r6Sfz1Z9EyjwPYTthu2f2n7haLnGUW2r9neyH9v1oqep2i2X7F9y/bVXWsP2X7L9rv59weLnLFo++yjnhtE2LvzrYg4nX/9sOhhimZ7StI/SvpzSY9Kesb2o8VONbI+m//ecDmf9KqkJ+5ae0HSxYh4RNLF/P4ke1W/u4+kHhtE2NGPxyT9MiJ+FRH/J+mfJZ0peCaMuIi4JOnXdy2fkXQhv31B0tJRzjRq9tlHPSPs3Xne9s/zvyZN9F8Vc/OS/nvX/ffyNewVkn5s+7Ltc0UPM6KOR8SN/Pb7ko4XOcwI66lBhF2S7X+1ffUeX2ckfVvSJySdlnRD0jeKnBVj5TMR8SntHLL6ku0/LXqgURY7l+hxmd7v6rlBI/8JSkchIj7fzXa2X5b0L0MeZxw0JT286/4f5GvYJSKa+fdbtl/XziGsS8VONXJu2j4RETdsn5B0q+iBRk1E3Gzf7rZBvGLvIP9la3tK0tX9tp0g/ynpEdt/aPv3Jf2FpDcLnmmk2L7f9gPt25K+IH537uVNSWfz22clvVHgLCOpnwbxir2zv7d9Wjt/Rbwm6blCpxkBEfGR7ecl1SVNSXolIt4ueKxRc1zS67alnf/OvhcRPyp2pGLZfk3S45KO2X5P0tckvSTp+7af1c6//Pp0cRMWb5999HivDeKdpwCQGA7FAEBiCDsAJIawA0BiCDsAJIawA0BiCDsAJIawA0BiCDsAJOb/AWIa1pguLY/fAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the features\n",
    "X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n",
    "\n",
    "# Create the labels\n",
    "y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n",
    "\n",
    "# Visualise it\n",
    "plt.scatter(X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Output Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'bedroom', b'bathroom', b'garage'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700])>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a demo tensor for our housing price prediction problem\n",
    "\n",
    "house_info = tf.constant([\"bedroom\",\"bathroom\", \"garage\"])\n",
    "house_price = tf.constant([939700])\n",
    "house_info, house_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 1]), TensorShape([8]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "X = X[..., tf.newaxis]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in Modelling with Tensorflow\n",
    "\n",
    "1. **Creating a model** - define the input and output layers, as well as the hidden layers of a deep learning model.\n",
    "2. **Compiling a model** - define the loss function (in other words, the function which tells our model how wrong it is) and the optimizer (tells our model how to improve the patterns its learning) and evaluation metrics (what we can use to interpret the performance of our model)\n",
    "3. **Fitting a model** - letting the model try to find patterns between X & y (features and labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 11.5048 - mae: 11.5048\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.3723 - mae: 11.3723\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.2398 - mae: 11.2398\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 11.1073 - mae: 11.1073\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.9748 - mae: 10.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2486e422230>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create a model using the Sequential API\n",
    "model  = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae, # Mean absolute error\n",
    "    optimizer = tf.keras.optimizers.SGD(), # Stochastic Gradient Descent\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 5)\n",
    "\n",
    "# Alternatively we can also do\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Dense(1, shape = (1,)))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
       " array([[-7.],\n",
       "        [-4.],\n",
       "        [-1.],\n",
       "        [ 2.],\n",
       "        [ 5.],\n",
       "        [ 8.],\n",
       "        [11.],\n",
       "        [14.]])>,\n",
       " <tf.Tensor: shape=(8,), dtype=float64, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.])>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out X and y\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000249737B8820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "# Try and make a prediction using our model\n",
    "y_pred = model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.71602]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred + 11 # Add the mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still off! The model should have predicted 27 but we can always improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our Model\n",
    "\n",
    "We can improve the model, by altering the steps we took to create a model\n",
    "\n",
    "1. **Creating a model** - here we might add more layers, increase the number of hidden units (all called neurons) within each of the hidden layers, change the activation function of each layer.\n",
    "2. **Compiling a model** - here we might change the loss function, optimization function or perhaps the **learning rate** of the optimization function\n",
    "3. **Fitting a model** - here we might fit a model for more **epochs** (leave it training for longer) or on more data (give the model more examples to learn from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 11.2219 - mae: 11.2219\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.0894 - mae: 11.0894\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.9569 - mae: 10.9569\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.8244 - mae: 10.8244\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.6919 - mae: 10.6919\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.5594 - mae: 10.5594\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.4269 - mae: 10.4269\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.2944 - mae: 10.2944\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.1619 - mae: 10.1619\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.0294 - mae: 10.0294\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8969 - mae: 9.8969\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7644 - mae: 9.7644\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6319 - mae: 9.6319\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.4994 - mae: 9.4994\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.3669 - mae: 9.3669\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.2344 - mae: 9.2344\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.1019 - mae: 9.1019\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.9694 - mae: 8.9694\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.8369 - mae: 8.8369\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.7044 - mae: 8.7044\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.5719 - mae: 8.5719\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.4394 - mae: 8.4394\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3069 - mae: 8.3069\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1744 - mae: 8.1744\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0419 - mae: 8.0419\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.9094 - mae: 7.9094\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7769 - mae: 7.7769\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.6444 - mae: 7.6444\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.5119 - mae: 7.5119\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.3794 - mae: 7.3794\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.2750 - mae: 7.2750\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2694 - mae: 7.2694\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2638 - mae: 7.2638\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2581 - mae: 7.2581\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.2525 - mae: 7.2525\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2469 - mae: 7.2469\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2413 - mae: 7.2413\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2356 - mae: 7.2356\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2300 - mae: 7.2300\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.2244 - mae: 7.2244\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.2188 - mae: 7.2188\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2131 - mae: 7.2131\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2075 - mae: 7.2075\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2019 - mae: 7.2019\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1962 - mae: 7.1962\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1906 - mae: 7.1906\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.1850 - mae: 7.1850\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1794 - mae: 7.1794\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1737 - mae: 7.1737\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1681 - mae: 7.1681\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1625 - mae: 7.1625\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1569 - mae: 7.1569\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1512 - mae: 7.1512\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1456 - mae: 7.1456\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1400 - mae: 7.1400\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.1344 - mae: 7.1344\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1287 - mae: 7.1287\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1231 - mae: 7.1231\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1175 - mae: 7.1175\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1119 - mae: 7.1119\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1062 - mae: 7.1062\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1006 - mae: 7.1006\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0950 - mae: 7.0950\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0894 - mae: 7.0894\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0838 - mae: 7.0838\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0781 - mae: 7.0781\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0725 - mae: 7.0725\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0669 - mae: 7.0669\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0613 - mae: 7.0613\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0556 - mae: 7.0556\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0500 - mae: 7.0500\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0444 - mae: 7.0444\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0388 - mae: 7.0388\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.0331 - mae: 7.0331\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0275 - mae: 7.0275\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0219 - mae: 7.0219\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0163 - mae: 7.0163\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0106 - mae: 7.0106\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0050 - mae: 7.0050\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9994 - mae: 6.9994\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9938 - mae: 6.9938\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9881 - mae: 6.9881\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9825 - mae: 6.9825\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9769 - mae: 6.9769\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9713 - mae: 6.9713\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9656 - mae: 6.9656\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9600 - mae: 6.9600\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.9544 - mae: 6.9544\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9488 - mae: 6.9488\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9431 - mae: 6.9431\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9375 - mae: 6.9375\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9319 - mae: 6.9319\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9262 - mae: 6.9262\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9206 - mae: 6.9206\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9150 - mae: 6.9150\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9094 - mae: 6.9094\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9038 - mae: 6.9038\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.8981 - mae: 6.8981\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8925 - mae: 6.8925\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8869 - mae: 6.8869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24958f5be80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase the epochs\n",
    "# 1. Create a model using the Sequential API\n",
    "model  = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae, # Mean absolute error\n",
    "    optimizer = tf.keras.optimizers.SGD(), # Stochastic Gradient Descent\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
       " array([[-7.],\n",
       "        [-4.],\n",
       "        [-1.],\n",
       "        [ 2.],\n",
       "        [ 5.],\n",
       "        [ 8.],\n",
       "        [11.],\n",
       "        [14.]])>,\n",
       " <tf.Tensor: shape=(8,), dtype=float64, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.])>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remind ourselves of the data\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[29.739855]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see if our model's prediction has improved\n",
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yes! Its pretty close to 27!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 12.3193 - mae: 12.3193\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.7804 - mae: 11.7804\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.2324 - mae: 11.2324\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.6601 - mae: 10.6601\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.0632 - mae: 10.0632\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4503 - mae: 9.4503\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.7991 - mae: 8.7991\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1072 - mae: 8.1072\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.3691 - mae: 7.3691\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.5758 - mae: 6.5758\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.7205 - mae: 5.7205\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7947 - mae: 4.7947\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3581 - mae: 4.3581\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3134 - mae: 4.3134\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.2550 - mae: 4.2550\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.2442 - mae: 4.2442\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.1520 - mae: 4.1520\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1739 - mae: 4.1739\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0681 - mae: 4.0681\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0807 - mae: 4.0807\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9954 - mae: 3.9954\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9739 - mae: 3.9739\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9208 - mae: 3.9208\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9047 - mae: 3.9047\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9267 - mae: 3.9267\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8797 - mae: 3.8797\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9341 - mae: 3.9341\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8678 - mae: 3.8678\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.9274 - mae: 3.9274\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8751 - mae: 3.8751\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9080 - mae: 3.9080\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8893 - mae: 3.8893\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8834 - mae: 3.8834\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8969 - mae: 3.8969\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8581 - mae: 3.8581\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9046 - mae: 3.9046\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8386 - mae: 3.8386\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9054 - mae: 3.9054\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8482 - mae: 3.8482\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8862 - mae: 3.8862\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8605 - mae: 3.8605\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.8608 - mae: 3.8608\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8683 - mae: 3.8683\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8352 - mae: 3.8352\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8762 - mae: 3.8762\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8106 - mae: 3.8106\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8821 - mae: 3.8821\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8234 - mae: 3.8234\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8626 - mae: 3.8626\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8328 - mae: 3.8328\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.8369 - mae: 3.8369\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8408 - mae: 3.8408\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8111 - mae: 3.8111\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8489 - mae: 3.8489\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7850 - mae: 3.7850\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8585 - mae: 3.8585\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7982 - mae: 3.7982\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8377 - mae: 3.8377\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8062 - mae: 3.8062\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8117 - mae: 3.8117\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8144 - mae: 3.8144\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7856 - mae: 3.7856\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8227 - mae: 3.8227\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7593 - mae: 3.7593\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8352 - mae: 3.8352\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7725 - mae: 3.7725\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8115 - mae: 3.8115\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7807 - mae: 3.7807\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7853 - mae: 3.7853\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7891 - mae: 3.7891\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7588 - mae: 3.7588\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7975 - mae: 3.7975\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7337 - mae: 3.7337\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8105 - mae: 3.8105\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7478 - mae: 3.7478\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7840 - mae: 3.7840\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7563 - mae: 3.7563\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7575 - mae: 3.7575\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7648 - mae: 3.7648\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7307 - mae: 3.7307\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7735 - mae: 3.7735\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7125 - mae: 3.7125\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7820 - mae: 3.7820\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7242 - mae: 3.7242\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7552 - mae: 3.7552\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7329 - mae: 3.7329\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7284 - mae: 3.7284\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7416 - mae: 3.7416\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7013 - mae: 3.7013\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7505 - mae: 3.7505\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.6921 - mae: 3.6921\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7522 - mae: 3.7522\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7016 - mae: 3.7016\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7251 - mae: 3.7251\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7105 - mae: 3.7105\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6979 - mae: 3.6979\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7194 - mae: 3.7194\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.6705 - mae: 3.6705\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7299 - mae: 3.7299\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6711 - mae: 3.6711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249763e7ac0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test by adding in a hidden layer\n",
    "\n",
    "# 1. Create the model \n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[31.22314]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to see if its performance has improved.\n",
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohkay.....this seems a bit off...but we're getting there. This might be overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 12.7339 - mae: 12.7339\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.6498 - mae: 12.6498\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 12.5666 - mae: 12.5666\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 12.4824 - mae: 12.4824\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.3987 - mae: 12.3987\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.3151 - mae: 12.3151\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.2313 - mae: 12.2313\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.1475 - mae: 12.1475\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 12.0636 - mae: 12.0636\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.9797 - mae: 11.9797\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.8960 - mae: 11.8960\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.8131 - mae: 11.8131\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.7301 - mae: 11.7301\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.6470 - mae: 11.6470\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5639 - mae: 11.5639\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.4806 - mae: 11.4806\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.3973 - mae: 11.3973\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.3140 - mae: 11.3140\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.2306 - mae: 11.2306\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.1471 - mae: 11.1471\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.0633 - mae: 11.0633\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.9795 - mae: 10.9795\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.8956 - mae: 10.8956\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.8120 - mae: 10.8120\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.7287 - mae: 10.7287\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.6454 - mae: 10.6454\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.5619 - mae: 10.5619\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.4782 - mae: 10.4782\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.3943 - mae: 10.3943\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.3101 - mae: 10.3101\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.2256 - mae: 10.2256\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.1409 - mae: 10.1409\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.0650 - mae: 10.0650\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0023 - mae: 10.0023\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.9390 - mae: 9.9390\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.8751 - mae: 9.8751\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.8108 - mae: 9.8108\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7458 - mae: 9.7458\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6804 - mae: 9.6804\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.6145 - mae: 9.6145\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.5482 - mae: 9.5482\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.4813 - mae: 9.4813\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.4138 - mae: 9.4138\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.3459 - mae: 9.3459\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.2776 - mae: 9.2776\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.2088 - mae: 9.2088\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.1395 - mae: 9.1395\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.0698 - mae: 9.0698\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.9997 - mae: 8.9997\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.9291 - mae: 8.9291\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.8583 - mae: 8.8583\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.7871 - mae: 8.7871\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.7154 - mae: 8.7154\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.6433 - mae: 8.6433\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.5707 - mae: 8.5707\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.4975 - mae: 8.4975\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.4240 - mae: 8.4240\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.3498 - mae: 8.3498\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.2752 - mae: 8.2752\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.2000 - mae: 8.2000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1244 - mae: 8.1244\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0483 - mae: 8.0483\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.9716 - mae: 7.9716\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.8944 - mae: 7.8944\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.8167 - mae: 7.8167\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7385 - mae: 7.7385\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.6598 - mae: 7.6598\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5806 - mae: 7.5806\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.5011 - mae: 7.5011\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4209 - mae: 7.4209\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.3402 - mae: 7.3402\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2589 - mae: 7.2589\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1770 - mae: 7.1770\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0944 - mae: 7.0944\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0113 - mae: 7.0113\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9275 - mae: 6.9275\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8432 - mae: 6.8432\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.7582 - mae: 6.7582\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.6726 - mae: 6.6726\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5864 - mae: 6.5864\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4996 - mae: 6.4996\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.4122 - mae: 6.4122\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.3241 - mae: 6.3241\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.2354 - mae: 6.2354\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1461 - mae: 6.1461\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.0561 - mae: 6.0561\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9655 - mae: 5.9655\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8742 - mae: 5.8742\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.7823 - mae: 5.7823\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.6897 - mae: 5.6897\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5965 - mae: 5.5965\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5026 - mae: 5.5026\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4080 - mae: 5.4080\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3128 - mae: 5.3128\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.2168 - mae: 5.2168\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.1201 - mae: 5.1201\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.0227 - mae: 5.0227\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9286 - mae: 4.9286\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8307 - mae: 4.8307\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.7286 - mae: 4.7286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497d708940>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try the Adam optimizer this time\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[26.75023]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah this is worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 12.4190 - mae: 12.4190\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.8429 - mae: 11.8429\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.2581 - mae: 11.2581\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.6721 - mae: 10.6721\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0804 - mae: 10.0804\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.5489 - mae: 9.5489\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.1225 - mae: 9.1225\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.6844 - mae: 8.6844\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.2325 - mae: 8.2325\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.7671 - mae: 7.7671\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2883 - mae: 7.2883\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.7943 - mae: 6.7943\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.2844 - mae: 6.2844\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.7579 - mae: 5.7579\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.2143 - mae: 5.2143\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.6535 - mae: 4.6535\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0754 - mae: 4.0754\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8555 - mae: 3.8555\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7786 - mae: 3.7786\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8772 - mae: 3.8772\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9592 - mae: 3.9592\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0442 - mae: 4.0442\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.2009 - mae: 4.2009\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.2947 - mae: 4.2947\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3319 - mae: 4.3319\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3194 - mae: 4.3194\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2630 - mae: 4.2630\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1675 - mae: 4.1675\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0397 - mae: 4.0397\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8833 - mae: 3.8833\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7864 - mae: 3.7864\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7056 - mae: 3.7056\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.6225 - mae: 3.6225\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.5392 - mae: 3.5392\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.4557 - mae: 3.4557\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3690 - mae: 3.3690\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3832 - mae: 3.3832\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3844 - mae: 3.3844\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.3621 - mae: 3.3621\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3699 - mae: 3.3699\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3355 - mae: 3.3355\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.2558 - mae: 3.2558\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1418 - mae: 3.1418\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0561 - mae: 3.0561\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0588 - mae: 3.0588\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0587 - mae: 3.0587\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0463 - mae: 3.0463\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0225 - mae: 3.0225\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9879 - mae: 2.9879\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9434 - mae: 2.9434\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.8864 - mae: 2.8864\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.8234 - mae: 2.8234\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.7595 - mae: 2.7595\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.6876 - mae: 2.6876\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6068 - mae: 2.6068\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5252 - mae: 2.5252\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4880 - mae: 2.4880\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4156 - mae: 2.4156\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3245 - mae: 2.3245\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2534 - mae: 2.2534\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1764 - mae: 2.1764\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1018 - mae: 2.1018\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0224 - mae: 2.0224\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9605 - mae: 1.9605\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8925 - mae: 1.8925\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7844 - mae: 1.7844\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7111 - mae: 1.7111\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6392 - mae: 1.6392\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5528 - mae: 1.5528\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4482 - mae: 1.4482\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3288 - mae: 1.3288\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2027 - mae: 1.2027\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0693 - mae: 1.0693\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9998 - mae: 0.9998\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8935 - mae: 0.8935\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7530 - mae: 0.7530\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5703 - mae: 0.5703\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4596 - mae: 0.4596\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3370 - mae: 0.3370\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1705 - mae: 0.1705\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3356 - mae: 0.3356\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2114 - mae: 0.2114\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2668 - mae: 0.2668\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3382 - mae: 0.3382\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4465 - mae: 0.4465\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4333 - mae: 0.4333\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4359 - mae: 0.4359\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3670 - mae: 0.3670\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3806 - mae: 0.3806\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3484 - mae: 0.3484\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3203 - mae: 0.3203\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2763 - mae: 0.2763\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3990 - mae: 0.3990\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2378 - mae: 0.2378\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4056 - mae: 0.4056\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5224 - mae: 0.5224\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4573 - mae: 0.4573\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1685 - mae: 0.1685\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3968 - mae: 0.3968\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5774 - mae: 0.5774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497d7d36d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to reduce the number of neurons in the hidden layer. Maybe it will end up improving things for us?\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(50, activation = \"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[28.953987]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best model so far! We're sitting almost at 27!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving a model - Methods\n",
    "\n",
    "Common ways to improve a deep model:\n",
    "* Adding layers\n",
    "* Increase the number of hidden units\n",
    "* Change the activation functions\n",
    "* Change the optimization function\n",
    "* Change the learning rate\n",
    "* Fitting on more data\n",
    "* Fitting for longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model\n",
    "In practice, a typical workflow you'll go through when building neural networks is:\n",
    "\n",
    "```\n",
    "Build a model -> fit it -> evaluate it -> tweak a model -> fit it -> evaluate it -> tweak a model -> fit it .....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to evaluation....there are 3 words you should memorize:\n",
    ">\"Visualize, visualize, visualize\"\n",
    "\n",
    "Its a good idea to visualize:\n",
    "* The data - what data are we working with? What does it look like\n",
    "* The model itself - what does our model look like?\n",
    "* The training of a model - how does a model perform while it learns?\n",
    "* The predictions of the model - how do the predictions of a model line up against the ground truth (the original labels)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bigger dataset\n",
    "\n",
    "X = tf.cast(tf.range(-100, 100, 4), dtype=tf.float32)\n",
    "\n",
    "# Make labels for the dataset\n",
    "y = X + 10 + tf.random.uniform(X.shape, -10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2497d898220>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 sets ...\n",
    "\n",
    "* **Training set** - the model learns from this data, which is typically 70-80%\n",
    "* **Validation set** - the model gets tuned on this data, which is typically 10-15% of the data available\n",
    "* **Test set** - the model gets evaluated on this data to test what it has learnt, this set is typically 10-15% of the total data available\n",
    "\n",
    "Analogy<br>\n",
    "Course materials -> Practice exam  -> Final exam<br>\n",
    "[train_set]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[validation_set]&nbsp;&nbsp;&nbsp;[test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three sets...\n",
    "random_permutation = np.random.permutation(X.numpy().shape[0])\n",
    "test_set_idx, val_set_idx, train_set_idx = random_permutation[:len(random_permutation)//10], random_permutation[len(random_permutation)//10:int(0.3*len(random_permutation))], random_permutation[int(0.3*len(random_permutation)):]\n",
    "\n",
    "X_train, X_val, X_test = tf.convert_to_tensor(X.numpy()[train_set_idx])[..., tf.newaxis], tf.convert_to_tensor(X.numpy()[val_set_idx])[..., tf.newaxis], tf.convert_to_tensor(X.numpy()[test_set_idx])[..., tf.newaxis]\n",
    "y_train, y_val, y_test = tf.convert_to_tensor(y.numpy()[train_set_idx]), tf.convert_to_tensor(y.numpy()[val_set_idx]), tf.convert_to_tensor(y.numpy()[test_set_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the data\n",
    "\n",
    "Now we've got our data in training and test sets....lets visualise it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "# Plot the training data in blue\n",
    "plt.scatter(X_train, y_train, c=\"b\", label = \"training_data\")\n",
    "# Plot the val data in green\n",
    "plt.scatter(X_val, y_val, c = \"g\", label = \"validation_data\")\n",
    "# Plot the test data in red\n",
    "plt.scatter(X_test, y_test, c = \"r\", label = \"test_data\")\n",
    "# Show a legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1,activation = None, name = \"Dense_Layer\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pratik Sanghavi\\Desktop\\Projects\\Machine-Learning\\17. Tensorflow Developer Certification Notes\\Neural_Network_Regression_with_Tensorflow.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pratik%20Sanghavi/Desktop/Projects/Machine-Learning/17.%20Tensorflow%20Developer%20Certification%20Notes/Neural_Network_Regression_with_Tensorflow.ipynb#ch0000043?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msummary()\n",
      "File \u001b[1;32mc:\\Users\\Pratik Sanghavi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2869\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable)\u001b[0m\n\u001b[0;32m   2847\u001b[0m \u001b[39m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m   2848\u001b[0m \n\u001b[0;32m   2849\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2866\u001b[0m \u001b[39m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m   2867\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2868\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m-> 2869\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2870\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2871\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2872\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2873\u001b[0m layer_utils\u001b[39m.\u001b[39mprint_summary(\n\u001b[0;32m   2874\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2875\u001b[0m     line_length\u001b[39m=\u001b[39mline_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2878\u001b[0m     expand_nested\u001b[39m=\u001b[39mexpand_nested,\n\u001b[0;32m   2879\u001b[0m     show_trainable\u001b[39m=\u001b[39mshow_trainable)\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because input shape to the model is not defined. Lets create a model with a defined input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1,input_shape = (1,), activation = None, name = \"Dense_Layer\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_Layer (Dense)         (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presto! We have our model instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total params - total number of parameters in the model\n",
    "* Trainable params - these are the parameters (patterns) the model can update as it trains\n",
    "* Non-trainable params - these parameteres aren't updated during training (this is typical when you have parameters from other models during transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot_ng as pydot\n",
    "import graphviz\n",
    "pydot.find_graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 - 0s - loss: 21.0919 - mae: 21.0919 - val_loss: 7.6659 - val_mae: 7.6659 - 119ms/epoch - 59ms/step\n",
      "Epoch 2/100\n",
      "2/2 - 0s - loss: 10.6932 - mae: 10.6932 - val_loss: 11.0732 - val_mae: 11.0732 - 82ms/epoch - 41ms/step\n",
      "Epoch 3/100\n",
      "2/2 - 0s - loss: 10.4076 - mae: 10.4076 - val_loss: 19.2308 - val_mae: 19.2308 - 60ms/epoch - 30ms/step\n",
      "Epoch 4/100\n",
      "2/2 - 0s - loss: 24.7479 - mae: 24.7479 - val_loss: 7.0820 - val_mae: 7.0820 - 59ms/epoch - 29ms/step\n",
      "Epoch 5/100\n",
      "2/2 - 0s - loss: 9.4267 - mae: 9.4267 - val_loss: 7.6226 - val_mae: 7.6226 - 60ms/epoch - 30ms/step\n",
      "Epoch 6/100\n",
      "2/2 - 0s - loss: 10.8548 - mae: 10.8548 - val_loss: 10.0043 - val_mae: 10.0043 - 61ms/epoch - 31ms/step\n",
      "Epoch 7/100\n",
      "2/2 - 0s - loss: 10.7805 - mae: 10.7805 - val_loss: 10.1858 - val_mae: 10.1858 - 54ms/epoch - 27ms/step\n",
      "Epoch 8/100\n",
      "2/2 - 0s - loss: 9.6701 - mae: 9.6701 - val_loss: 7.5864 - val_mae: 7.5864 - 60ms/epoch - 30ms/step\n",
      "Epoch 9/100\n",
      "2/2 - 0s - loss: 9.8537 - mae: 9.8537 - val_loss: 36.9268 - val_mae: 36.9268 - 51ms/epoch - 25ms/step\n",
      "Epoch 10/100\n",
      "2/2 - 0s - loss: 34.7312 - mae: 34.7312 - val_loss: 21.3380 - val_mae: 21.3380 - 56ms/epoch - 28ms/step\n",
      "Epoch 11/100\n",
      "2/2 - 0s - loss: 19.1501 - mae: 19.1501 - val_loss: 34.2043 - val_mae: 34.2043 - 60ms/epoch - 30ms/step\n",
      "Epoch 12/100\n",
      "2/2 - 0s - loss: 31.6341 - mae: 31.6341 - val_loss: 7.0301 - val_mae: 7.0301 - 57ms/epoch - 29ms/step\n",
      "Epoch 13/100\n",
      "2/2 - 0s - loss: 9.3065 - mae: 9.3065 - val_loss: 28.9005 - val_mae: 28.9005 - 48ms/epoch - 24ms/step\n",
      "Epoch 14/100\n",
      "2/2 - 0s - loss: 27.7367 - mae: 27.7367 - val_loss: 13.6343 - val_mae: 13.6343 - 70ms/epoch - 35ms/step\n",
      "Epoch 15/100\n",
      "2/2 - 0s - loss: 12.2846 - mae: 12.2846 - val_loss: 24.0605 - val_mae: 24.0605 - 60ms/epoch - 30ms/step\n",
      "Epoch 16/100\n",
      "2/2 - 0s - loss: 22.7740 - mae: 22.7740 - val_loss: 7.0246 - val_mae: 7.0246 - 60ms/epoch - 30ms/step\n",
      "Epoch 17/100\n",
      "2/2 - 0s - loss: 9.7780 - mae: 9.7780 - val_loss: 22.1850 - val_mae: 22.1850 - 57ms/epoch - 28ms/step\n",
      "Epoch 18/100\n",
      "2/2 - 0s - loss: 19.7918 - mae: 19.7918 - val_loss: 25.4722 - val_mae: 25.4722 - 46ms/epoch - 23ms/step\n",
      "Epoch 19/100\n",
      "2/2 - 0s - loss: 24.4506 - mae: 24.4506 - val_loss: 7.4028 - val_mae: 7.4028 - 57ms/epoch - 29ms/step\n",
      "Epoch 20/100\n",
      "2/2 - 0s - loss: 11.0872 - mae: 11.0872 - val_loss: 7.2908 - val_mae: 7.2908 - 58ms/epoch - 29ms/step\n",
      "Epoch 21/100\n",
      "2/2 - 0s - loss: 10.6798 - mae: 10.6798 - val_loss: 26.5735 - val_mae: 26.5735 - 60ms/epoch - 30ms/step\n",
      "Epoch 22/100\n",
      "2/2 - 0s - loss: 32.0041 - mae: 32.0041 - val_loss: 13.4528 - val_mae: 13.4528 - 57ms/epoch - 29ms/step\n",
      "Epoch 23/100\n",
      "2/2 - 0s - loss: 11.9175 - mae: 11.9175 - val_loss: 10.2665 - val_mae: 10.2665 - 48ms/epoch - 24ms/step\n",
      "Epoch 24/100\n",
      "2/2 - 0s - loss: 10.9065 - mae: 10.9065 - val_loss: 21.5800 - val_mae: 21.5800 - 56ms/epoch - 28ms/step\n",
      "Epoch 25/100\n",
      "2/2 - 0s - loss: 20.1593 - mae: 20.1593 - val_loss: 26.1780 - val_mae: 26.1780 - 42ms/epoch - 21ms/step\n",
      "Epoch 26/100\n",
      "2/2 - 0s - loss: 25.3472 - mae: 25.3472 - val_loss: 14.8040 - val_mae: 14.8040 - 52ms/epoch - 26ms/step\n",
      "Epoch 27/100\n",
      "2/2 - 0s - loss: 12.9855 - mae: 12.9855 - val_loss: 9.4758 - val_mae: 9.4758 - 58ms/epoch - 29ms/step\n",
      "Epoch 28/100\n",
      "2/2 - 0s - loss: 9.2944 - mae: 9.2944 - val_loss: 14.0676 - val_mae: 14.0676 - 59ms/epoch - 29ms/step\n",
      "Epoch 29/100\n",
      "2/2 - 0s - loss: 19.3000 - mae: 19.3000 - val_loss: 10.4771 - val_mae: 10.4771 - 58ms/epoch - 29ms/step\n",
      "Epoch 30/100\n",
      "2/2 - 0s - loss: 13.9623 - mae: 13.9623 - val_loss: 27.7402 - val_mae: 27.7402 - 61ms/epoch - 30ms/step\n",
      "Epoch 31/100\n",
      "2/2 - 0s - loss: 35.2243 - mae: 35.2243 - val_loss: 9.5826 - val_mae: 9.5826 - 56ms/epoch - 28ms/step\n",
      "Epoch 32/100\n",
      "2/2 - 0s - loss: 13.2138 - mae: 13.2138 - val_loss: 22.0842 - val_mae: 22.0842 - 44ms/epoch - 22ms/step\n",
      "Epoch 33/100\n",
      "2/2 - 0s - loss: 19.9044 - mae: 19.9044 - val_loss: 7.2206 - val_mae: 7.2206 - 44ms/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "2/2 - 0s - loss: 9.9706 - mae: 9.9706 - val_loss: 8.3471 - val_mae: 8.3471 - 56ms/epoch - 28ms/step\n",
      "Epoch 35/100\n",
      "2/2 - 0s - loss: 13.4433 - mae: 13.4433 - val_loss: 22.1725 - val_mae: 22.1725 - 44ms/epoch - 22ms/step\n",
      "Epoch 36/100\n",
      "2/2 - 0s - loss: 26.7751 - mae: 26.7751 - val_loss: 11.0127 - val_mae: 11.0127 - 59ms/epoch - 30ms/step\n",
      "Epoch 37/100\n",
      "2/2 - 0s - loss: 10.4215 - mae: 10.4215 - val_loss: 13.0556 - val_mae: 13.0556 - 50ms/epoch - 25ms/step\n",
      "Epoch 38/100\n",
      "2/2 - 0s - loss: 17.4617 - mae: 17.4617 - val_loss: 6.7667 - val_mae: 6.7667 - 54ms/epoch - 27ms/step\n",
      "Epoch 39/100\n",
      "2/2 - 0s - loss: 9.2009 - mae: 9.2009 - val_loss: 11.6820 - val_mae: 11.6820 - 65ms/epoch - 33ms/step\n",
      "Epoch 40/100\n",
      "2/2 - 0s - loss: 15.9893 - mae: 15.9893 - val_loss: 26.9387 - val_mae: 26.9387 - 62ms/epoch - 31ms/step\n",
      "Epoch 41/100\n",
      "2/2 - 0s - loss: 31.7842 - mae: 31.7842 - val_loss: 20.0272 - val_mae: 20.0272 - 54ms/epoch - 27ms/step\n",
      "Epoch 42/100\n",
      "2/2 - 0s - loss: 19.3218 - mae: 19.3218 - val_loss: 7.1814 - val_mae: 7.1814 - 55ms/epoch - 27ms/step\n",
      "Epoch 43/100\n",
      "2/2 - 0s - loss: 10.1486 - mae: 10.1486 - val_loss: 14.1990 - val_mae: 14.1990 - 61ms/epoch - 30ms/step\n",
      "Epoch 44/100\n",
      "2/2 - 0s - loss: 11.8786 - mae: 11.8786 - val_loss: 9.4667 - val_mae: 9.4667 - 63ms/epoch - 32ms/step\n",
      "Epoch 45/100\n",
      "2/2 - 0s - loss: 13.0893 - mae: 13.0893 - val_loss: 18.6276 - val_mae: 18.6276 - 74ms/epoch - 37ms/step\n",
      "Epoch 46/100\n",
      "2/2 - 0s - loss: 21.4610 - mae: 21.4610 - val_loss: 38.6007 - val_mae: 38.6007 - 63ms/epoch - 31ms/step\n",
      "Epoch 47/100\n",
      "2/2 - 0s - loss: 36.7199 - mae: 36.7199 - val_loss: 7.1144 - val_mae: 7.1144 - 60ms/epoch - 30ms/step\n",
      "Epoch 48/100\n",
      "2/2 - 0s - loss: 9.6397 - mae: 9.6397 - val_loss: 11.4183 - val_mae: 11.4183 - 59ms/epoch - 29ms/step\n",
      "Epoch 49/100\n",
      "2/2 - 0s - loss: 14.3117 - mae: 14.3117 - val_loss: 19.7880 - val_mae: 19.7880 - 73ms/epoch - 37ms/step\n",
      "Epoch 50/100\n",
      "2/2 - 0s - loss: 24.0011 - mae: 24.0011 - val_loss: 6.8031 - val_mae: 6.8031 - 60ms/epoch - 30ms/step\n",
      "Epoch 51/100\n",
      "2/2 - 0s - loss: 9.0940 - mae: 9.0940 - val_loss: 16.7400 - val_mae: 16.7400 - 62ms/epoch - 31ms/step\n",
      "Epoch 52/100\n",
      "2/2 - 0s - loss: 15.9464 - mae: 15.9464 - val_loss: 8.9878 - val_mae: 8.9878 - 62ms/epoch - 31ms/step\n",
      "Epoch 53/100\n",
      "2/2 - 0s - loss: 9.2741 - mae: 9.2741 - val_loss: 8.0834 - val_mae: 8.0834 - 70ms/epoch - 35ms/step\n",
      "Epoch 54/100\n",
      "2/2 - 0s - loss: 9.4902 - mae: 9.4902 - val_loss: 35.7572 - val_mae: 35.7572 - 49ms/epoch - 24ms/step\n",
      "Epoch 55/100\n",
      "2/2 - 0s - loss: 34.9905 - mae: 34.9905 - val_loss: 6.6602 - val_mae: 6.6602 - 59ms/epoch - 30ms/step\n",
      "Epoch 56/100\n",
      "2/2 - 0s - loss: 9.2123 - mae: 9.2123 - val_loss: 32.0667 - val_mae: 32.0667 - 47ms/epoch - 24ms/step\n",
      "Epoch 57/100\n",
      "2/2 - 0s - loss: 29.1383 - mae: 29.1383 - val_loss: 20.9338 - val_mae: 20.9338 - 60ms/epoch - 30ms/step\n",
      "Epoch 58/100\n",
      "2/2 - 0s - loss: 24.8407 - mae: 24.8407 - val_loss: 6.7361 - val_mae: 6.7361 - 62ms/epoch - 31ms/step\n",
      "Epoch 59/100\n",
      "2/2 - 0s - loss: 8.8204 - mae: 8.8204 - val_loss: 28.1745 - val_mae: 28.1745 - 59ms/epoch - 30ms/step\n",
      "Epoch 60/100\n",
      "2/2 - 0s - loss: 25.7229 - mae: 25.7229 - val_loss: 6.7393 - val_mae: 6.7393 - 57ms/epoch - 29ms/step\n",
      "Epoch 61/100\n",
      "2/2 - 0s - loss: 9.1477 - mae: 9.1477 - val_loss: 6.5967 - val_mae: 6.5967 - 66ms/epoch - 33ms/step\n",
      "Epoch 62/100\n",
      "2/2 - 0s - loss: 8.9287 - mae: 8.9287 - val_loss: 7.0336 - val_mae: 7.0336 - 72ms/epoch - 36ms/step\n",
      "Epoch 63/100\n",
      "2/2 - 0s - loss: 10.6341 - mae: 10.6341 - val_loss: 11.6111 - val_mae: 11.6111 - 64ms/epoch - 32ms/step\n",
      "Epoch 64/100\n",
      "2/2 - 0s - loss: 16.1037 - mae: 16.1037 - val_loss: 15.6624 - val_mae: 15.6624 - 54ms/epoch - 27ms/step\n",
      "Epoch 65/100\n",
      "2/2 - 0s - loss: 19.1475 - mae: 19.1475 - val_loss: 6.7421 - val_mae: 6.7421 - 46ms/epoch - 23ms/step\n",
      "Epoch 66/100\n",
      "2/2 - 0s - loss: 9.0966 - mae: 9.0966 - val_loss: 32.2280 - val_mae: 32.2280 - 62ms/epoch - 31ms/step\n",
      "Epoch 67/100\n",
      "2/2 - 0s - loss: 30.8946 - mae: 30.8946 - val_loss: 17.7685 - val_mae: 17.7685 - 60ms/epoch - 30ms/step\n",
      "Epoch 68/100\n",
      "2/2 - 0s - loss: 15.8074 - mae: 15.8074 - val_loss: 27.5897 - val_mae: 27.5897 - 59ms/epoch - 30ms/step\n",
      "Epoch 69/100\n",
      "2/2 - 0s - loss: 26.6428 - mae: 26.6428 - val_loss: 30.7760 - val_mae: 30.7760 - 60ms/epoch - 30ms/step\n",
      "Epoch 70/100\n",
      "2/2 - 0s - loss: 27.8489 - mae: 27.8489 - val_loss: 16.2560 - val_mae: 16.2560 - 55ms/epoch - 28ms/step\n",
      "Epoch 71/100\n",
      "2/2 - 0s - loss: 16.3597 - mae: 16.3597 - val_loss: 20.0877 - val_mae: 20.0877 - 50ms/epoch - 25ms/step\n",
      "Epoch 72/100\n",
      "2/2 - 0s - loss: 16.6844 - mae: 16.6844 - val_loss: 18.3195 - val_mae: 18.3195 - 51ms/epoch - 25ms/step\n",
      "Epoch 73/100\n",
      "2/2 - 0s - loss: 23.7152 - mae: 23.7152 - val_loss: 6.4814 - val_mae: 6.4814 - 53ms/epoch - 26ms/step\n",
      "Epoch 74/100\n",
      "2/2 - 0s - loss: 9.0529 - mae: 9.0529 - val_loss: 19.3488 - val_mae: 19.3488 - 51ms/epoch - 25ms/step\n",
      "Epoch 75/100\n",
      "2/2 - 0s - loss: 22.3613 - mae: 22.3613 - val_loss: 37.1487 - val_mae: 37.1487 - 61ms/epoch - 31ms/step\n",
      "Epoch 76/100\n",
      "2/2 - 0s - loss: 34.8810 - mae: 34.8810 - val_loss: 13.0000 - val_mae: 13.0000 - 76ms/epoch - 38ms/step\n",
      "Epoch 77/100\n",
      "2/2 - 0s - loss: 16.5735 - mae: 16.5735 - val_loss: 6.4801 - val_mae: 6.4801 - 73ms/epoch - 37ms/step\n",
      "Epoch 78/100\n",
      "2/2 - 0s - loss: 8.8851 - mae: 8.8851 - val_loss: 23.3028 - val_mae: 23.3028 - 56ms/epoch - 28ms/step\n",
      "Epoch 79/100\n",
      "2/2 - 0s - loss: 29.1397 - mae: 29.1397 - val_loss: 6.4674 - val_mae: 6.4674 - 45ms/epoch - 22ms/step\n",
      "Epoch 80/100\n",
      "2/2 - 0s - loss: 8.7136 - mae: 8.7136 - val_loss: 16.6795 - val_mae: 16.6795 - 60ms/epoch - 30ms/step\n",
      "Epoch 81/100\n",
      "2/2 - 0s - loss: 14.4406 - mae: 14.4406 - val_loss: 12.4772 - val_mae: 12.4772 - 59ms/epoch - 29ms/step\n",
      "Epoch 82/100\n",
      "2/2 - 0s - loss: 16.8255 - mae: 16.8255 - val_loss: 13.3499 - val_mae: 13.3499 - 60ms/epoch - 30ms/step\n",
      "Epoch 83/100\n",
      "2/2 - 0s - loss: 18.8904 - mae: 18.8904 - val_loss: 18.6972 - val_mae: 18.6972 - 55ms/epoch - 28ms/step\n",
      "Epoch 84/100\n",
      "2/2 - 0s - loss: 21.2434 - mae: 21.2434 - val_loss: 42.5332 - val_mae: 42.5332 - 47ms/epoch - 24ms/step\n",
      "Epoch 85/100\n",
      "2/2 - 0s - loss: 39.6001 - mae: 39.6001 - val_loss: 18.8385 - val_mae: 18.8385 - 58ms/epoch - 29ms/step\n",
      "Epoch 86/100\n",
      "2/2 - 0s - loss: 23.1227 - mae: 23.1227 - val_loss: 13.1907 - val_mae: 13.1907 - 46ms/epoch - 23ms/step\n",
      "Epoch 87/100\n",
      "2/2 - 0s - loss: 14.0929 - mae: 14.0929 - val_loss: 20.0272 - val_mae: 20.0272 - 53ms/epoch - 27ms/step\n",
      "Epoch 88/100\n",
      "2/2 - 0s - loss: 19.3532 - mae: 19.3532 - val_loss: 7.4879 - val_mae: 7.4879 - 40ms/epoch - 20ms/step\n",
      "Epoch 89/100\n",
      "2/2 - 0s - loss: 8.6947 - mae: 8.6947 - val_loss: 12.0914 - val_mae: 12.0914 - 55ms/epoch - 28ms/step\n",
      "Epoch 90/100\n",
      "2/2 - 0s - loss: 16.7868 - mae: 16.7868 - val_loss: 7.0506 - val_mae: 7.0506 - 57ms/epoch - 29ms/step\n",
      "Epoch 91/100\n",
      "2/2 - 0s - loss: 10.0106 - mae: 10.0106 - val_loss: 20.4651 - val_mae: 20.4651 - 57ms/epoch - 29ms/step\n",
      "Epoch 92/100\n",
      "2/2 - 0s - loss: 23.7325 - mae: 23.7325 - val_loss: 22.2657 - val_mae: 22.2657 - 47ms/epoch - 23ms/step\n",
      "Epoch 93/100\n",
      "2/2 - 0s - loss: 19.8397 - mae: 19.8397 - val_loss: 16.5585 - val_mae: 16.5585 - 72ms/epoch - 36ms/step\n",
      "Epoch 94/100\n",
      "2/2 - 0s - loss: 13.5399 - mae: 13.5399 - val_loss: 6.3398 - val_mae: 6.3398 - 51ms/epoch - 26ms/step\n",
      "Epoch 95/100\n",
      "2/2 - 0s - loss: 8.6418 - mae: 8.6418 - val_loss: 6.9250 - val_mae: 6.9250 - 67ms/epoch - 34ms/step\n",
      "Epoch 96/100\n",
      "2/2 - 0s - loss: 8.5972 - mae: 8.5972 - val_loss: 21.5465 - val_mae: 21.5465 - 50ms/epoch - 25ms/step\n",
      "Epoch 97/100\n",
      "2/2 - 0s - loss: 25.1709 - mae: 25.1709 - val_loss: 6.3764 - val_mae: 6.3764 - 60ms/epoch - 30ms/step\n",
      "Epoch 98/100\n",
      "2/2 - 0s - loss: 8.9929 - mae: 8.9929 - val_loss: 11.7024 - val_mae: 11.7024 - 67ms/epoch - 34ms/step\n",
      "Epoch 99/100\n",
      "2/2 - 0s - loss: 15.6724 - mae: 15.6724 - val_loss: 6.3731 - val_mae: 6.3731 - 53ms/epoch - 26ms/step\n",
      "Epoch 100/100\n",
      "2/2 - 0s - loss: 8.8148 - mae: 8.8148 - val_loss: 11.2143 - val_mae: 11.2143 - 60ms/epoch - 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bcfff80a30>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets fit our model to the training data\n",
    "model.fit(X_train,y_train, validation_data=(X_val, y_val), epochs = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a model that builds automatically by defining the input_shape argument\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10,input_shape = (1,), activation = \"relu\", name = \"hidden_layer\"),\n",
    "    tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "], name = \"model_1\")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer (Dense)        (None, 10)                20        \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 - 0s - loss: 118.8339 - mae: 118.8339 - val_loss: 64.7255 - val_mae: 64.7255 - 377ms/epoch - 188ms/step\n",
      "Epoch 2/100\n",
      "2/2 - 0s - loss: 72.4998 - mae: 72.4998 - val_loss: 22.4055 - val_mae: 22.4055 - 40ms/epoch - 20ms/step\n",
      "Epoch 3/100\n",
      "2/2 - 0s - loss: 26.6928 - mae: 26.6928 - val_loss: 14.2413 - val_mae: 14.2413 - 40ms/epoch - 20ms/step\n",
      "Epoch 4/100\n",
      "2/2 - 0s - loss: 17.9859 - mae: 17.9859 - val_loss: 10.7150 - val_mae: 10.7150 - 70ms/epoch - 35ms/step\n",
      "Epoch 5/100\n",
      "2/2 - 0s - loss: 11.2340 - mae: 11.2340 - val_loss: 12.2345 - val_mae: 12.2345 - 55ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "2/2 - 0s - loss: 12.3554 - mae: 12.3554 - val_loss: 19.0321 - val_mae: 19.0321 - 36ms/epoch - 18ms/step\n",
      "Epoch 7/100\n",
      "2/2 - 0s - loss: 22.3680 - mae: 22.3680 - val_loss: 18.6058 - val_mae: 18.6058 - 38ms/epoch - 19ms/step\n",
      "Epoch 8/100\n",
      "2/2 - 0s - loss: 20.4914 - mae: 20.4914 - val_loss: 22.5020 - val_mae: 22.5020 - 49ms/epoch - 24ms/step\n",
      "Epoch 9/100\n",
      "2/2 - 0s - loss: 23.4925 - mae: 23.4925 - val_loss: 20.4313 - val_mae: 20.4313 - 33ms/epoch - 16ms/step\n",
      "Epoch 10/100\n",
      "2/2 - 0s - loss: 25.4870 - mae: 25.4870 - val_loss: 23.6705 - val_mae: 23.6705 - 38ms/epoch - 19ms/step\n",
      "Epoch 11/100\n",
      "2/2 - 0s - loss: 28.3565 - mae: 28.3565 - val_loss: 11.0046 - val_mae: 11.0046 - 36ms/epoch - 18ms/step\n",
      "Epoch 12/100\n",
      "2/2 - 0s - loss: 13.0493 - mae: 13.0493 - val_loss: 17.8856 - val_mae: 17.8856 - 37ms/epoch - 19ms/step\n",
      "Epoch 13/100\n",
      "2/2 - 0s - loss: 19.0164 - mae: 19.0164 - val_loss: 24.6863 - val_mae: 24.6863 - 58ms/epoch - 29ms/step\n",
      "Epoch 14/100\n",
      "2/2 - 0s - loss: 28.0951 - mae: 28.0951 - val_loss: 10.5980 - val_mae: 10.5980 - 44ms/epoch - 22ms/step\n",
      "Epoch 15/100\n",
      "2/2 - 0s - loss: 11.2259 - mae: 11.2259 - val_loss: 21.1987 - val_mae: 21.1987 - 36ms/epoch - 18ms/step\n",
      "Epoch 16/100\n",
      "2/2 - 0s - loss: 23.8946 - mae: 23.8946 - val_loss: 19.4162 - val_mae: 19.4162 - 44ms/epoch - 22ms/step\n",
      "Epoch 17/100\n",
      "2/2 - 0s - loss: 19.9789 - mae: 19.9789 - val_loss: 21.6779 - val_mae: 21.6779 - 43ms/epoch - 21ms/step\n",
      "Epoch 18/100\n",
      "2/2 - 0s - loss: 24.7119 - mae: 24.7119 - val_loss: 14.6725 - val_mae: 14.6725 - 31ms/epoch - 15ms/step\n",
      "Epoch 19/100\n",
      "2/2 - 0s - loss: 17.4357 - mae: 17.4357 - val_loss: 13.2404 - val_mae: 13.2404 - 32ms/epoch - 16ms/step\n",
      "Epoch 20/100\n",
      "2/2 - 0s - loss: 16.5489 - mae: 16.5489 - val_loss: 11.7093 - val_mae: 11.7093 - 35ms/epoch - 18ms/step\n",
      "Epoch 21/100\n",
      "2/2 - 0s - loss: 11.9612 - mae: 11.9612 - val_loss: 12.2344 - val_mae: 12.2344 - 49ms/epoch - 24ms/step\n",
      "Epoch 22/100\n",
      "2/2 - 0s - loss: 14.2690 - mae: 14.2690 - val_loss: 16.0022 - val_mae: 16.0022 - 30ms/epoch - 15ms/step\n",
      "Epoch 23/100\n",
      "2/2 - 0s - loss: 18.6115 - mae: 18.6115 - val_loss: 10.3183 - val_mae: 10.3183 - 31ms/epoch - 15ms/step\n",
      "Epoch 24/100\n",
      "2/2 - 0s - loss: 13.9547 - mae: 13.9547 - val_loss: 19.2430 - val_mae: 19.2430 - 40ms/epoch - 20ms/step\n",
      "Epoch 25/100\n",
      "2/2 - 0s - loss: 23.8016 - mae: 23.8016 - val_loss: 14.3220 - val_mae: 14.3220 - 37ms/epoch - 18ms/step\n",
      "Epoch 26/100\n",
      "2/2 - 0s - loss: 18.2779 - mae: 18.2779 - val_loss: 11.3038 - val_mae: 11.3038 - 44ms/epoch - 22ms/step\n",
      "Epoch 27/100\n",
      "2/2 - 0s - loss: 11.9194 - mae: 11.9194 - val_loss: 11.8568 - val_mae: 11.8568 - 44ms/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "2/2 - 0s - loss: 13.1659 - mae: 13.1659 - val_loss: 11.8608 - val_mae: 11.8608 - 33ms/epoch - 17ms/step\n",
      "Epoch 29/100\n",
      "2/2 - 0s - loss: 12.4750 - mae: 12.4750 - val_loss: 10.8198 - val_mae: 10.8198 - 33ms/epoch - 16ms/step\n",
      "Epoch 30/100\n",
      "2/2 - 0s - loss: 10.9355 - mae: 10.9355 - val_loss: 16.6105 - val_mae: 16.6105 - 35ms/epoch - 18ms/step\n",
      "Epoch 31/100\n",
      "2/2 - 0s - loss: 20.9166 - mae: 20.9166 - val_loss: 19.8180 - val_mae: 19.8180 - 40ms/epoch - 20ms/step\n",
      "Epoch 32/100\n",
      "2/2 - 0s - loss: 25.1899 - mae: 25.1899 - val_loss: 10.4920 - val_mae: 10.4920 - 38ms/epoch - 19ms/step\n",
      "Epoch 33/100\n",
      "2/2 - 0s - loss: 11.1113 - mae: 11.1113 - val_loss: 13.4298 - val_mae: 13.4298 - 30ms/epoch - 15ms/step\n",
      "Epoch 34/100\n",
      "2/2 - 0s - loss: 16.8585 - mae: 16.8585 - val_loss: 15.8095 - val_mae: 15.8095 - 35ms/epoch - 17ms/step\n",
      "Epoch 35/100\n",
      "2/2 - 0s - loss: 19.4629 - mae: 19.4629 - val_loss: 24.5145 - val_mae: 24.5145 - 32ms/epoch - 16ms/step\n",
      "Epoch 36/100\n",
      "2/2 - 0s - loss: 27.7471 - mae: 27.7471 - val_loss: 19.2628 - val_mae: 19.2628 - 31ms/epoch - 15ms/step\n",
      "Epoch 37/100\n",
      "2/2 - 0s - loss: 21.0091 - mae: 21.0091 - val_loss: 9.7627 - val_mae: 9.7627 - 30ms/epoch - 15ms/step\n",
      "Epoch 38/100\n",
      "2/2 - 0s - loss: 10.9134 - mae: 10.9134 - val_loss: 9.7640 - val_mae: 9.7640 - 32ms/epoch - 16ms/step\n",
      "Epoch 39/100\n",
      "2/2 - 0s - loss: 11.0352 - mae: 11.0352 - val_loss: 10.5621 - val_mae: 10.5621 - 26ms/epoch - 13ms/step\n",
      "Epoch 40/100\n",
      "2/2 - 0s - loss: 12.8333 - mae: 12.8333 - val_loss: 12.9952 - val_mae: 12.9952 - 29ms/epoch - 14ms/step\n",
      "Epoch 41/100\n",
      "2/2 - 0s - loss: 13.4925 - mae: 13.4925 - val_loss: 9.7412 - val_mae: 9.7412 - 36ms/epoch - 18ms/step\n",
      "Epoch 42/100\n",
      "2/2 - 0s - loss: 11.3614 - mae: 11.3614 - val_loss: 23.1147 - val_mae: 23.1147 - 34ms/epoch - 17ms/step\n",
      "Epoch 43/100\n",
      "2/2 - 0s - loss: 26.9990 - mae: 26.9990 - val_loss: 13.0483 - val_mae: 13.0483 - 41ms/epoch - 21ms/step\n",
      "Epoch 44/100\n",
      "2/2 - 0s - loss: 16.8761 - mae: 16.8761 - val_loss: 12.9780 - val_mae: 12.9780 - 27ms/epoch - 13ms/step\n",
      "Epoch 45/100\n",
      "2/2 - 0s - loss: 15.6315 - mae: 15.6315 - val_loss: 10.6073 - val_mae: 10.6073 - 35ms/epoch - 17ms/step\n",
      "Epoch 46/100\n",
      "2/2 - 0s - loss: 11.0607 - mae: 11.0607 - val_loss: 14.0778 - val_mae: 14.0778 - 32ms/epoch - 16ms/step\n",
      "Epoch 47/100\n",
      "2/2 - 0s - loss: 16.7716 - mae: 16.7716 - val_loss: 9.6049 - val_mae: 9.6049 - 28ms/epoch - 14ms/step\n",
      "Epoch 48/100\n",
      "2/2 - 0s - loss: 11.1348 - mae: 11.1348 - val_loss: 26.0088 - val_mae: 26.0088 - 29ms/epoch - 15ms/step\n",
      "Epoch 49/100\n",
      "2/2 - 0s - loss: 29.7052 - mae: 29.7052 - val_loss: 10.0206 - val_mae: 10.0206 - 32ms/epoch - 16ms/step\n",
      "Epoch 50/100\n",
      "2/2 - 0s - loss: 11.5177 - mae: 11.5177 - val_loss: 17.4996 - val_mae: 17.4996 - 29ms/epoch - 14ms/step\n",
      "Epoch 51/100\n",
      "2/2 - 0s - loss: 20.3270 - mae: 20.3270 - val_loss: 9.6470 - val_mae: 9.6470 - 31ms/epoch - 16ms/step\n",
      "Epoch 52/100\n",
      "2/2 - 0s - loss: 10.7292 - mae: 10.7292 - val_loss: 16.3492 - val_mae: 16.3492 - 25ms/epoch - 12ms/step\n",
      "Epoch 53/100\n",
      "2/2 - 0s - loss: 19.6332 - mae: 19.6332 - val_loss: 13.9227 - val_mae: 13.9227 - 27ms/epoch - 14ms/step\n",
      "Epoch 54/100\n",
      "2/2 - 0s - loss: 17.2011 - mae: 17.2011 - val_loss: 9.5235 - val_mae: 9.5235 - 24ms/epoch - 12ms/step\n",
      "Epoch 55/100\n",
      "2/2 - 0s - loss: 11.0855 - mae: 11.0855 - val_loss: 27.9063 - val_mae: 27.9063 - 31ms/epoch - 15ms/step\n",
      "Epoch 56/100\n",
      "2/2 - 0s - loss: 31.5856 - mae: 31.5856 - val_loss: 10.0508 - val_mae: 10.0508 - 30ms/epoch - 15ms/step\n",
      "Epoch 57/100\n",
      "2/2 - 0s - loss: 11.3009 - mae: 11.3009 - val_loss: 9.4670 - val_mae: 9.4670 - 30ms/epoch - 15ms/step\n",
      "Epoch 58/100\n",
      "2/2 - 0s - loss: 10.6583 - mae: 10.6583 - val_loss: 9.5192 - val_mae: 9.5192 - 29ms/epoch - 14ms/step\n",
      "Epoch 59/100\n",
      "2/2 - 0s - loss: 10.7271 - mae: 10.7271 - val_loss: 11.0205 - val_mae: 11.0205 - 33ms/epoch - 17ms/step\n",
      "Epoch 60/100\n",
      "2/2 - 0s - loss: 11.2720 - mae: 11.2720 - val_loss: 11.4990 - val_mae: 11.4990 - 36ms/epoch - 18ms/step\n",
      "Epoch 61/100\n",
      "2/2 - 0s - loss: 14.7178 - mae: 14.7178 - val_loss: 10.0389 - val_mae: 10.0389 - 40ms/epoch - 20ms/step\n",
      "Epoch 62/100\n",
      "2/2 - 0s - loss: 13.4129 - mae: 13.4129 - val_loss: 31.7013 - val_mae: 31.7013 - 32ms/epoch - 16ms/step\n",
      "Epoch 63/100\n",
      "2/2 - 0s - loss: 35.0240 - mae: 35.0240 - val_loss: 20.1637 - val_mae: 20.1637 - 30ms/epoch - 15ms/step\n",
      "Epoch 64/100\n",
      "2/2 - 0s - loss: 22.0368 - mae: 22.0368 - val_loss: 23.5945 - val_mae: 23.5945 - 29ms/epoch - 14ms/step\n",
      "Epoch 65/100\n",
      "2/2 - 0s - loss: 25.0446 - mae: 25.0446 - val_loss: 10.1823 - val_mae: 10.1823 - 27ms/epoch - 14ms/step\n",
      "Epoch 66/100\n",
      "2/2 - 0s - loss: 12.9265 - mae: 12.9265 - val_loss: 10.2593 - val_mae: 10.2593 - 28ms/epoch - 14ms/step\n",
      "Epoch 67/100\n",
      "2/2 - 0s - loss: 12.1902 - mae: 12.1902 - val_loss: 15.1864 - val_mae: 15.1864 - 28ms/epoch - 14ms/step\n",
      "Epoch 68/100\n",
      "2/2 - 0s - loss: 17.3468 - mae: 17.3468 - val_loss: 10.0791 - val_mae: 10.0791 - 32ms/epoch - 16ms/step\n",
      "Epoch 69/100\n",
      "2/2 - 0s - loss: 10.7676 - mae: 10.7676 - val_loss: 11.1824 - val_mae: 11.1824 - 28ms/epoch - 14ms/step\n",
      "Epoch 70/100\n",
      "2/2 - 0s - loss: 11.5412 - mae: 11.5412 - val_loss: 9.9480 - val_mae: 9.9480 - 29ms/epoch - 15ms/step\n",
      "Epoch 71/100\n",
      "2/2 - 0s - loss: 10.5646 - mae: 10.5646 - val_loss: 10.9116 - val_mae: 10.9116 - 25ms/epoch - 12ms/step\n",
      "Epoch 72/100\n",
      "2/2 - 0s - loss: 13.4185 - mae: 13.4185 - val_loss: 11.4621 - val_mae: 11.4621 - 30ms/epoch - 15ms/step\n",
      "Epoch 73/100\n",
      "2/2 - 0s - loss: 13.7753 - mae: 13.7753 - val_loss: 23.1728 - val_mae: 23.1728 - 29ms/epoch - 15ms/step\n",
      "Epoch 74/100\n",
      "2/2 - 0s - loss: 24.3479 - mae: 24.3479 - val_loss: 34.6920 - val_mae: 34.6920 - 31ms/epoch - 15ms/step\n",
      "Epoch 75/100\n",
      "2/2 - 0s - loss: 35.5748 - mae: 35.5748 - val_loss: 9.2773 - val_mae: 9.2773 - 26ms/epoch - 13ms/step\n",
      "Epoch 76/100\n",
      "2/2 - 0s - loss: 10.4824 - mae: 10.4824 - val_loss: 9.5706 - val_mae: 9.5706 - 30ms/epoch - 15ms/step\n",
      "Epoch 77/100\n",
      "2/2 - 0s - loss: 10.6234 - mae: 10.6234 - val_loss: 17.0078 - val_mae: 17.0078 - 32ms/epoch - 16ms/step\n",
      "Epoch 78/100\n",
      "2/2 - 0s - loss: 20.9841 - mae: 20.9841 - val_loss: 15.9836 - val_mae: 15.9836 - 35ms/epoch - 17ms/step\n",
      "Epoch 79/100\n",
      "2/2 - 0s - loss: 18.1987 - mae: 18.1987 - val_loss: 13.9273 - val_mae: 13.9273 - 36ms/epoch - 18ms/step\n",
      "Epoch 80/100\n",
      "2/2 - 0s - loss: 17.1022 - mae: 17.1022 - val_loss: 11.9644 - val_mae: 11.9644 - 30ms/epoch - 15ms/step\n",
      "Epoch 81/100\n",
      "2/2 - 0s - loss: 13.2399 - mae: 13.2399 - val_loss: 9.2849 - val_mae: 9.2849 - 29ms/epoch - 15ms/step\n",
      "Epoch 82/100\n",
      "2/2 - 0s - loss: 11.0375 - mae: 11.0375 - val_loss: 9.9470 - val_mae: 9.9470 - 29ms/epoch - 15ms/step\n",
      "Epoch 83/100\n",
      "2/2 - 0s - loss: 12.3678 - mae: 12.3678 - val_loss: 11.0190 - val_mae: 11.0190 - 30ms/epoch - 15ms/step\n",
      "Epoch 84/100\n",
      "2/2 - 0s - loss: 13.6945 - mae: 13.6945 - val_loss: 10.7671 - val_mae: 10.7671 - 28ms/epoch - 14ms/step\n",
      "Epoch 85/100\n",
      "2/2 - 0s - loss: 12.5089 - mae: 12.5089 - val_loss: 15.0898 - val_mae: 15.0898 - 37ms/epoch - 19ms/step\n",
      "Epoch 86/100\n",
      "2/2 - 0s - loss: 17.8598 - mae: 17.8598 - val_loss: 9.9208 - val_mae: 9.9208 - 32ms/epoch - 16ms/step\n",
      "Epoch 87/100\n",
      "2/2 - 0s - loss: 10.4206 - mae: 10.4206 - val_loss: 9.1545 - val_mae: 9.1545 - 32ms/epoch - 16ms/step\n",
      "Epoch 88/100\n",
      "2/2 - 0s - loss: 10.4539 - mae: 10.4539 - val_loss: 12.2718 - val_mae: 12.2718 - 27ms/epoch - 14ms/step\n",
      "Epoch 89/100\n",
      "2/2 - 0s - loss: 14.0246 - mae: 14.0246 - val_loss: 12.8670 - val_mae: 12.8670 - 30ms/epoch - 15ms/step\n",
      "Epoch 90/100\n",
      "2/2 - 0s - loss: 17.0586 - mae: 17.0586 - val_loss: 9.5130 - val_mae: 9.5130 - 35ms/epoch - 18ms/step\n",
      "Epoch 91/100\n",
      "2/2 - 0s - loss: 10.3430 - mae: 10.3430 - val_loss: 9.1851 - val_mae: 9.1851 - 35ms/epoch - 17ms/step\n",
      "Epoch 92/100\n",
      "2/2 - 0s - loss: 10.2251 - mae: 10.2251 - val_loss: 18.1703 - val_mae: 18.1703 - 30ms/epoch - 15ms/step\n",
      "Epoch 93/100\n",
      "2/2 - 0s - loss: 19.8371 - mae: 19.8371 - val_loss: 9.4953 - val_mae: 9.4953 - 30ms/epoch - 15ms/step\n",
      "Epoch 94/100\n",
      "2/2 - 0s - loss: 10.3123 - mae: 10.3123 - val_loss: 9.1799 - val_mae: 9.1799 - 35ms/epoch - 18ms/step\n",
      "Epoch 95/100\n",
      "2/2 - 0s - loss: 11.0320 - mae: 11.0320 - val_loss: 9.1358 - val_mae: 9.1358 - 37ms/epoch - 19ms/step\n",
      "Epoch 96/100\n",
      "2/2 - 0s - loss: 10.1459 - mae: 10.1459 - val_loss: 23.6136 - val_mae: 23.6136 - 26ms/epoch - 13ms/step\n",
      "Epoch 97/100\n",
      "2/2 - 0s - loss: 25.2519 - mae: 25.2519 - val_loss: 13.1011 - val_mae: 13.1011 - 30ms/epoch - 15ms/step\n",
      "Epoch 98/100\n",
      "2/2 - 0s - loss: 15.6688 - mae: 15.6688 - val_loss: 9.0370 - val_mae: 9.0370 - 29ms/epoch - 15ms/step\n",
      "Epoch 99/100\n",
      "2/2 - 0s - loss: 10.4011 - mae: 10.4011 - val_loss: 16.1230 - val_mae: 16.1230 - 37ms/epoch - 19ms/step\n",
      "Epoch 100/100\n",
      "2/2 - 0s - loss: 20.7977 - mae: 20.7977 - val_loss: 16.1216 - val_mae: 16.1216 - 35ms/epoch - 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497d9dcf40>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets fit our model to the training data\n",
    "model.fit(X_train,y_train, validation_data=(X_val, y_val), epochs = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising our Model's predictions\n",
    "To visualise predictions its a good idea to plot them against the ground truth labels.\n",
    "\n",
    "Often you'll see this in the form of y_test or y_true versus y_pred (ground truth vs your model's predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-53.24751  ],\n",
       "       [ -4.239314 ],\n",
       "       [-66.8609   ],\n",
       "       [-31.466093 ],\n",
       "       [ -6.9619923]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([-60.212627 ,  11.738316 , -95.907974 , -29.188904 ,   3.1377392],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you feel like you're going to reuse some kind of functionality in the future its a good idea to turn it into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a plotting function\n",
    "def plot_predictions(\n",
    "    train_data = X_train,\n",
    "    train_labels = y_train,\n",
    "    test_data = X_test,\n",
    "    test_labels = y_test,\n",
    "    predictions = y_pred\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots training data, test data and compares predictions to ground truth\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,7))\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c =\"b\", label = \"Training data\")\n",
    "    # Plot testing data in green\n",
    "    plt.scatter(test_data, test_labels, c = \"g\", label = \"Testing data\")\n",
    "    # Plot model's predictions in red\n",
    "    plt.scatter(test_data, predictions, c = \"r\", label = \"Predictions\")\n",
    "    # Show the legend\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Model's Predictions with Regression Evaluation Metrics\n",
    "\n",
    "Depending on the problem you're working on, there will be different evaluation metrics to evaluate your model's performance. Since we're working on a regression, two of the main metrics:\n",
    "* MAE - mean absolute error, \"on average, how wrong is each of the model's predictions\"\n",
    "* MSE - mean squared error, \"square the average errors\"\n",
    "\n",
    "|Metric Name|Metric Formula|Tensorflow Code|When to use|\n",
    "|-----------|--------------|---------------|-----------|\n",
    "|Mean absolute error (MAE)|$MAE=\\frac{\\sum_{i=1}^nabs(y_i - x_i)}{n}$|tf.keras.losses.MAE() or tf.metrics.mean_absolute_error()|As a great starter metric for any regression problem|\n",
    "|Mean square error (MSE)|$MSE=\\frac{\\sum_{i=1}^n(y_i - x_i)^2}{n}$|tf.keras.losses.MSE() or tf.metrics.mean_square_error()|When larger errors are more significant than smaller errors|\n",
    "|Huber|$L_g(y, f(x)) = \\frac{(y-f(x))^2}{2} ? abs(y-f(x)) \\le \\delta; \\delta abs(y-f(x)) - \\frac{\\delta^2}{2} otherwise $|tf.keras.losses.Huber()|Combination of MSE and MAE. Less sensitive to outliers than MSE|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 16.1216 - mae: 16.1216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16.121566772460938, 16.121566772460938]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.873349>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean absolute error\n",
    "\n",
    "y_pred, y_test\n",
    "mae = tf.keras.losses.MAE(y_test, tf.squeeze(tf.constant(y_pred)))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=250.94405>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean squared error\n",
    "mse = tf.keras.losses.MSE(y_test, tf.squeeze(tf.constant(y_pred)))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some functions to reuse MAE and MSE\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.keras.losses.MAE(y_true, tf.squeeze(tf.constant(y_pred)))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true, tf.squeeze(tf.constant(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments to improve our model\n",
    "\n",
    "```\n",
    "Build a model -> fit it -> evaluate it -> tweak it -> fit it -> evaluate it -> tweak it ....\n",
    "```\n",
    "\n",
    "Machine Learning Explorer's Motto:\n",
    "\n",
    "> \"Visualise, visualise, visualise\"\n",
    "\n",
    "Machine Learning Practitioner's Motto:\n",
    "\n",
    "> \"Experiment, experiment, experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get more data - get more examples for your model to train on (more opportunities to learn patterns or relationships between features and labels)\n",
    "2. Make your model larger (using a more complex model) - this might come in the form of more layers or more hidden units in each layer\n",
    "3. Train for longer - give your model more of a chance to find patterns in the data\n",
    "\n",
    "Lets do 3 modelling experiments:\n",
    "1. `model_1` - same as the original model, 1 layer, trained for 100 epochs\n",
    "2. `model_2` - 2 layers, trained for 100 epochs\n",
    "3. `model_3` - 2 layers, trained for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 26.4073 - mae: 26.4073 - val_loss: 20.0428 - val_mae: 20.0428\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 25.4800 - mae: 25.4800 - val_loss: 19.1579 - val_mae: 19.1579\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 24.5592 - mae: 24.5592 - val_loss: 18.3908 - val_mae: 18.3908\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 23.6680 - mae: 23.6680 - val_loss: 17.6559 - val_mae: 17.6559\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 22.7454 - mae: 22.7454 - val_loss: 16.9095 - val_mae: 16.9095\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 21.8626 - mae: 21.8626 - val_loss: 16.1556 - val_mae: 16.1556\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 20.9029 - mae: 20.9029 - val_loss: 15.3932 - val_mae: 15.3932\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 19.9968 - mae: 19.9968 - val_loss: 14.6443 - val_mae: 14.6443\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 19.0654 - mae: 19.0654 - val_loss: 13.9784 - val_mae: 13.9784\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18.1167 - mae: 18.1167 - val_loss: 13.3192 - val_mae: 13.3192\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 17.1974 - mae: 17.1974 - val_loss: 12.6681 - val_mae: 12.6681\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 16.2352 - mae: 16.2352 - val_loss: 12.0113 - val_mae: 12.0113\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 15.3677 - mae: 15.3677 - val_loss: 11.3760 - val_mae: 11.3760\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 14.4784 - mae: 14.4784 - val_loss: 10.9394 - val_mae: 10.9394\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 13.8282 - mae: 13.8282 - val_loss: 10.7144 - val_mae: 10.7144\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 13.2136 - mae: 13.2136 - val_loss: 10.4896 - val_mae: 10.4896\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 12.6390 - mae: 12.6390 - val_loss: 10.2671 - val_mae: 10.2671\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 12.2003 - mae: 12.2003 - val_loss: 10.0503 - val_mae: 10.0503\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 11.7661 - mae: 11.7661 - val_loss: 9.9983 - val_mae: 9.9983\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 11.4602 - mae: 11.4602 - val_loss: 9.9571 - val_mae: 9.9571\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 11.2552 - mae: 11.2552 - val_loss: 9.9175 - val_mae: 9.9175\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 11.1169 - mae: 11.1169 - val_loss: 9.8788 - val_mae: 9.8788\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 11.0611 - mae: 11.0611 - val_loss: 9.8396 - val_mae: 9.8396\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 11.0446 - mae: 11.0446 - val_loss: 9.8030 - val_mae: 9.8030\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 10.9642 - mae: 10.9642 - val_loss: 9.7706 - val_mae: 9.7706\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10.9188 - mae: 10.9188 - val_loss: 9.7406 - val_mae: 9.7406\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 10.8893 - mae: 10.8893 - val_loss: 9.7110 - val_mae: 9.7110\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 10.8634 - mae: 10.8634 - val_loss: 9.6811 - val_mae: 9.6811\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 10.8399 - mae: 10.8399 - val_loss: 9.6509 - val_mae: 9.6509\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10.8266 - mae: 10.8266 - val_loss: 9.6228 - val_mae: 9.6228\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 10.7941 - mae: 10.7941 - val_loss: 9.5970 - val_mae: 9.5970\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 10.7729 - mae: 10.7729 - val_loss: 9.5712 - val_mae: 9.5712\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 10.7488 - mae: 10.7488 - val_loss: 9.5450 - val_mae: 9.5450\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 10.7236 - mae: 10.7236 - val_loss: 9.5172 - val_mae: 9.5172\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 10.6946 - mae: 10.6946 - val_loss: 9.4872 - val_mae: 9.4872\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 10.6708 - mae: 10.6708 - val_loss: 9.5167 - val_mae: 9.5167\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 10.6559 - mae: 10.6559 - val_loss: 9.5837 - val_mae: 9.5837\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 10.6305 - mae: 10.6305 - val_loss: 9.6343 - val_mae: 9.6343\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 10.6098 - mae: 10.6098 - val_loss: 9.6667 - val_mae: 9.6667\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 10.5812 - mae: 10.5812 - val_loss: 9.6888 - val_mae: 9.6888\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 10.5711 - mae: 10.5711 - val_loss: 9.6868 - val_mae: 9.6868\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 10.5391 - mae: 10.5391 - val_loss: 9.6412 - val_mae: 9.6412\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 10.5187 - mae: 10.5187 - val_loss: 9.5749 - val_mae: 9.5749\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 10.5051 - mae: 10.5051 - val_loss: 9.5101 - val_mae: 9.5101\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 10.4901 - mae: 10.4901 - val_loss: 9.4612 - val_mae: 9.4612\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 10.4719 - mae: 10.4719 - val_loss: 9.4345 - val_mae: 9.4345\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 10.4511 - mae: 10.4511 - val_loss: 9.4225 - val_mae: 9.4225\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.4341 - mae: 10.4341 - val_loss: 9.3925 - val_mae: 9.3925\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 10.4119 - mae: 10.4119 - val_loss: 9.3411 - val_mae: 9.3411\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 10.3975 - mae: 10.3975 - val_loss: 9.3036 - val_mae: 9.3036\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 10.3758 - mae: 10.3758 - val_loss: 9.2800 - val_mae: 9.2800\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 10.3569 - mae: 10.3569 - val_loss: 9.2669 - val_mae: 9.2669\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 10.3366 - mae: 10.3366 - val_loss: 9.2661 - val_mae: 9.2661\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 10.3155 - mae: 10.3155 - val_loss: 9.2694 - val_mae: 9.2694\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 10.3022 - mae: 10.3022 - val_loss: 9.2516 - val_mae: 9.2516\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 10.2748 - mae: 10.2748 - val_loss: 9.1987 - val_mae: 9.1987\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.2683 - mae: 10.2683 - val_loss: 9.1591 - val_mae: 9.1591\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 10.2419 - mae: 10.2419 - val_loss: 9.1414 - val_mae: 9.1414\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 10.2227 - mae: 10.2227 - val_loss: 9.1268 - val_mae: 9.1268\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 10.2033 - mae: 10.2033 - val_loss: 9.1146 - val_mae: 9.1146\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 10.1832 - mae: 10.1832 - val_loss: 9.1062 - val_mae: 9.1062\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 10.1658 - mae: 10.1658 - val_loss: 9.0933 - val_mae: 9.0933\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 10.1435 - mae: 10.1435 - val_loss: 9.0677 - val_mae: 9.0677\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 10.1255 - mae: 10.1255 - val_loss: 9.0635 - val_mae: 9.0635\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 10.1069 - mae: 10.1069 - val_loss: 9.0700 - val_mae: 9.0700\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 10.0861 - mae: 10.0861 - val_loss: 9.0545 - val_mae: 9.0545\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 10.0667 - mae: 10.0667 - val_loss: 9.0334 - val_mae: 9.0334\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 10.0484 - mae: 10.0484 - val_loss: 9.0257 - val_mae: 9.0257\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 10.0292 - mae: 10.0292 - val_loss: 9.0302 - val_mae: 9.0302\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 10.0121 - mae: 10.0121 - val_loss: 9.0340 - val_mae: 9.0340\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 9.9923 - mae: 9.9923 - val_loss: 9.0173 - val_mae: 9.0173\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 9.9727 - mae: 9.9727 - val_loss: 8.9932 - val_mae: 8.9932\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 9.9546 - mae: 9.9546 - val_loss: 8.9841 - val_mae: 8.9841\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 9.9325 - mae: 9.9325 - val_loss: 9.0094 - val_mae: 9.0094\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 9.9214 - mae: 9.9214 - val_loss: 9.0449 - val_mae: 9.0449\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.8933 - mae: 9.8933 - val_loss: 9.0613 - val_mae: 9.0613\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 9.8794 - mae: 9.8794 - val_loss: 9.0579 - val_mae: 9.0579\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 9.8555 - mae: 9.8555 - val_loss: 9.0426 - val_mae: 9.0426\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 9.8362 - mae: 9.8362 - val_loss: 9.0489 - val_mae: 9.0489\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 9.8172 - mae: 9.8172 - val_loss: 9.0677 - val_mae: 9.0677\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.7999 - mae: 9.7999 - val_loss: 9.0775 - val_mae: 9.0775\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 9.7798 - mae: 9.7798 - val_loss: 9.0684 - val_mae: 9.0684\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 9.7615 - mae: 9.7615 - val_loss: 9.0494 - val_mae: 9.0494\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 9.7441 - mae: 9.7441 - val_loss: 9.0405 - val_mae: 9.0405\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 9.7255 - mae: 9.7255 - val_loss: 9.0603 - val_mae: 9.0603\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 9.7002 - mae: 9.7002 - val_loss: 9.1165 - val_mae: 9.1165\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 9.6867 - mae: 9.6867 - val_loss: 9.1830 - val_mae: 9.1830\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 9.6997 - mae: 9.6997 - val_loss: 9.2160 - val_mae: 9.2160\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.7129 - mae: 9.7129 - val_loss: 9.2460 - val_mae: 9.2460\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.7376 - mae: 9.7376 - val_loss: 9.2905 - val_mae: 9.2905\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.7645 - mae: 9.7645 - val_loss: 9.2672 - val_mae: 9.2672\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.7297 - mae: 9.7297 - val_loss: 9.1833 - val_mae: 9.1833\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 9.6735 - mae: 9.6735 - val_loss: 9.0923 - val_mae: 9.0923\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.6031 - mae: 9.6031 - val_loss: 9.0095 - val_mae: 9.0095\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.5497 - mae: 9.5497 - val_loss: 8.9217 - val_mae: 8.9217\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 9.5525 - mae: 9.5525 - val_loss: 8.8598 - val_mae: 8.8598\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 9.5295 - mae: 9.5295 - val_loss: 8.8366 - val_mae: 8.8366\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 9.5121 - mae: 9.5121 - val_loss: 8.8297 - val_mae: 8.8297\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 9.4990 - mae: 9.4990 - val_loss: 8.8122 - val_mae: 8.8122\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.4787 - mae: 9.4787 - val_loss: 8.7619 - val_mae: 8.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497eb6c3a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(1, activation = None, input_shape = (1, ), name = \"output_layer\")\n",
    "    ], name = 'model_1'\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = ['mae']\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make and plot predictions for model_1\n",
    "y_preds_1 = model.predict(X_test)\n",
    "plot_predictions(predictions = y_preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=16.769896>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=306.29105>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_1 = mae(y_test, y_preds_1)\n",
    "mse_1 = mse(y_test, y_preds_1)\n",
    "mae_1,mse_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 48.9170 - mae: 48.9170 - val_loss: 30.0751 - val_mae: 30.0751\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 35.1800 - mae: 35.1800 - val_loss: 17.3734 - val_mae: 17.3734\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 21.3764 - mae: 21.3764 - val_loss: 6.3092 - val_mae: 6.3092\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 8.4734 - mae: 8.4734 - val_loss: 8.8267 - val_mae: 8.8267\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 11.3824 - mae: 11.3824 - val_loss: 14.1676 - val_mae: 14.1676\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 16.2367 - mae: 16.2367 - val_loss: 12.7001 - val_mae: 12.7001\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 14.5700 - mae: 14.5700 - val_loss: 8.8944 - val_mae: 8.8944\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12.0028 - mae: 12.0028 - val_loss: 5.7233 - val_mae: 5.7233\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10.1785 - mae: 10.1785 - val_loss: 5.7765 - val_mae: 5.7765\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 8.2552 - mae: 8.2552 - val_loss: 6.7994 - val_mae: 6.7994\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 9.6232 - mae: 9.6232 - val_loss: 8.5537 - val_mae: 8.5537\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 10.6407 - mae: 10.6407 - val_loss: 10.0930 - val_mae: 10.0930\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 11.4048 - mae: 11.4048 - val_loss: 8.7935 - val_mae: 8.7935\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.7438 - mae: 9.7438 - val_loss: 6.1834 - val_mae: 6.1834\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 7.2700 - mae: 7.2700 - val_loss: 4.6946 - val_mae: 4.6946\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6.6516 - mae: 6.6516 - val_loss: 4.6011 - val_mae: 4.6011\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 8.8518 - mae: 8.8518 - val_loss: 5.2828 - val_mae: 5.2828\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 9.1889 - mae: 9.1889 - val_loss: 4.4279 - val_mae: 4.4279\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 8.0146 - mae: 8.0146 - val_loss: 4.4804 - val_mae: 4.4804\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6.8654 - mae: 6.8654 - val_loss: 5.6847 - val_mae: 5.6847\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.3463 - mae: 6.3463 - val_loss: 6.4738 - val_mae: 6.4738\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.7427 - mae: 6.7427 - val_loss: 6.3109 - val_mae: 6.3109\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 7.1759 - mae: 7.1759 - val_loss: 5.9320 - val_mae: 5.9320\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6.6674 - mae: 6.6674 - val_loss: 5.0261 - val_mae: 5.0261\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.0380 - mae: 6.0380 - val_loss: 4.6623 - val_mae: 4.6623\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.8186 - mae: 6.8186 - val_loss: 4.5450 - val_mae: 4.5450\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 7.9554 - mae: 7.9554 - val_loss: 4.3041 - val_mae: 4.3041\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 8.1073 - mae: 8.1073 - val_loss: 4.0556 - val_mae: 4.0556\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 7.3105 - mae: 7.3105 - val_loss: 4.6297 - val_mae: 4.6297\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 6.1655 - mae: 6.1655 - val_loss: 6.0669 - val_mae: 6.0669\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.2809 - mae: 6.2809 - val_loss: 7.5921 - val_mae: 7.5921\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8.2143 - mae: 8.2143 - val_loss: 7.6732 - val_mae: 7.6732\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 8.8420 - mae: 8.8420 - val_loss: 6.5149 - val_mae: 6.5149\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 7.9335 - mae: 7.9335 - val_loss: 5.3938 - val_mae: 5.3938\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.2623 - mae: 6.2623 - val_loss: 4.3863 - val_mae: 4.3863\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.2640 - mae: 6.2640 - val_loss: 4.7917 - val_mae: 4.7917\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 8.2800 - mae: 8.2800 - val_loss: 6.2911 - val_mae: 6.2911\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 9.5134 - mae: 9.5134 - val_loss: 7.2484 - val_mae: 7.2484\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 9.9571 - mae: 9.9571 - val_loss: 7.3794 - val_mae: 7.3794\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9.8100 - mae: 9.8100 - val_loss: 6.2761 - val_mae: 6.2761\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 8.8748 - mae: 8.8748 - val_loss: 4.5479 - val_mae: 4.5479\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.3224 - mae: 7.3224 - val_loss: 5.1389 - val_mae: 5.1389\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.0906 - mae: 6.0906 - val_loss: 6.7471 - val_mae: 6.7471\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.8497 - mae: 6.8497 - val_loss: 7.7752 - val_mae: 7.7752\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 8.1393 - mae: 8.1393 - val_loss: 7.4250 - val_mae: 7.4250\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.6259 - mae: 7.6259 - val_loss: 5.7226 - val_mae: 5.7226\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.1112 - mae: 6.1112 - val_loss: 4.5903 - val_mae: 4.5903\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 5.9804 - mae: 5.9804 - val_loss: 4.4446 - val_mae: 4.4446\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.1639 - mae: 7.1639 - val_loss: 4.4518 - val_mae: 4.4518\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 7.3682 - mae: 7.3682 - val_loss: 4.5323 - val_mae: 4.5323\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 4.7141 - val_mae: 4.7141\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.9849 - mae: 5.9849 - val_loss: 5.1159 - val_mae: 5.1159\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.1466 - mae: 6.1466 - val_loss: 5.4232 - val_mae: 5.4232\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.3792 - mae: 6.3792 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.3134 - mae: 6.3134 - val_loss: 5.2228 - val_mae: 5.2228\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.0365 - mae: 6.0365 - val_loss: 4.9721 - val_mae: 4.9721\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.8925 - mae: 5.8925 - val_loss: 5.1421 - val_mae: 5.1421\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.8394 - mae: 5.8394 - val_loss: 5.3208 - val_mae: 5.3208\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.8355 - mae: 5.8355 - val_loss: 5.4494 - val_mae: 5.4494\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.2547 - mae: 6.2547 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 5.0305 - val_mae: 5.0305\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.0662 - mae: 6.0662 - val_loss: 4.8287 - val_mae: 4.8287\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.9408 - mae: 6.9408 - val_loss: 4.7979 - val_mae: 4.7979\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 8.3557 - mae: 8.3557 - val_loss: 4.8949 - val_mae: 4.8949\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 8.1828 - mae: 8.1828 - val_loss: 5.0451 - val_mae: 5.0451\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 7.0640 - mae: 7.0640 - val_loss: 4.9385 - val_mae: 4.9385\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.1923 - mae: 6.1923 - val_loss: 5.5874 - val_mae: 5.5874\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6.1034 - mae: 6.1034 - val_loss: 6.9171 - val_mae: 6.9171\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 6.9212 - mae: 6.9212 - val_loss: 7.2269 - val_mae: 7.2269\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 7.8923 - mae: 7.8923 - val_loss: 6.0135 - val_mae: 6.0135\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 7.6747 - mae: 7.6747 - val_loss: 4.5512 - val_mae: 4.5512\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 7.0100 - mae: 7.0100 - val_loss: 4.1061 - val_mae: 4.1061\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 7.1203 - mae: 7.1203 - val_loss: 3.9312 - val_mae: 3.9312\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 6.9148 - mae: 6.9148 - val_loss: 4.3364 - val_mae: 4.3364\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.4613 - mae: 6.4613 - val_loss: 4.8124 - val_mae: 4.8124\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.8529 - mae: 5.8529 - val_loss: 5.4392 - val_mae: 5.4392\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.2120 - mae: 6.2120 - val_loss: 5.8063 - val_mae: 5.8063\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 7.1301 - mae: 7.1301 - val_loss: 5.8724 - val_mae: 5.8724\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.9493 - mae: 6.9493 - val_loss: 5.5437 - val_mae: 5.5437\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 5.8889 - mae: 5.8889 - val_loss: 5.0598 - val_mae: 5.0598\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.1660 - mae: 6.1660 - val_loss: 5.4264 - val_mae: 5.4264\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.5037 - mae: 7.5037 - val_loss: 6.0319 - val_mae: 6.0319\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 8.1850 - mae: 8.1850 - val_loss: 5.4039 - val_mae: 5.4039\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 7.7611 - mae: 7.7611 - val_loss: 4.3062 - val_mae: 4.3062\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.3601 - mae: 6.3601 - val_loss: 4.9511 - val_mae: 4.9511\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.6208 - mae: 5.6208 - val_loss: 5.5269 - val_mae: 5.5269\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.6550 - mae: 6.6550 - val_loss: 5.7836 - val_mae: 5.7836\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.3101 - mae: 7.3101 - val_loss: 5.6079 - val_mae: 5.6079\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.7560 - mae: 6.7560 - val_loss: 4.9515 - val_mae: 4.9515\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.7513 - mae: 5.7513 - val_loss: 4.0867 - val_mae: 4.0867\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.7476 - mae: 6.7476 - val_loss: 4.5734 - val_mae: 4.5734\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.6530 - mae: 7.6530 - val_loss: 4.6551 - val_mae: 4.6551\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.3583 - mae: 7.3583 - val_loss: 4.4336 - val_mae: 4.4336\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.6141 - mae: 6.6141 - val_loss: 5.2215 - val_mae: 5.2215\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.7725 - mae: 5.7725 - val_loss: 5.5899 - val_mae: 5.5899\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.9679 - mae: 5.9679 - val_loss: 5.7327 - val_mae: 5.7327\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.5845 - mae: 6.5845 - val_loss: 5.5771 - val_mae: 5.5771\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.3526 - mae: 6.3526 - val_loss: 4.9853 - val_mae: 4.9853\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.8332 - mae: 5.8332 - val_loss: 4.3508 - val_mae: 4.3508\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.0349 - mae: 6.0349 - val_loss: 4.2080 - val_mae: 4.2080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2498010d690>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", input_shape = (1, ), name = \"hidden_layer\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = 'model_2'\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = ['mae']\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds_2 = model.predict(X_test)\n",
    "plot_predictions(predictions = y_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=11.347539>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=138.56165>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_2 = mae(y_test, y_preds_2)\n",
    "mse_2 = mse(y_test, y_preds_2)\n",
    "mae_2,mse_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a substantial reduction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 176ms/step - loss: 48.9170 - mae: 48.9170 - val_loss: 30.0751 - val_mae: 30.0751\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 35.1800 - mae: 35.1800 - val_loss: 17.3734 - val_mae: 17.3734\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 94ms/step - loss: 21.3764 - mae: 21.3764 - val_loss: 6.3092 - val_mae: 6.3092\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 8.4734 - mae: 8.4734 - val_loss: 8.8267 - val_mae: 8.8267\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 11.3824 - mae: 11.3824 - val_loss: 14.1676 - val_mae: 14.1676\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 16.2367 - mae: 16.2367 - val_loss: 12.7001 - val_mae: 12.7001\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 14.5700 - mae: 14.5700 - val_loss: 8.8944 - val_mae: 8.8944\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 12.0028 - mae: 12.0028 - val_loss: 5.7233 - val_mae: 5.7233\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 10.1785 - mae: 10.1785 - val_loss: 5.7765 - val_mae: 5.7765\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 8.2552 - mae: 8.2552 - val_loss: 6.7994 - val_mae: 6.7994\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 9.6232 - mae: 9.6232 - val_loss: 8.5537 - val_mae: 8.5537\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 10.6407 - mae: 10.6407 - val_loss: 10.0930 - val_mae: 10.0930\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 11.4048 - mae: 11.4048 - val_loss: 8.7935 - val_mae: 8.7935\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.7438 - mae: 9.7438 - val_loss: 6.1834 - val_mae: 6.1834\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 7.2700 - mae: 7.2700 - val_loss: 4.6946 - val_mae: 4.6946\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.6516 - mae: 6.6516 - val_loss: 4.6011 - val_mae: 4.6011\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 8.8518 - mae: 8.8518 - val_loss: 5.2828 - val_mae: 5.2828\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 9.1889 - mae: 9.1889 - val_loss: 4.4279 - val_mae: 4.4279\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 8.0146 - mae: 8.0146 - val_loss: 4.4804 - val_mae: 4.4804\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 6.8654 - mae: 6.8654 - val_loss: 5.6847 - val_mae: 5.6847\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 6.3463 - mae: 6.3463 - val_loss: 6.4738 - val_mae: 6.4738\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.7427 - mae: 6.7427 - val_loss: 6.3109 - val_mae: 6.3109\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 7.1759 - mae: 7.1759 - val_loss: 5.9320 - val_mae: 5.9320\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 6.6674 - mae: 6.6674 - val_loss: 5.0261 - val_mae: 5.0261\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 6.0380 - mae: 6.0380 - val_loss: 4.6623 - val_mae: 4.6623\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 6.8186 - mae: 6.8186 - val_loss: 4.5450 - val_mae: 4.5450\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 7.9554 - mae: 7.9554 - val_loss: 4.3041 - val_mae: 4.3041\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 8.1073 - mae: 8.1073 - val_loss: 4.0556 - val_mae: 4.0556\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 7.3105 - mae: 7.3105 - val_loss: 4.6297 - val_mae: 4.6297\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.1655 - mae: 6.1655 - val_loss: 6.0669 - val_mae: 6.0669\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.2809 - mae: 6.2809 - val_loss: 7.5921 - val_mae: 7.5921\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 8.2143 - mae: 8.2143 - val_loss: 7.6732 - val_mae: 7.6732\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 8.8420 - mae: 8.8420 - val_loss: 6.5149 - val_mae: 6.5149\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 7.9335 - mae: 7.9335 - val_loss: 5.3938 - val_mae: 5.3938\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 6.2623 - mae: 6.2623 - val_loss: 4.3863 - val_mae: 4.3863\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 6.2640 - mae: 6.2640 - val_loss: 4.7917 - val_mae: 4.7917\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 8.2800 - mae: 8.2800 - val_loss: 6.2911 - val_mae: 6.2911\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 9.5134 - mae: 9.5134 - val_loss: 7.2484 - val_mae: 7.2484\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 9.9571 - mae: 9.9571 - val_loss: 7.3794 - val_mae: 7.3794\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 9.8100 - mae: 9.8100 - val_loss: 6.2761 - val_mae: 6.2761\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 8.8748 - mae: 8.8748 - val_loss: 4.5479 - val_mae: 4.5479\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 7.3224 - mae: 7.3224 - val_loss: 5.1389 - val_mae: 5.1389\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 6.0906 - mae: 6.0906 - val_loss: 6.7471 - val_mae: 6.7471\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.8497 - mae: 6.8497 - val_loss: 7.7752 - val_mae: 7.7752\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 8.1393 - mae: 8.1393 - val_loss: 7.4250 - val_mae: 7.4250\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 7.6259 - mae: 7.6259 - val_loss: 5.7226 - val_mae: 5.7226\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 6.1112 - mae: 6.1112 - val_loss: 4.5903 - val_mae: 4.5903\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 5.9804 - mae: 5.9804 - val_loss: 4.4446 - val_mae: 4.4446\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 7.1639 - mae: 7.1639 - val_loss: 4.4518 - val_mae: 4.4518\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 7.3682 - mae: 7.3682 - val_loss: 4.5323 - val_mae: 4.5323\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 4.7141 - val_mae: 4.7141\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.9849 - mae: 5.9849 - val_loss: 5.1159 - val_mae: 5.1159\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 82ms/step - loss: 6.1466 - mae: 6.1466 - val_loss: 5.4232 - val_mae: 5.4232\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 6.3792 - mae: 6.3792 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 6.3134 - mae: 6.3134 - val_loss: 5.2228 - val_mae: 5.2228\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 6.0365 - mae: 6.0365 - val_loss: 4.9721 - val_mae: 4.9721\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.8925 - mae: 5.8925 - val_loss: 5.1421 - val_mae: 5.1421\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5.8394 - mae: 5.8394 - val_loss: 5.3208 - val_mae: 5.3208\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 5.8355 - mae: 5.8355 - val_loss: 5.4494 - val_mae: 5.4494\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.2547 - mae: 6.2547 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 5.0305 - val_mae: 5.0305\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.0662 - mae: 6.0662 - val_loss: 4.8287 - val_mae: 4.8287\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 6.9408 - mae: 6.9408 - val_loss: 4.7979 - val_mae: 4.7979\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 8.3557 - mae: 8.3557 - val_loss: 4.8949 - val_mae: 4.8949\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 8.1828 - mae: 8.1828 - val_loss: 5.0451 - val_mae: 5.0451\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 7.0640 - mae: 7.0640 - val_loss: 4.9385 - val_mae: 4.9385\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 6.1923 - mae: 6.1923 - val_loss: 5.5874 - val_mae: 5.5874\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 6.1034 - mae: 6.1034 - val_loss: 6.9171 - val_mae: 6.9171\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.9212 - mae: 6.9212 - val_loss: 7.2269 - val_mae: 7.2269\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 7.8923 - mae: 7.8923 - val_loss: 6.0135 - val_mae: 6.0135\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 7.6747 - mae: 7.6747 - val_loss: 4.5512 - val_mae: 4.5512\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 7.0100 - mae: 7.0100 - val_loss: 4.1061 - val_mae: 4.1061\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 7.1203 - mae: 7.1203 - val_loss: 3.9312 - val_mae: 3.9312\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 6.9148 - mae: 6.9148 - val_loss: 4.3364 - val_mae: 4.3364\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.4613 - mae: 6.4613 - val_loss: 4.8124 - val_mae: 4.8124\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.8529 - mae: 5.8529 - val_loss: 5.4392 - val_mae: 5.4392\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.2120 - mae: 6.2120 - val_loss: 5.8063 - val_mae: 5.8063\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 7.1301 - mae: 7.1301 - val_loss: 5.8724 - val_mae: 5.8724\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 6.9493 - mae: 6.9493 - val_loss: 5.5437 - val_mae: 5.5437\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.8889 - mae: 5.8889 - val_loss: 5.0598 - val_mae: 5.0598\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 6.1660 - mae: 6.1660 - val_loss: 5.4264 - val_mae: 5.4264\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 7.5037 - mae: 7.5037 - val_loss: 6.0319 - val_mae: 6.0319\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 8.1850 - mae: 8.1850 - val_loss: 5.4039 - val_mae: 5.4039\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 7.7611 - mae: 7.7611 - val_loss: 4.3062 - val_mae: 4.3062\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.3601 - mae: 6.3601 - val_loss: 4.9511 - val_mae: 4.9511\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 5.6208 - mae: 5.6208 - val_loss: 5.5269 - val_mae: 5.5269\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.6550 - mae: 6.6550 - val_loss: 5.7836 - val_mae: 5.7836\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 7.3101 - mae: 7.3101 - val_loss: 5.6079 - val_mae: 5.6079\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.7560 - mae: 6.7560 - val_loss: 4.9515 - val_mae: 4.9515\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.7513 - mae: 5.7513 - val_loss: 4.0867 - val_mae: 4.0867\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.7476 - mae: 6.7476 - val_loss: 4.5734 - val_mae: 4.5734\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 7.6530 - mae: 7.6530 - val_loss: 4.6551 - val_mae: 4.6551\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 7.3583 - mae: 7.3583 - val_loss: 4.4336 - val_mae: 4.4336\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.6141 - mae: 6.6141 - val_loss: 5.2215 - val_mae: 5.2215\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.7725 - mae: 5.7725 - val_loss: 5.5899 - val_mae: 5.5899\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.9679 - mae: 5.9679 - val_loss: 5.7327 - val_mae: 5.7327\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.5845 - mae: 6.5845 - val_loss: 5.5771 - val_mae: 5.5771\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.3526 - mae: 6.3526 - val_loss: 4.9853 - val_mae: 4.9853\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.8332 - mae: 5.8332 - val_loss: 4.3508 - val_mae: 4.3508\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.0349 - mae: 6.0349 - val_loss: 4.2080 - val_mae: 4.2080\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 6.3263 - mae: 6.3263 - val_loss: 4.3384 - val_mae: 4.3384\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 6.2169 - mae: 6.2169 - val_loss: 4.4818 - val_mae: 4.4818\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 5.9955 - mae: 5.9955 - val_loss: 4.4775 - val_mae: 4.4775\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.9257 - mae: 5.9257 - val_loss: 4.4888 - val_mae: 4.4888\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.8485 - mae: 5.8485 - val_loss: 4.6769 - val_mae: 4.6769\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.7487 - mae: 5.7487 - val_loss: 4.7907 - val_mae: 4.7907\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.6981 - mae: 5.6981 - val_loss: 5.0011 - val_mae: 5.0011\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.7950 - mae: 5.7950 - val_loss: 5.3597 - val_mae: 5.3597\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.9402 - mae: 5.9402 - val_loss: 5.6088 - val_mae: 5.6088\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.9932 - mae: 5.9932 - val_loss: 6.1575 - val_mae: 6.1575\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 6.2910 - mae: 6.2910 - val_loss: 6.0590 - val_mae: 6.0590\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 6.0013 - mae: 6.0013 - val_loss: 4.7769 - val_mae: 4.7769\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.7004 - mae: 5.7004 - val_loss: 4.8101 - val_mae: 4.8101\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.6242 - mae: 6.6242 - val_loss: 5.2456 - val_mae: 5.2456\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.9602 - mae: 6.9602 - val_loss: 5.0825 - val_mae: 5.0825\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.5902 - mae: 6.5902 - val_loss: 4.6376 - val_mae: 4.6376\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.8482 - mae: 5.8482 - val_loss: 4.4863 - val_mae: 4.4863\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.8608 - mae: 5.8608 - val_loss: 4.7773 - val_mae: 4.7773\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 6.6886 - mae: 6.6886 - val_loss: 6.1694 - val_mae: 6.1694\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 8.0984 - mae: 8.0984 - val_loss: 6.0983 - val_mae: 6.0983\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 7.9884 - mae: 7.9884 - val_loss: 5.2118 - val_mae: 5.2118\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.3367 - mae: 6.3367 - val_loss: 4.7342 - val_mae: 4.7342\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.5945 - mae: 5.5945 - val_loss: 4.7454 - val_mae: 4.7454\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.8888 - mae: 5.8888 - val_loss: 5.2575 - val_mae: 5.2575\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.4990 - mae: 6.4990 - val_loss: 5.1537 - val_mae: 5.1537\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.3675 - mae: 6.3675 - val_loss: 5.3636 - val_mae: 5.3636\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.9891 - mae: 5.9891 - val_loss: 5.3894 - val_mae: 5.3894\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.6191 - mae: 5.6191 - val_loss: 5.4079 - val_mae: 5.4079\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.6887 - mae: 5.6887 - val_loss: 5.2008 - val_mae: 5.2008\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.6356 - mae: 5.6356 - val_loss: 4.7794 - val_mae: 4.7794\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 5.5028 - mae: 5.5028 - val_loss: 4.7454 - val_mae: 4.7454\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.8228 - mae: 5.8228 - val_loss: 5.0253 - val_mae: 5.0253\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.0750 - mae: 6.0750 - val_loss: 4.9323 - val_mae: 4.9323\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.8491 - mae: 5.8491 - val_loss: 4.9188 - val_mae: 4.9188\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 5.5581 - mae: 5.5581 - val_loss: 5.0682 - val_mae: 5.0682\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.4355 - mae: 5.4355 - val_loss: 5.4332 - val_mae: 5.4332\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.5859 - mae: 5.5859 - val_loss: 6.4631 - val_mae: 6.4631\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.3825 - mae: 6.3825 - val_loss: 6.7289 - val_mae: 6.7289\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 6.5644 - mae: 6.5644 - val_loss: 5.4823 - val_mae: 5.4823\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.6900 - mae: 5.6900 - val_loss: 4.7781 - val_mae: 4.7781\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.5114 - mae: 5.5114 - val_loss: 4.5641 - val_mae: 4.5641\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 5.6631 - mae: 5.6631 - val_loss: 4.5830 - val_mae: 4.5830\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 5.6079 - mae: 5.6079 - val_loss: 4.5806 - val_mae: 4.5806\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 5.4848 - mae: 5.4848 - val_loss: 4.9293 - val_mae: 4.9293\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 5.5594 - mae: 5.5594 - val_loss: 5.4932 - val_mae: 5.4932\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.6873 - mae: 5.6873 - val_loss: 5.4637 - val_mae: 5.4637\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.5486 - mae: 5.5486 - val_loss: 5.3886 - val_mae: 5.3886\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 5.7020 - mae: 5.7020 - val_loss: 5.2289 - val_mae: 5.2289\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.0614 - mae: 6.0614 - val_loss: 4.9911 - val_mae: 4.9911\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.8281 - mae: 5.8281 - val_loss: 4.7620 - val_mae: 4.7620\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.7575 - mae: 5.7575 - val_loss: 4.3457 - val_mae: 4.3457\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.6110 - mae: 5.6110 - val_loss: 4.2382 - val_mae: 4.2382\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 6.0098 - mae: 6.0098 - val_loss: 4.7797 - val_mae: 4.7797\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.6640 - mae: 6.6640 - val_loss: 4.9660 - val_mae: 4.9660\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 6.6965 - mae: 6.6965 - val_loss: 4.3547 - val_mae: 4.3547\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.7617 - mae: 5.7617 - val_loss: 5.0999 - val_mae: 5.0999\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.4454 - mae: 5.4454 - val_loss: 5.6231 - val_mae: 5.6231\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.7893 - mae: 5.7893 - val_loss: 6.1812 - val_mae: 6.1812\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 6.0221 - mae: 6.0221 - val_loss: 6.1495 - val_mae: 6.1495\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 6.4495 - mae: 6.4495 - val_loss: 5.3623 - val_mae: 5.3623\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.5389 - mae: 6.5389 - val_loss: 4.7460 - val_mae: 4.7460\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.3311 - mae: 6.3311 - val_loss: 4.1083 - val_mae: 4.1083\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 6.1623 - mae: 6.1623 - val_loss: 4.5284 - val_mae: 4.5284\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.9229 - mae: 5.9229 - val_loss: 4.7845 - val_mae: 4.7845\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.5786 - mae: 5.5786 - val_loss: 5.0869 - val_mae: 5.0869\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.5674 - mae: 5.5674 - val_loss: 5.2983 - val_mae: 5.2983\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.4466 - mae: 5.4466 - val_loss: 5.3921 - val_mae: 5.3921\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.4583 - mae: 5.4583 - val_loss: 5.5655 - val_mae: 5.5655\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.5870 - mae: 5.5870 - val_loss: 5.4320 - val_mae: 5.4320\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.5012 - mae: 5.5012 - val_loss: 5.1958 - val_mae: 5.1958\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.4557 - mae: 5.4557 - val_loss: 5.1004 - val_mae: 5.1004\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 5.3545 - mae: 5.3545 - val_loss: 5.2119 - val_mae: 5.2119\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.3192 - mae: 5.3192 - val_loss: 5.2746 - val_mae: 5.2746\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.4273 - mae: 5.4273 - val_loss: 5.3599 - val_mae: 5.3599\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.5423 - mae: 5.5423 - val_loss: 4.9706 - val_mae: 4.9706\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 5.5032 - mae: 5.5032 - val_loss: 4.5874 - val_mae: 4.5874\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 5.4014 - mae: 5.4014 - val_loss: 4.2149 - val_mae: 4.2149\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.6099 - mae: 5.6099 - val_loss: 4.7751 - val_mae: 4.7751\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 6.3025 - mae: 6.3025 - val_loss: 5.2774 - val_mae: 5.2774\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 6.7939 - mae: 6.7939 - val_loss: 5.1950 - val_mae: 5.1950\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.6421 - mae: 6.6421 - val_loss: 4.8285 - val_mae: 4.8285\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.6289 - mae: 5.6289 - val_loss: 4.7733 - val_mae: 4.7733\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.3036 - mae: 5.3036 - val_loss: 5.0216 - val_mae: 5.0216\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 5.2232 - mae: 5.2232 - val_loss: 5.0829 - val_mae: 5.0829\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.1326 - mae: 5.1326 - val_loss: 4.8706 - val_mae: 4.8706\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.1592 - mae: 5.1592 - val_loss: 4.6735 - val_mae: 4.6735\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 5.5578 - mae: 5.5578 - val_loss: 4.6983 - val_mae: 4.6983\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.8506 - mae: 5.8506 - val_loss: 4.6367 - val_mae: 4.6367\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.5657 - mae: 5.5657 - val_loss: 4.5272 - val_mae: 4.5272\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.0929 - mae: 5.0929 - val_loss: 4.7475 - val_mae: 4.7475\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.0624 - mae: 5.0624 - val_loss: 4.9389 - val_mae: 4.9389\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.3534 - mae: 5.3534 - val_loss: 5.1139 - val_mae: 5.1139\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.6724 - mae: 5.6724 - val_loss: 5.1587 - val_mae: 5.1587\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.1584 - mae: 5.1584 - val_loss: 5.2776 - val_mae: 5.2776\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.0915 - mae: 5.0915 - val_loss: 5.7029 - val_mae: 5.7029\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.4517 - mae: 5.4517 - val_loss: 5.9039 - val_mae: 5.9039\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.6577 - mae: 5.6577 - val_loss: 5.6598 - val_mae: 5.6598\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.4558 - mae: 5.4558 - val_loss: 5.1190 - val_mae: 5.1190\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.0697 - mae: 5.0697 - val_loss: 4.7519 - val_mae: 4.7519\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.1324 - mae: 5.1324 - val_loss: 4.4882 - val_mae: 4.4882\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.1650 - mae: 5.1650 - val_loss: 5.1366 - val_mae: 5.1366\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.4162 - mae: 5.4162 - val_loss: 4.8390 - val_mae: 4.8390\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 5.0338 - mae: 5.0338 - val_loss: 4.7516 - val_mae: 4.7516\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.7603 - mae: 4.7603 - val_loss: 5.0126 - val_mae: 5.0126\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.9293 - mae: 4.9293 - val_loss: 5.2260 - val_mae: 5.2260\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 4.9451 - mae: 4.9451 - val_loss: 5.6857 - val_mae: 5.6857\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.4415 - mae: 5.4415 - val_loss: 5.7189 - val_mae: 5.7189\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.5450 - mae: 5.5450 - val_loss: 5.0454 - val_mae: 5.0454\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.7588 - mae: 4.7588 - val_loss: 4.7345 - val_mae: 4.7345\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.3047 - mae: 5.3047 - val_loss: 5.7647 - val_mae: 5.7647\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.7365 - mae: 6.7365 - val_loss: 5.8253 - val_mae: 5.8253\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 6.6072 - mae: 6.6072 - val_loss: 4.8698 - val_mae: 4.8698\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.9828 - mae: 4.9828 - val_loss: 5.2599 - val_mae: 5.2599\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 4.8691 - mae: 4.8691 - val_loss: 6.0102 - val_mae: 6.0102\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.8286 - mae: 5.8286 - val_loss: 7.0387 - val_mae: 7.0387\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 6.8423 - mae: 6.8423 - val_loss: 6.9228 - val_mae: 6.9228\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.6418 - mae: 6.6418 - val_loss: 5.5711 - val_mae: 5.5711\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.3139 - mae: 5.3139 - val_loss: 4.8427 - val_mae: 4.8427\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.1644 - mae: 5.1644 - val_loss: 5.3213 - val_mae: 5.3213\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.3412 - mae: 6.3412 - val_loss: 4.9457 - val_mae: 4.9457\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.8544 - mae: 5.8544 - val_loss: 5.1177 - val_mae: 5.1177\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 5.3572 - mae: 5.3572 - val_loss: 5.1343 - val_mae: 5.1343\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.3349 - mae: 5.3349 - val_loss: 4.8741 - val_mae: 4.8741\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.8048 - mae: 4.8048 - val_loss: 4.9733 - val_mae: 4.9733\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.8847 - mae: 4.8847 - val_loss: 5.1132 - val_mae: 5.1132\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.9209 - mae: 4.9209 - val_loss: 5.3855 - val_mae: 5.3855\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 5.0476 - mae: 5.0476 - val_loss: 5.3925 - val_mae: 5.3925\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.5517 - mae: 5.5517 - val_loss: 5.2151 - val_mae: 5.2151\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.4966 - mae: 5.4966 - val_loss: 5.4659 - val_mae: 5.4659\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 5.8325 - mae: 5.8325 - val_loss: 5.2791 - val_mae: 5.2791\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.7394 - mae: 5.7394 - val_loss: 5.2515 - val_mae: 5.2515\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.0734 - mae: 5.0734 - val_loss: 5.0409 - val_mae: 5.0409\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.9026 - mae: 4.9026 - val_loss: 5.1197 - val_mae: 5.1197\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 5.0491 - mae: 5.0491 - val_loss: 5.1875 - val_mae: 5.1875\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.3680 - mae: 5.3680 - val_loss: 5.2424 - val_mae: 5.2424\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.2890 - mae: 5.2890 - val_loss: 5.3547 - val_mae: 5.3547\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.1157 - mae: 5.1157 - val_loss: 5.6666 - val_mae: 5.6666\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.2700 - mae: 5.2700 - val_loss: 5.8092 - val_mae: 5.8092\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 6.2197 - mae: 6.2197 - val_loss: 5.7041 - val_mae: 5.7041\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 6.8385 - mae: 6.8385 - val_loss: 5.9503 - val_mae: 5.9503\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 6.7999 - mae: 6.7999 - val_loss: 5.4644 - val_mae: 5.4644\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 5.6361 - mae: 5.6361 - val_loss: 4.7722 - val_mae: 4.7722\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.8882 - mae: 4.8882 - val_loss: 5.0134 - val_mae: 5.0134\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.7994 - mae: 4.7994 - val_loss: 5.2789 - val_mae: 5.2789\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.0344 - mae: 5.0344 - val_loss: 5.3262 - val_mae: 5.3262\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.1718 - mae: 5.1718 - val_loss: 5.1157 - val_mae: 5.1157\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.8584 - mae: 4.8584 - val_loss: 4.9449 - val_mae: 4.9449\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.8317 - mae: 4.8317 - val_loss: 4.9611 - val_mae: 4.9611\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 4.9682 - mae: 4.9682 - val_loss: 4.9506 - val_mae: 4.9506\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.0590 - mae: 5.0590 - val_loss: 4.8152 - val_mae: 4.8152\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.9097 - mae: 4.9097 - val_loss: 4.8422 - val_mae: 4.8422\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.8167 - mae: 4.8167 - val_loss: 5.0980 - val_mae: 5.0980\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 4.9266 - mae: 4.9266 - val_loss: 5.2212 - val_mae: 5.2212\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.9420 - mae: 4.9420 - val_loss: 5.3253 - val_mae: 5.3253\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.9178 - mae: 4.9178 - val_loss: 5.3460 - val_mae: 5.3460\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.9308 - mae: 4.9308 - val_loss: 5.3701 - val_mae: 5.3701\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.0183 - mae: 5.0183 - val_loss: 5.4076 - val_mae: 5.4076\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5.2116 - mae: 5.2116 - val_loss: 5.4446 - val_mae: 5.4446\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.4010 - mae: 5.4010 - val_loss: 5.3969 - val_mae: 5.3969\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.2239 - mae: 5.2239 - val_loss: 5.2757 - val_mae: 5.2757\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.7841 - mae: 4.7841 - val_loss: 5.2406 - val_mae: 5.2406\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.7575 - mae: 4.7575 - val_loss: 5.2307 - val_mae: 5.2307\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.8471 - mae: 4.8471 - val_loss: 5.0987 - val_mae: 5.0987\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.6582 - mae: 4.6582 - val_loss: 5.0138 - val_mae: 5.0138\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.6613 - mae: 4.6613 - val_loss: 4.9644 - val_mae: 4.9644\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.1041 - mae: 5.1041 - val_loss: 5.0118 - val_mae: 5.0118\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 4.8491 - mae: 4.8491 - val_loss: 5.1209 - val_mae: 5.1209\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.5772 - mae: 4.5772 - val_loss: 5.4096 - val_mae: 5.4096\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.1478 - mae: 5.1478 - val_loss: 5.6801 - val_mae: 5.6801\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.7006 - mae: 5.7006 - val_loss: 5.6286 - val_mae: 5.6286\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 5.5463 - mae: 5.5463 - val_loss: 5.3285 - val_mae: 5.3285\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.6866 - mae: 4.6866 - val_loss: 5.2305 - val_mae: 5.2305\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.6567 - mae: 4.6567 - val_loss: 5.3927 - val_mae: 5.3927\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.4457 - mae: 5.4457 - val_loss: 5.6004 - val_mae: 5.6004\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.5812 - mae: 5.5812 - val_loss: 5.2701 - val_mae: 5.2701\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.7774 - mae: 4.7774 - val_loss: 5.3906 - val_mae: 5.3906\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 4.6591 - mae: 4.6591 - val_loss: 5.4071 - val_mae: 5.4071\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 5.1144 - mae: 5.1144 - val_loss: 5.2968 - val_mae: 5.2968\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.8224 - mae: 4.8224 - val_loss: 5.1105 - val_mae: 5.1105\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.3617 - mae: 5.3617 - val_loss: 6.4024 - val_mae: 6.4024\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.5524 - mae: 6.5524 - val_loss: 6.7793 - val_mae: 6.7793\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.7911 - mae: 6.7911 - val_loss: 5.7109 - val_mae: 5.7109\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.6414 - mae: 5.6414 - val_loss: 5.3099 - val_mae: 5.3099\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.7155 - mae: 4.7155 - val_loss: 5.3687 - val_mae: 5.3687\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 4.5177 - mae: 4.5177 - val_loss: 5.2909 - val_mae: 5.2909\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.7761 - mae: 4.7761 - val_loss: 5.7824 - val_mae: 5.7824\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.4365 - mae: 5.4365 - val_loss: 5.9145 - val_mae: 5.9145\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.5832 - mae: 5.5832 - val_loss: 5.2574 - val_mae: 5.2574\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.7319 - mae: 4.7319 - val_loss: 5.3037 - val_mae: 5.3037\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 4.7649 - mae: 4.7649 - val_loss: 5.5888 - val_mae: 5.5888\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.1006 - mae: 5.1006 - val_loss: 5.9145 - val_mae: 5.9145\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 106ms/step - loss: 5.1559 - mae: 5.1559 - val_loss: 6.0433 - val_mae: 6.0433\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 5.5529 - mae: 5.5529 - val_loss: 5.6676 - val_mae: 5.6676\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 5.0901 - mae: 5.0901 - val_loss: 5.3989 - val_mae: 5.3989\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 4.5967 - mae: 4.5967 - val_loss: 5.2717 - val_mae: 5.2717\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.0645 - mae: 5.0645 - val_loss: 5.4697 - val_mae: 5.4697\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.5000 - mae: 5.5000 - val_loss: 5.4846 - val_mae: 5.4846\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 5.4295 - mae: 5.4295 - val_loss: 5.3788 - val_mae: 5.3788\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 4.9329 - mae: 4.9329 - val_loss: 5.3481 - val_mae: 5.3481\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.7313 - mae: 4.7313 - val_loss: 5.3523 - val_mae: 5.3523\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 4.5930 - mae: 4.5930 - val_loss: 5.3222 - val_mae: 5.3222\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.2494 - mae: 5.2494 - val_loss: 5.3255 - val_mae: 5.3255\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.8591 - mae: 4.8591 - val_loss: 5.3684 - val_mae: 5.3684\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.4642 - mae: 4.4642 - val_loss: 5.3882 - val_mae: 5.3882\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 4.5766 - mae: 4.5766 - val_loss: 5.3814 - val_mae: 5.3814\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.5373 - mae: 4.5373 - val_loss: 5.4919 - val_mae: 5.4919\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.6238 - mae: 4.6238 - val_loss: 5.6082 - val_mae: 5.6082\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.4248 - mae: 5.4248 - val_loss: 5.9869 - val_mae: 5.9869\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.7816 - mae: 5.7816 - val_loss: 5.6834 - val_mae: 5.6834\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.3971 - mae: 5.3971 - val_loss: 5.2730 - val_mae: 5.2730\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.4060 - mae: 4.4060 - val_loss: 5.4989 - val_mae: 5.4989\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.2158 - mae: 6.2158 - val_loss: 6.1042 - val_mae: 6.1042\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 6.8697 - mae: 6.8697 - val_loss: 5.6377 - val_mae: 5.6377\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.4071 - mae: 5.4071 - val_loss: 5.2429 - val_mae: 5.2429\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.7695 - mae: 4.7695 - val_loss: 5.2191 - val_mae: 5.2191\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.6808 - mae: 4.6808 - val_loss: 5.4864 - val_mae: 5.4864\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.6150 - mae: 4.6150 - val_loss: 5.5310 - val_mae: 5.5310\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.6100 - mae: 4.6100 - val_loss: 5.4827 - val_mae: 5.4827\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.5350 - mae: 4.5350 - val_loss: 5.3554 - val_mae: 5.3554\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4.6147 - mae: 4.6147 - val_loss: 5.2500 - val_mae: 5.2500\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.5352 - mae: 4.5352 - val_loss: 5.2916 - val_mae: 5.2916\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.5556 - mae: 4.5556 - val_loss: 5.3221 - val_mae: 5.3221\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 4.5576 - mae: 4.5576 - val_loss: 5.5250 - val_mae: 5.5250\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.7385 - mae: 4.7385 - val_loss: 5.8407 - val_mae: 5.8407\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.4214 - mae: 5.4214 - val_loss: 6.2461 - val_mae: 6.2461\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.9501 - mae: 5.9501 - val_loss: 6.2208 - val_mae: 6.2208\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.2977 - mae: 5.2977 - val_loss: 6.0069 - val_mae: 6.0069\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.2887 - mae: 5.2887 - val_loss: 6.6937 - val_mae: 6.6937\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.4777 - mae: 6.4777 - val_loss: 6.8937 - val_mae: 6.8937\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.6940 - mae: 6.6940 - val_loss: 5.5374 - val_mae: 5.5374\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.7275 - mae: 5.7275 - val_loss: 5.0550 - val_mae: 5.0550\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.5961 - mae: 4.5961 - val_loss: 5.5293 - val_mae: 5.5293\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 5.9808 - mae: 5.9808 - val_loss: 6.3791 - val_mae: 6.3791\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 7.2436 - mae: 7.2436 - val_loss: 6.7931 - val_mae: 6.7931\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 7.6213 - mae: 7.6213 - val_loss: 6.7635 - val_mae: 6.7635\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 7.1174 - mae: 7.1174 - val_loss: 6.4841 - val_mae: 6.4841\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 5.7983 - mae: 5.7983 - val_loss: 6.0242 - val_mae: 6.0242\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.5909 - mae: 5.5909 - val_loss: 6.3091 - val_mae: 6.3091\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.2687 - mae: 6.2687 - val_loss: 6.7306 - val_mae: 6.7306\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 6.8257 - mae: 6.8257 - val_loss: 5.9086 - val_mae: 5.9086\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 6.1827 - mae: 6.1827 - val_loss: 5.0274 - val_mae: 5.0274\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.0295 - mae: 5.0295 - val_loss: 5.4245 - val_mae: 5.4245\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.6496 - mae: 5.6496 - val_loss: 5.3632 - val_mae: 5.3632\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.7047 - mae: 5.7047 - val_loss: 5.1909 - val_mae: 5.1909\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.1936 - mae: 5.1936 - val_loss: 5.2065 - val_mae: 5.2065\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.5182 - mae: 4.5182 - val_loss: 5.2935 - val_mae: 5.2935\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 4.6969 - mae: 4.6969 - val_loss: 5.4699 - val_mae: 5.4699\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.2364 - mae: 5.2364 - val_loss: 5.6187 - val_mae: 5.6187\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 4.8626 - mae: 4.8626 - val_loss: 5.6214 - val_mae: 5.6214\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.8607 - mae: 4.8607 - val_loss: 5.5852 - val_mae: 5.5852\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.9061 - mae: 4.9061 - val_loss: 5.6609 - val_mae: 5.6609\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.8519 - mae: 4.8519 - val_loss: 5.7940 - val_mae: 5.7940\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.9133 - mae: 4.9133 - val_loss: 5.6218 - val_mae: 5.6218\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.2787 - mae: 5.2787 - val_loss: 5.5875 - val_mae: 5.5875\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 5.4286 - mae: 5.4286 - val_loss: 5.4359 - val_mae: 5.4359\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.7747 - mae: 4.7747 - val_loss: 5.4257 - val_mae: 5.4257\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.5260 - mae: 4.5260 - val_loss: 5.3343 - val_mae: 5.3343\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 4.5877 - mae: 4.5877 - val_loss: 5.3567 - val_mae: 5.3567\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.8726 - mae: 4.8726 - val_loss: 5.3376 - val_mae: 5.3376\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 4.5930 - mae: 4.5930 - val_loss: 5.3079 - val_mae: 5.3079\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.3620 - mae: 4.3620 - val_loss: 5.4216 - val_mae: 5.4216\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.9317 - mae: 4.9317 - val_loss: 5.9355 - val_mae: 5.9355\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.6778 - mae: 5.6778 - val_loss: 5.8422 - val_mae: 5.8422\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.0841 - mae: 5.0841 - val_loss: 5.8153 - val_mae: 5.8153\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.8398 - mae: 4.8398 - val_loss: 5.5951 - val_mae: 5.5951\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.8516 - mae: 4.8516 - val_loss: 5.3479 - val_mae: 5.3479\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.8017 - mae: 4.8017 - val_loss: 5.2569 - val_mae: 5.2569\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 4.7088 - mae: 4.7088 - val_loss: 5.1897 - val_mae: 5.1897\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 4.7450 - mae: 4.7450 - val_loss: 5.2443 - val_mae: 5.2443\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.7183 - mae: 4.7183 - val_loss: 5.1734 - val_mae: 5.1734\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 4.5190 - mae: 4.5190 - val_loss: 5.2733 - val_mae: 5.2733\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.5001 - mae: 4.5001 - val_loss: 5.4026 - val_mae: 5.4026\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.4647 - mae: 4.4647 - val_loss: 5.4800 - val_mae: 5.4800\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.9119 - mae: 4.9119 - val_loss: 5.4770 - val_mae: 5.4770\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.1141 - mae: 5.1141 - val_loss: 5.2512 - val_mae: 5.2512\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.6506 - mae: 4.6506 - val_loss: 5.1562 - val_mae: 5.1562\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.6360 - mae: 4.6360 - val_loss: 5.3708 - val_mae: 5.3708\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 4.8198 - mae: 4.8198 - val_loss: 5.2265 - val_mae: 5.2265\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.5407 - mae: 4.5407 - val_loss: 5.3589 - val_mae: 5.3589\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 4.4905 - mae: 4.4905 - val_loss: 5.5353 - val_mae: 5.5353\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.6912 - mae: 4.6912 - val_loss: 5.6906 - val_mae: 5.6906\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.7628 - mae: 4.7628 - val_loss: 5.6127 - val_mae: 5.6127\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 4.7423 - mae: 4.7423 - val_loss: 5.5533 - val_mae: 5.5533\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 4.7184 - mae: 4.7184 - val_loss: 5.5296 - val_mae: 5.5296\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 4.6539 - mae: 4.6539 - val_loss: 5.5977 - val_mae: 5.5977\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.6697 - mae: 4.6697 - val_loss: 5.5639 - val_mae: 5.5639\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.5720 - mae: 4.5720 - val_loss: 5.4240 - val_mae: 5.4240\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.4691 - mae: 4.4691 - val_loss: 5.3852 - val_mae: 5.3852\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 4.4425 - mae: 4.4425 - val_loss: 5.3793 - val_mae: 5.3793\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 4.4627 - mae: 4.4627 - val_loss: 5.4599 - val_mae: 5.4599\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.4429 - mae: 4.4429 - val_loss: 5.8046 - val_mae: 5.8046\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.7042 - mae: 4.7042 - val_loss: 6.8349 - val_mae: 6.8349\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.9996 - mae: 5.9996 - val_loss: 7.4599 - val_mae: 7.4599\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 6.7418 - mae: 6.7418 - val_loss: 7.0714 - val_mae: 7.0714\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.7412 - mae: 6.7412 - val_loss: 6.0118 - val_mae: 6.0118\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.9486 - mae: 5.9486 - val_loss: 5.5052 - val_mae: 5.5052\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.4090 - mae: 5.4090 - val_loss: 5.3272 - val_mae: 5.3272\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.8027 - mae: 4.8027 - val_loss: 5.8532 - val_mae: 5.8532\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.4858 - mae: 5.4858 - val_loss: 5.8508 - val_mae: 5.8508\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 6.0444 - mae: 6.0444 - val_loss: 5.5071 - val_mae: 5.5071\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 5.3004 - mae: 5.3004 - val_loss: 5.2847 - val_mae: 5.2847\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 4.4418 - mae: 4.4418 - val_loss: 5.6448 - val_mae: 5.6448\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.7791 - mae: 4.7791 - val_loss: 6.8036 - val_mae: 6.8036\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.8991 - mae: 5.8991 - val_loss: 6.9634 - val_mae: 6.9634\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 6.5851 - mae: 6.5851 - val_loss: 6.1633 - val_mae: 6.1633\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.3485 - mae: 5.3485 - val_loss: 5.4596 - val_mae: 5.4596\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.4613 - mae: 4.4613 - val_loss: 5.2909 - val_mae: 5.2909\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.6411 - mae: 4.6411 - val_loss: 5.7197 - val_mae: 5.7197\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.2012 - mae: 5.2012 - val_loss: 5.6885 - val_mae: 5.6885\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.0701 - mae: 5.0701 - val_loss: 5.2247 - val_mae: 5.2247\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.5470 - mae: 4.5470 - val_loss: 5.5227 - val_mae: 5.5227\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.4740 - mae: 4.4740 - val_loss: 6.4692 - val_mae: 6.4692\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.4656 - mae: 5.4656 - val_loss: 7.0044 - val_mae: 7.0044\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 6.0826 - mae: 6.0826 - val_loss: 6.7504 - val_mae: 6.7504\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 5.7280 - mae: 5.7280 - val_loss: 5.8563 - val_mae: 5.8563\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.9674 - mae: 4.9674 - val_loss: 5.2276 - val_mae: 5.2276\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.4142 - mae: 4.4142 - val_loss: 5.2414 - val_mae: 5.2414\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.8288 - mae: 4.8288 - val_loss: 6.0369 - val_mae: 6.0369\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 6.4916 - mae: 6.4916 - val_loss: 6.1739 - val_mae: 6.1739\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 6.7089 - mae: 6.7089 - val_loss: 5.4702 - val_mae: 5.4702\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.5028 - mae: 5.5028 - val_loss: 5.2368 - val_mae: 5.2368\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.4443 - mae: 4.4443 - val_loss: 5.7724 - val_mae: 5.7724\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.8120 - mae: 4.8120 - val_loss: 6.1270 - val_mae: 6.1270\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 5.1013 - mae: 5.1013 - val_loss: 5.8762 - val_mae: 5.8762\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.0300 - mae: 5.0300 - val_loss: 5.5168 - val_mae: 5.5168\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 4.5873 - mae: 4.5873 - val_loss: 5.2310 - val_mae: 5.2310\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.4328 - mae: 4.4328 - val_loss: 5.0369 - val_mae: 5.0369\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.7333 - mae: 4.7333 - val_loss: 5.6512 - val_mae: 5.6512\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 5.3931 - mae: 5.3931 - val_loss: 5.6815 - val_mae: 5.6815\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 5.4078 - mae: 5.4078 - val_loss: 5.2270 - val_mae: 5.2270\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 4.8661 - mae: 4.8661 - val_loss: 5.2004 - val_mae: 5.2004\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 4.5204 - mae: 4.5204 - val_loss: 5.8512 - val_mae: 5.8512\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.1576 - mae: 5.1576 - val_loss: 7.0426 - val_mae: 7.0426\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.4854 - mae: 6.4854 - val_loss: 7.2022 - val_mae: 7.2022\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.3776 - mae: 6.3776 - val_loss: 6.4415 - val_mae: 6.4415\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.9784 - mae: 5.9784 - val_loss: 5.5050 - val_mae: 5.5050\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.3557 - mae: 5.3557 - val_loss: 5.3231 - val_mae: 5.3231\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.6353 - mae: 4.6353 - val_loss: 5.2079 - val_mae: 5.2079\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.5939 - mae: 4.5939 - val_loss: 5.1730 - val_mae: 5.1730\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.8384 - mae: 4.8384 - val_loss: 5.2261 - val_mae: 5.2261\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 4.5278 - mae: 4.5278 - val_loss: 5.2770 - val_mae: 5.2770\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.3676 - mae: 4.3676 - val_loss: 5.3008 - val_mae: 5.3008\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.8911 - mae: 4.8911 - val_loss: 5.3365 - val_mae: 5.3365\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 5.2428 - mae: 5.2428 - val_loss: 5.3616 - val_mae: 5.3616\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.8359 - mae: 4.8359 - val_loss: 5.1712 - val_mae: 5.1712\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.6148 - mae: 4.6148 - val_loss: 5.1428 - val_mae: 5.1428\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.8476 - mae: 4.8476 - val_loss: 5.1818 - val_mae: 5.1818\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.5983 - mae: 4.5983 - val_loss: 5.2782 - val_mae: 5.2782\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.4129 - mae: 4.4129 - val_loss: 5.3230 - val_mae: 5.3230\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.3568 - mae: 4.3568 - val_loss: 5.2279 - val_mae: 5.2279\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.3850 - mae: 4.3850 - val_loss: 5.2568 - val_mae: 5.2568\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.3609 - mae: 5.3609 - val_loss: 6.0283 - val_mae: 6.0283\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 5.8929 - mae: 5.8929 - val_loss: 5.4963 - val_mae: 5.4963\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 5.2410 - mae: 5.2410 - val_loss: 5.3614 - val_mae: 5.3614\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 4.4634 - mae: 4.4634 - val_loss: 5.3631 - val_mae: 5.3631\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.3608 - mae: 4.3608 - val_loss: 5.3904 - val_mae: 5.3904\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 4.3632 - mae: 4.3632 - val_loss: 5.4238 - val_mae: 5.4238\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 4.7057 - mae: 4.7057 - val_loss: 5.4071 - val_mae: 5.4071\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 4.5603 - mae: 4.5603 - val_loss: 5.3321 - val_mae: 5.3321\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.4072 - mae: 4.4072 - val_loss: 5.2390 - val_mae: 5.2390\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 4.4666 - mae: 4.4666 - val_loss: 5.1751 - val_mae: 5.1751\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.5636 - mae: 4.5636 - val_loss: 5.1796 - val_mae: 5.1796\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 4.6020 - mae: 4.6020 - val_loss: 5.3335 - val_mae: 5.3335\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 4.7610 - mae: 4.7610 - val_loss: 5.1907 - val_mae: 5.1907\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 4.6535 - mae: 4.6535 - val_loss: 5.1444 - val_mae: 5.1444\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 4.4420 - mae: 4.4420 - val_loss: 5.3294 - val_mae: 5.3294\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 4.4360 - mae: 4.4360 - val_loss: 5.6210 - val_mae: 5.6210\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 4.8529 - mae: 4.8529 - val_loss: 6.0793 - val_mae: 6.0793\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.1453 - mae: 5.1453 - val_loss: 5.8990 - val_mae: 5.8990\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.6903 - mae: 4.6903 - val_loss: 5.3012 - val_mae: 5.3012\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.2886 - mae: 4.2886 - val_loss: 5.4313 - val_mae: 5.4313\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 5.0318 - mae: 5.0318 - val_loss: 6.1001 - val_mae: 6.1001\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.8332 - mae: 5.8332 - val_loss: 6.1050 - val_mae: 6.1050\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.7155 - mae: 5.7155 - val_loss: 5.6086 - val_mae: 5.6086\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.1133 - mae: 5.1133 - val_loss: 5.1681 - val_mae: 5.1681\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.4126 - mae: 4.4126 - val_loss: 5.4762 - val_mae: 5.4762\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.5683 - mae: 4.5683 - val_loss: 5.8574 - val_mae: 5.8574\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.1896 - mae: 5.1896 - val_loss: 5.7528 - val_mae: 5.7528\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 4.7166 - mae: 4.7166 - val_loss: 5.4429 - val_mae: 5.4429\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 4.3852 - mae: 4.3852 - val_loss: 5.1448 - val_mae: 5.1448\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 4.6415 - mae: 4.6415 - val_loss: 5.4469 - val_mae: 5.4469\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 5.2274 - mae: 5.2274 - val_loss: 6.0460 - val_mae: 6.0460\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.7348 - mae: 5.7348 - val_loss: 6.1647 - val_mae: 6.1647\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.8088 - mae: 5.8088 - val_loss: 5.7744 - val_mae: 5.7744\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.6061 - mae: 5.6061 - val_loss: 5.2812 - val_mae: 5.2812\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.5598 - mae: 4.5598 - val_loss: 5.2702 - val_mae: 5.2702\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.5445 - mae: 4.5445 - val_loss: 5.8710 - val_mae: 5.8710\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.4988 - mae: 5.4988 - val_loss: 6.0331 - val_mae: 6.0331\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5.2703 - mae: 5.2703 - val_loss: 6.4751 - val_mae: 6.4751\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 5.4035 - mae: 5.4035 - val_loss: 6.5483 - val_mae: 6.5483\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.0613 - mae: 6.0613 - val_loss: 6.0431 - val_mae: 6.0431\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 5.5040 - mae: 5.5040 - val_loss: 5.3976 - val_mae: 5.3976\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.5050 - mae: 4.5050 - val_loss: 5.1617 - val_mae: 5.1617\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.3724 - mae: 4.3724 - val_loss: 5.0966 - val_mae: 5.0966\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.5706 - mae: 4.5706 - val_loss: 5.1046 - val_mae: 5.1046\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 4.6704 - mae: 4.6704 - val_loss: 5.1276 - val_mae: 5.1276\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 4.6213 - mae: 4.6213 - val_loss: 5.1527 - val_mae: 5.1527\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 4.4788 - mae: 4.4788 - val_loss: 5.2433 - val_mae: 5.2433\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 4.5153 - mae: 4.5153 - val_loss: 5.4703 - val_mae: 5.4703\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 4.7945 - mae: 4.7945 - val_loss: 5.9514 - val_mae: 5.9514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2498182e8c0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", input_shape = (1, ), name = \"hidden_layer\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = 'model_3'\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = ['mae']\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds_3 = model.predict(X_test)\n",
    "plot_predictions(predictions = y_preds_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=9.50535>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=120.30011>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_3 = mae(y_test, y_preds_3)\n",
    "mse_3 = mse(y_test, y_preds_3)\n",
    "mae_3,mse_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be performing really well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results of our experiments\n",
    "\n",
    "We've run a few experiments, let's compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    models  Mean Absolute Error  Mean Square Error\n",
      "0  model_1             9.505350         120.300110\n",
      "1  model_2            11.347539         138.561646\n",
      "2  model_3             9.505350         120.300110\n"
     ]
    }
   ],
   "source": [
    "# Lets compare our model's results using a pandas Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "model_results = pd.DataFrame({\"models\":[\"model_1\", \"model_2\", \"model_3\"], \"Mean Absolute Error\": [mae_1.numpy(), mae_2.numpy(), mae_3.numpy()], \"Mean Square Error\": [mse_1.numpy(), mse_2.numpy(), mse_3.numpy()]})\n",
    "print(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 seems to be performing the best.\n",
    "\n",
    ">**Note:** One of our main goals is to minimize the time between our experiments. The more experiments we do, the more things we'll figure our which don't work and in turn get closer to figuring out what does work. Remember the machine learning practitioner's motto: \"Experiment, experiment, experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking your experiments\n",
    "\n",
    "One really good habit in machine learning modelling is to track the results of your experiments\n",
    "\n",
    "And when doing so, it can be tedious if you're running lots of experiments.\n",
    "\n",
    "Luckily there are tools to help us!\n",
    "\n",
    "\n",
    "**Resource:** As you build more models, you'll want to look into using:\n",
    "* Tensorboard - a component of the TensorFlow library to help track modelling experiments (we'll see this one later)\n",
    "* Weights and Biases - a tool for tracking all kinds of machine learning experiments (plugs straight into TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = \"relu\", input_shape = (1, ), name = \"hidden_layer_1\"),\n",
    "    tf.keras.layers.Dense(256, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "    tf.keras.layers.Dense(64, activation = \"relu\", name = \"hidden_layer_3\"),\n",
    "    tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "], name =\"model_best\")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train,y_train, validation_data = (X_val, y_val),epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our models\n",
    "\n",
    "Saving our models allows us to use them outside of Google Colab (or wherever they were trained) such as in a web application or a mobile app\n",
    "\n",
    "In TensorFlow there are 2 formats:\n",
    "\n",
    "* SavedModel format: Restored using tf.keras.models.load_model; Compatible with TensorFlow Serving; Structure: a) `assets` directory b) `variables` directory c) `saved_model.pb` (Proteau by file)\n",
    "* HDF5 format: Hierarchical Data Format; Designed to store and organise large amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_savedmodel_format\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model using the SavedModel format\n",
    "model.save(\"Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_savedmodel_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in the HDF5 format\n",
    "model.save(\"Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_HDF5_format.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer (Dense)        (None, 128)               256       \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load in the SavedModel format model\n",
    "\n",
    "loaded_SavedModel_format = tf.keras.models.load_model(\"Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_savedmodel_format\")\n",
    "loaded_SavedModel_format.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 57ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=9.50535>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=120.30011>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_loaded = loaded_SavedModel_format.predict(X_test)\n",
    "mae(y_test, y_preds_loaded), mse(y_test, y_preds_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer (Dense)        (None, 128)               256       \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_h5_model = tf.keras.models.load_model(\"Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_HDF5_format.h5\")\n",
    "loaded_h5_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=9.50535>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=120.30011>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_loaded = loaded_h5_model.predict(X_test)\n",
    "mae(y_test, y_preds_loaded), mse(y_test, y_preds_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download a model (or any other file) from Google Colab\n",
    "\n",
    "If you want to download your files from Google Colab:\n",
    "\n",
    "1. You can go to the \"files\" tab and right click on the file you're after and click download\n",
    "2. Use code (see the cell below)\n",
    "3. Save it to Google Drive by connecting Google Drive and copying it there (see 2nd code cell below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a file from Google Colab\n",
    "# from google.colab import files\n",
    "\n",
    "# files.download(\"/content/best_model_HDF5_format.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a file from Google Colab to Google Drive (requires mounting Google Drive)\n",
    "# !cp /content/drive/best_model_HDF5_format.h5 /content/drive/MyDrive/tensorflow_course/Models\n",
    "# !ls /content/drive/MyDrive/tensorflow_course/Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Larger Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     sex     bmi  children smoker     region      charges\n",
       "0   19  female  27.900         0    yes  southwest  16884.92400\n",
       "1   18    male  33.770         1     no  southeast   1725.55230\n",
       "2   28    male  33.000         3     no  southeast   4449.46200\n",
       "3   33    male  22.705         0     no  northwest  21984.47061\n",
       "4   32    male  28.880         0     no  northwest   3866.85520"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the insurance dataset\n",
    "insurance_df = pd.read_csv(\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\")\n",
    "insurance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = insurance_df.iloc[:, :-1], insurance_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Use sklearn for Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\"sex\", \"smoker\", \"region\"]\n",
    "numerical_columns = [\"age\",\"bmi\", \"children\"]\n",
    "\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        ('std_scaling_layer', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "cat_pipeline = Pipeline(\n",
    "    [\n",
    "        ('one_hot_encoding', OneHotEncoder())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1338, 11)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        ('num_transform_layer', num_pipeline, numerical_columns),\n",
    "        ('cat_transform_layer', cat_pipeline, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_prepared = full_pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Use Pandas for data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>smoker_no</th>\n",
       "      <th>smoker_yes</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>10600.54830</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>2205.98080</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>1629.83350</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.94500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>29141.36030</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi  children      charges  sex_female  sex_male  smoker_no  \\\n",
       "0      19  27.900         0  16884.92400           1         0          0   \n",
       "1      18  33.770         1   1725.55230           0         1          1   \n",
       "2      28  33.000         3   4449.46200           0         1          1   \n",
       "3      33  22.705         0  21984.47061           0         1          1   \n",
       "4      32  28.880         0   3866.85520           0         1          1   \n",
       "...   ...     ...       ...          ...         ...       ...        ...   \n",
       "1333   50  30.970         3  10600.54830           0         1          1   \n",
       "1334   18  31.920         0   2205.98080           1         0          1   \n",
       "1335   18  36.850         0   1629.83350           1         0          1   \n",
       "1336   21  25.800         0   2007.94500           1         0          1   \n",
       "1337   61  29.070         0  29141.36030           1         0          0   \n",
       "\n",
       "      smoker_yes  region_northeast  region_northwest  region_southeast  \\\n",
       "0              1                 0                 0                 0   \n",
       "1              0                 0                 0                 1   \n",
       "2              0                 0                 0                 1   \n",
       "3              0                 0                 1                 0   \n",
       "4              0                 0                 1                 0   \n",
       "...          ...               ...               ...               ...   \n",
       "1333           0                 0                 1                 0   \n",
       "1334           0                 1                 0                 0   \n",
       "1335           0                 0                 0                 1   \n",
       "1336           0                 0                 0                 0   \n",
       "1337           1                 0                 1                 0   \n",
       "\n",
       "      region_southwest  \n",
       "0                    1  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  \n",
       "...                ...  \n",
       "1333                 0  \n",
       "1334                 0  \n",
       "1335                 0  \n",
       "1336                 1  \n",
       "1337                 0  \n",
       "\n",
       "[1338 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insurance_one_hot = pd.get_dummies(insurance_df)\n",
    "insurance_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can proceed from here. However, I prefer the first method since its much more succinct and does a better job of preparing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Train, Validation and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_permutation = np.random.permutation(X_prepared.shape[0])\n",
    "\n",
    "train_set_idx, val_set_idx, test_set_idx = random_permutation[int(0.3*len(random_permutation)):], random_permutation[len(random_permutation)//10:int(0.3*len(random_permutation))], random_permutation[:len(random_permutation)//10]\n",
    "X_train_prepared, X_val_prepared, X_test_prepared = tf.convert_to_tensor(X_prepared[train_set_idx]), tf.convert_to_tensor(X_prepared[val_set_idx]), tf.convert_to_tensor(X_prepared[test_set_idx])\n",
    "y_train, y_val, y_test = tf.convert_to_tensor(y[train_set_idx]), tf.convert_to_tensor(y[val_set_idx]), tf.convert_to_tensor(y[test_set_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we can use `train_test_split` from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Fit A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "30/30 [==============================] - 1s 14ms/step - loss: 315311232.0000 - mae: 13061.9658 - val_loss: 330626912.0000 - val_mae: 13440.3438\n",
      "Epoch 2/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 314284352.0000 - mae: 13024.5566 - val_loss: 328789120.0000 - val_mae: 13376.6982\n",
      "Epoch 3/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 311747296.0000 - mae: 12932.1885 - val_loss: 325141088.0000 - val_mae: 13249.3721\n",
      "Epoch 4/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 307366848.0000 - mae: 12775.6582 - val_loss: 319586656.0000 - val_mae: 13052.4492\n",
      "Epoch 5/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 301053792.0000 - mae: 12543.0186 - val_loss: 311991232.0000 - val_mae: 12780.1914\n",
      "Epoch 6/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 292924544.0000 - mae: 12241.3613 - val_loss: 302667296.0000 - val_mae: 12437.1689\n",
      "Epoch 7/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 283153280.0000 - mae: 11861.5811 - val_loss: 291538016.0000 - val_mae: 12016.9678\n",
      "Epoch 8/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 272009408.0000 - mae: 11425.7461 - val_loss: 279640960.0000 - val_mae: 11550.6943\n",
      "Epoch 9/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 260169056.0000 - mae: 10941.9893 - val_loss: 266670208.0000 - val_mae: 11037.7217\n",
      "Epoch 10/100\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 247475568.0000 - mae: 10428.2764 - val_loss: 253359312.0000 - val_mae: 10502.6484\n",
      "Epoch 11/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 234477808.0000 - mae: 9895.6436 - val_loss: 239802416.0000 - val_mae: 9956.9990\n",
      "Epoch 12/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 221421904.0000 - mae: 9385.3564 - val_loss: 226325904.0000 - val_mae: 9419.1289\n",
      "Epoch 13/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 208712640.0000 - mae: 8892.5674 - val_loss: 213346656.0000 - val_mae: 8906.4785\n",
      "Epoch 14/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 196463632.0000 - mae: 8454.1328 - val_loss: 201089376.0000 - val_mae: 8456.0479\n",
      "Epoch 15/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 185253120.0000 - mae: 8068.7520 - val_loss: 189081968.0000 - val_mae: 8065.3276\n",
      "Epoch 16/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 174585024.0000 - mae: 7755.3262 - val_loss: 178994016.0000 - val_mae: 7803.7095\n",
      "Epoch 17/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 165355408.0000 - mae: 7541.6528 - val_loss: 169570336.0000 - val_mae: 7605.0259\n",
      "Epoch 18/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 156784496.0000 - mae: 7395.8716 - val_loss: 161464112.0000 - val_mae: 7487.3984\n",
      "Epoch 19/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 149522576.0000 - mae: 7331.2822 - val_loss: 153762336.0000 - val_mae: 7429.4624\n",
      "Epoch 20/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 142909424.0000 - mae: 7314.1187 - val_loss: 147600960.0000 - val_mae: 7429.8789\n",
      "Epoch 21/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 137454592.0000 - mae: 7343.8169 - val_loss: 141916368.0000 - val_mae: 7471.1211\n",
      "Epoch 22/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 132456888.0000 - mae: 7375.7744 - val_loss: 137225920.0000 - val_mae: 7508.1616\n",
      "Epoch 23/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 128149504.0000 - mae: 7442.6289 - val_loss: 132749056.0000 - val_mae: 7567.6616\n",
      "Epoch 24/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 124204272.0000 - mae: 7481.1948 - val_loss: 128959672.0000 - val_mae: 7602.4458\n",
      "Epoch 25/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 120656464.0000 - mae: 7529.1030 - val_loss: 125015696.0000 - val_mae: 7649.5679\n",
      "Epoch 26/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 117297704.0000 - mae: 7561.7700 - val_loss: 121524232.0000 - val_mae: 7671.6406\n",
      "Epoch 27/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 114228040.0000 - mae: 7565.7432 - val_loss: 118544712.0000 - val_mae: 7664.4478\n",
      "Epoch 28/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 111416320.0000 - mae: 7565.0171 - val_loss: 115436824.0000 - val_mae: 7655.9810\n",
      "Epoch 29/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 108662560.0000 - mae: 7550.8452 - val_loss: 112513328.0000 - val_mae: 7626.3423\n",
      "Epoch 30/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 105947952.0000 - mae: 7520.3545 - val_loss: 109780224.0000 - val_mae: 7589.7012\n",
      "Epoch 31/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 103442592.0000 - mae: 7488.0073 - val_loss: 106956536.0000 - val_mae: 7547.6455\n",
      "Epoch 32/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 100945128.0000 - mae: 7446.7896 - val_loss: 104323064.0000 - val_mae: 7510.9644\n",
      "Epoch 33/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 98589104.0000 - mae: 7415.9229 - val_loss: 101903856.0000 - val_mae: 7457.2056\n",
      "Epoch 34/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 96338432.0000 - mae: 7346.4160 - val_loss: 99413648.0000 - val_mae: 7387.2075\n",
      "Epoch 35/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 94090744.0000 - mae: 7286.5063 - val_loss: 97033344.0000 - val_mae: 7320.7842\n",
      "Epoch 36/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 91913408.0000 - mae: 7212.5786 - val_loss: 94738880.0000 - val_mae: 7237.0708\n",
      "Epoch 37/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 89857632.0000 - mae: 7143.2539 - val_loss: 92505136.0000 - val_mae: 7155.5312\n",
      "Epoch 38/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 87776792.0000 - mae: 7048.5098 - val_loss: 90455864.0000 - val_mae: 7061.9946\n",
      "Epoch 39/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 85811208.0000 - mae: 6970.5552 - val_loss: 88340496.0000 - val_mae: 6983.8179\n",
      "Epoch 40/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 83927760.0000 - mae: 6913.0254 - val_loss: 86143104.0000 - val_mae: 6909.9165\n",
      "Epoch 41/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 81866024.0000 - mae: 6815.4995 - val_loss: 84093632.0000 - val_mae: 6809.9673\n",
      "Epoch 42/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 79955528.0000 - mae: 6726.7090 - val_loss: 82162656.0000 - val_mae: 6717.4551\n",
      "Epoch 43/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 78162008.0000 - mae: 6634.2510 - val_loss: 80146208.0000 - val_mae: 6621.0620\n",
      "Epoch 44/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 76420088.0000 - mae: 6551.3872 - val_loss: 78139840.0000 - val_mae: 6528.0327\n",
      "Epoch 45/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 74575096.0000 - mae: 6465.9370 - val_loss: 76387816.0000 - val_mae: 6436.9453\n",
      "Epoch 46/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 72835192.0000 - mae: 6385.0005 - val_loss: 74420360.0000 - val_mae: 6351.6992\n",
      "Epoch 47/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 71139784.0000 - mae: 6301.1470 - val_loss: 72573920.0000 - val_mae: 6251.6245\n",
      "Epoch 48/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 69415424.0000 - mae: 6201.0449 - val_loss: 70835752.0000 - val_mae: 6141.9961\n",
      "Epoch 49/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 67822184.0000 - mae: 6117.9390 - val_loss: 69103384.0000 - val_mae: 6060.4150\n",
      "Epoch 50/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 66239276.0000 - mae: 6033.2710 - val_loss: 67290760.0000 - val_mae: 5965.2896\n",
      "Epoch 51/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 64635248.0000 - mae: 5935.2842 - val_loss: 65686728.0000 - val_mae: 5873.6846\n",
      "Epoch 52/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 63132680.0000 - mae: 5862.7622 - val_loss: 64033580.0000 - val_mae: 5794.8594\n",
      "Epoch 53/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 61664980.0000 - mae: 5794.5850 - val_loss: 62460216.0000 - val_mae: 5719.8740\n",
      "Epoch 54/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 60209632.0000 - mae: 5717.0244 - val_loss: 60899476.0000 - val_mae: 5643.7539\n",
      "Epoch 55/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 58819852.0000 - mae: 5639.9917 - val_loss: 59422352.0000 - val_mae: 5571.1670\n",
      "Epoch 56/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 57486168.0000 - mae: 5562.1089 - val_loss: 58026772.0000 - val_mae: 5493.1719\n",
      "Epoch 57/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 56214412.0000 - mae: 5502.0464 - val_loss: 56626908.0000 - val_mae: 5438.1343\n",
      "Epoch 58/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 54968756.0000 - mae: 5434.3403 - val_loss: 55346888.0000 - val_mae: 5372.5518\n",
      "Epoch 59/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 53771256.0000 - mae: 5417.2764 - val_loss: 53915484.0000 - val_mae: 5359.0811\n",
      "Epoch 60/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 52596784.0000 - mae: 5368.0186 - val_loss: 52718564.0000 - val_mae: 5312.1304\n",
      "Epoch 61/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 51493104.0000 - mae: 5302.9585 - val_loss: 51504536.0000 - val_mae: 5237.9468\n",
      "Epoch 62/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 50412364.0000 - mae: 5249.2217 - val_loss: 50405704.0000 - val_mae: 5171.6694\n",
      "Epoch 63/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 49404048.0000 - mae: 5201.8711 - val_loss: 49266988.0000 - val_mae: 5133.8022\n",
      "Epoch 64/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 48430860.0000 - mae: 5162.2202 - val_loss: 48163952.0000 - val_mae: 5079.1084\n",
      "Epoch 65/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 47510556.0000 - mae: 5105.0693 - val_loss: 47142824.0000 - val_mae: 5015.5854\n",
      "Epoch 66/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 46636808.0000 - mae: 5042.8926 - val_loss: 46239876.0000 - val_mae: 4959.6797\n",
      "Epoch 67/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 45815320.0000 - mae: 5014.5303 - val_loss: 45358880.0000 - val_mae: 4940.2290\n",
      "Epoch 68/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 45058448.0000 - mae: 4977.2266 - val_loss: 44490468.0000 - val_mae: 4893.4917\n",
      "Epoch 69/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 44297564.0000 - mae: 4939.2520 - val_loss: 43623372.0000 - val_mae: 4847.9795\n",
      "Epoch 70/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 43618060.0000 - mae: 4884.1680 - val_loss: 42867424.0000 - val_mae: 4794.5400\n",
      "Epoch 71/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 42993964.0000 - mae: 4827.0156 - val_loss: 42210648.0000 - val_mae: 4738.8472\n",
      "Epoch 72/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 42385440.0000 - mae: 4818.6011 - val_loss: 41544252.0000 - val_mae: 4723.4556\n",
      "Epoch 73/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 41800764.0000 - mae: 4766.3003 - val_loss: 40906708.0000 - val_mae: 4668.6611\n",
      "Epoch 74/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 41304304.0000 - mae: 4731.2451 - val_loss: 40354984.0000 - val_mae: 4638.8760\n",
      "Epoch 75/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 40790884.0000 - mae: 4702.3628 - val_loss: 39808688.0000 - val_mae: 4598.6182\n",
      "Epoch 76/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 40371992.0000 - mae: 4649.7964 - val_loss: 39326336.0000 - val_mae: 4538.1289\n",
      "Epoch 77/100\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 39961368.0000 - mae: 4636.3735 - val_loss: 38836652.0000 - val_mae: 4537.7959\n",
      "Epoch 78/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 39565256.0000 - mae: 4596.9019 - val_loss: 38441044.0000 - val_mae: 4478.7070\n",
      "Epoch 79/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 39229616.0000 - mae: 4547.7188 - val_loss: 38063764.0000 - val_mae: 4457.2119\n",
      "Epoch 80/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 38950792.0000 - mae: 4550.1558 - val_loss: 37690536.0000 - val_mae: 4429.7715\n",
      "Epoch 81/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 38638192.0000 - mae: 4528.0024 - val_loss: 37369912.0000 - val_mae: 4417.3662\n",
      "Epoch 82/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 38379424.0000 - mae: 4506.2300 - val_loss: 37074512.0000 - val_mae: 4396.5688\n",
      "Epoch 83/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 38116772.0000 - mae: 4472.8550 - val_loss: 36797936.0000 - val_mae: 4350.5986\n",
      "Epoch 84/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 37910972.0000 - mae: 4426.5972 - val_loss: 36550192.0000 - val_mae: 4316.7173\n",
      "Epoch 85/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 37725612.0000 - mae: 4396.8999 - val_loss: 36348692.0000 - val_mae: 4291.8345\n",
      "Epoch 86/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 37538080.0000 - mae: 4403.4463 - val_loss: 36212264.0000 - val_mae: 4301.2354\n",
      "Epoch 87/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 37405060.0000 - mae: 4400.3862 - val_loss: 36003012.0000 - val_mae: 4282.8198\n",
      "Epoch 88/100\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 37268836.0000 - mae: 4371.9268 - val_loss: 35851904.0000 - val_mae: 4252.5913\n",
      "Epoch 89/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 37141172.0000 - mae: 4341.4028 - val_loss: 35688468.0000 - val_mae: 4228.6094\n",
      "Epoch 90/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 37036688.0000 - mae: 4327.1606 - val_loss: 35538144.0000 - val_mae: 4220.5439\n",
      "Epoch 91/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 36929384.0000 - mae: 4312.5449 - val_loss: 35478312.0000 - val_mae: 4204.6382\n",
      "Epoch 92/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 36846640.0000 - mae: 4302.4346 - val_loss: 35361908.0000 - val_mae: 4194.7769\n",
      "Epoch 93/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 36761616.0000 - mae: 4279.4766 - val_loss: 35261188.0000 - val_mae: 4164.6724\n",
      "Epoch 94/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 36694868.0000 - mae: 4254.4609 - val_loss: 35183968.0000 - val_mae: 4153.9116\n",
      "Epoch 95/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 36617852.0000 - mae: 4255.5469 - val_loss: 35134628.0000 - val_mae: 4156.4658\n",
      "Epoch 96/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 36615428.0000 - mae: 4277.3325 - val_loss: 35120304.0000 - val_mae: 4166.0312\n",
      "Epoch 97/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 36525784.0000 - mae: 4241.3149 - val_loss: 34990092.0000 - val_mae: 4125.1162\n",
      "Epoch 98/100\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 36477204.0000 - mae: 4209.3486 - val_loss: 34929856.0000 - val_mae: 4102.9453\n",
      "Epoch 99/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 36437972.0000 - mae: 4201.6582 - val_loss: 34889744.0000 - val_mae: 4104.3486\n",
      "Epoch 100/100\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 36414404.0000 - mae: 4190.1587 - val_loss: 34831100.0000 - val_mae: 4090.6553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2066e88c850>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create a model\n",
    "model_1 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, input_shape = (11,), activation = \"relu\", name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = \"model_2\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_1.compile(\n",
    "    loss = tf.keras.losses.MSE,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-2),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model_1.fit(X_train_prepared, y_train, validation_data = (X_val_prepared, y_val), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 10ms/step - loss: 41103312.0000 - mae: 4410.6392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[41103312.0, 4410.63916015625]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(X_test_prepared, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=13068.823416060834>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our model is doing just okay. Lets try to improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "30/30 [==============================] - 1s 14ms/step - loss: 315342112.0000 - mae: 13062.5098 - val_loss: 330380512.0000 - val_mae: 13429.4492\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 310404480.0000 - mae: 12858.5381 - val_loss: 313046528.0000 - val_mae: 12760.3320\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 255993360.0000 - mae: 10839.3809 - val_loss: 198191680.0000 - val_mae: 8745.0615\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 148675712.0000 - mae: 8985.1016 - val_loss: 128068824.0000 - val_mae: 9071.2471\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 114206528.0000 - mae: 8287.6777 - val_loss: 108333656.0000 - val_mae: 8155.9087\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 96684632.0000 - mae: 8018.4150 - val_loss: 90605808.0000 - val_mae: 7611.3218\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 80124368.0000 - mae: 7018.0552 - val_loss: 74889144.0000 - val_mae: 6869.3604\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 65815004.0000 - mae: 6421.5137 - val_loss: 59907200.0000 - val_mae: 6194.9941\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 52578972.0000 - mae: 5863.5654 - val_loss: 46933860.0000 - val_mae: 5264.3071\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 42683820.0000 - mae: 5086.0435 - val_loss: 38346824.0000 - val_mae: 4706.8062\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 37100684.0000 - mae: 4541.5850 - val_loss: 33816828.0000 - val_mae: 4311.4702\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 34607132.0000 - mae: 4050.9800 - val_loss: 32869758.0000 - val_mae: 4156.5991\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 33594848.0000 - mae: 4002.0833 - val_loss: 30831302.0000 - val_mae: 3758.6340\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 32327158.0000 - mae: 3765.2458 - val_loss: 30419276.0000 - val_mae: 3761.4033\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 31804542.0000 - mae: 3725.3936 - val_loss: 29224704.0000 - val_mae: 3536.6836\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 31457552.0000 - mae: 3612.6443 - val_loss: 28695998.0000 - val_mae: 3527.7996\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 30734494.0000 - mae: 3511.7947 - val_loss: 28242240.0000 - val_mae: 3521.2285\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 29939880.0000 - mae: 3489.8479 - val_loss: 27167850.0000 - val_mae: 3294.6455\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 29398996.0000 - mae: 3444.9104 - val_loss: 26620618.0000 - val_mae: 3291.7300\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 28844706.0000 - mae: 3342.0410 - val_loss: 25951814.0000 - val_mae: 3157.2578\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 28264072.0000 - mae: 3247.4336 - val_loss: 25329634.0000 - val_mae: 3175.8032\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 27664000.0000 - mae: 3295.3564 - val_loss: 24982032.0000 - val_mae: 3202.3345\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 27201698.0000 - mae: 3183.5059 - val_loss: 24115538.0000 - val_mae: 2999.7297\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 26796982.0000 - mae: 3169.3323 - val_loss: 23537884.0000 - val_mae: 2947.1982\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 26322328.0000 - mae: 3052.9304 - val_loss: 23238422.0000 - val_mae: 3016.3533\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25693408.0000 - mae: 3157.6138 - val_loss: 22661590.0000 - val_mae: 2895.0366\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25383450.0000 - mae: 3082.4399 - val_loss: 22141442.0000 - val_mae: 2836.5393\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25181314.0000 - mae: 2991.0420 - val_loss: 21627708.0000 - val_mae: 2667.9258\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24722058.0000 - mae: 2914.4983 - val_loss: 21524306.0000 - val_mae: 2903.6726\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24776592.0000 - mae: 3077.0879 - val_loss: 20975136.0000 - val_mae: 2805.0886\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 24253304.0000 - mae: 2912.2449 - val_loss: 21692146.0000 - val_mae: 3138.9729\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24096352.0000 - mae: 2914.2021 - val_loss: 20488476.0000 - val_mae: 2721.7871\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23527330.0000 - mae: 2982.7976 - val_loss: 20167322.0000 - val_mae: 2565.9036\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23511506.0000 - mae: 2864.2898 - val_loss: 19930758.0000 - val_mae: 2548.8672\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23172672.0000 - mae: 2873.4644 - val_loss: 19754390.0000 - val_mae: 2560.0801\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23007694.0000 - mae: 2844.8552 - val_loss: 19518008.0000 - val_mae: 2626.1797\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22823538.0000 - mae: 2881.1689 - val_loss: 19385376.0000 - val_mae: 2592.2339\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22678170.0000 - mae: 2878.9521 - val_loss: 19329640.0000 - val_mae: 2412.1235\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22881332.0000 - mae: 2804.0154 - val_loss: 19149054.0000 - val_mae: 2592.9629\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22487376.0000 - mae: 2805.5637 - val_loss: 19070012.0000 - val_mae: 2645.3726\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22532006.0000 - mae: 2810.2510 - val_loss: 19052112.0000 - val_mae: 2446.4673\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22348890.0000 - mae: 2823.0261 - val_loss: 18807466.0000 - val_mae: 2582.3098\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22228720.0000 - mae: 2782.2156 - val_loss: 18915626.0000 - val_mae: 2696.8127\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22369226.0000 - mae: 2808.0254 - val_loss: 18775728.0000 - val_mae: 2453.6997\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22152124.0000 - mae: 2924.1748 - val_loss: 18709212.0000 - val_mae: 2401.7395\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22458042.0000 - mae: 2821.5012 - val_loss: 18653266.0000 - val_mae: 2322.7402\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22330682.0000 - mae: 2809.2241 - val_loss: 18525754.0000 - val_mae: 2444.7227\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22064088.0000 - mae: 2796.0945 - val_loss: 18716642.0000 - val_mae: 2722.8862\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22109340.0000 - mae: 2783.4453 - val_loss: 18610638.0000 - val_mae: 2644.6804\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22083628.0000 - mae: 2873.1812 - val_loss: 18355968.0000 - val_mae: 2529.2173\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21790686.0000 - mae: 2724.3469 - val_loss: 18454450.0000 - val_mae: 2639.1343\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 21796798.0000 - mae: 2902.0151 - val_loss: 18398102.0000 - val_mae: 2383.7559\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21795588.0000 - mae: 2725.0237 - val_loss: 18248270.0000 - val_mae: 2520.9028\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21848074.0000 - mae: 2794.5974 - val_loss: 18315370.0000 - val_mae: 2550.5002\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21828836.0000 - mae: 2846.8076 - val_loss: 18111506.0000 - val_mae: 2394.6277\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22006188.0000 - mae: 2877.7825 - val_loss: 18264068.0000 - val_mae: 2593.9934\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21521254.0000 - mae: 2832.5005 - val_loss: 18204322.0000 - val_mae: 2453.8005\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 21516008.0000 - mae: 2813.1831 - val_loss: 18370534.0000 - val_mae: 2306.3186\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 21502706.0000 - mae: 2753.8125 - val_loss: 18696920.0000 - val_mae: 2838.5356\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21635150.0000 - mae: 2819.4087 - val_loss: 18224708.0000 - val_mae: 2602.7239\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21389450.0000 - mae: 2739.0220 - val_loss: 18043864.0000 - val_mae: 2413.9600\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21381806.0000 - mae: 2773.8193 - val_loss: 18282834.0000 - val_mae: 2662.1357\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21354362.0000 - mae: 2712.8042 - val_loss: 17958254.0000 - val_mae: 2429.6907\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21288382.0000 - mae: 2751.6777 - val_loss: 17922250.0000 - val_mae: 2477.8882\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21270690.0000 - mae: 2796.2993 - val_loss: 18137746.0000 - val_mae: 2287.2703\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21493986.0000 - mae: 2743.0479 - val_loss: 17971606.0000 - val_mae: 2340.6140\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21144040.0000 - mae: 2695.3315 - val_loss: 18206910.0000 - val_mae: 2670.2012\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21148792.0000 - mae: 2693.4744 - val_loss: 18249832.0000 - val_mae: 2710.7607\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21482800.0000 - mae: 2863.0830 - val_loss: 18072690.0000 - val_mae: 2603.3801\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21144242.0000 - mae: 2730.4592 - val_loss: 17847858.0000 - val_mae: 2517.0134\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21134952.0000 - mae: 2771.0884 - val_loss: 17873940.0000 - val_mae: 2489.4597\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21014816.0000 - mae: 2686.6323 - val_loss: 18451334.0000 - val_mae: 2782.4548\n",
      "Epoch 73/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21149274.0000 - mae: 2703.4106 - val_loss: 18168026.0000 - val_mae: 2709.4875\n",
      "Epoch 74/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21090518.0000 - mae: 2835.9373 - val_loss: 17925662.0000 - val_mae: 2567.2861\n",
      "Epoch 75/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20864990.0000 - mae: 2664.8772 - val_loss: 18698644.0000 - val_mae: 2864.3621\n",
      "Epoch 76/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21034204.0000 - mae: 2729.7839 - val_loss: 17909070.0000 - val_mae: 2501.9558\n",
      "Epoch 77/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 20794072.0000 - mae: 2757.2202 - val_loss: 17708824.0000 - val_mae: 2471.1895\n",
      "Epoch 78/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 20832898.0000 - mae: 2700.2705 - val_loss: 17901652.0000 - val_mae: 2534.0977\n",
      "Epoch 79/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20750968.0000 - mae: 2665.1255 - val_loss: 17753882.0000 - val_mae: 2480.1511\n",
      "Epoch 80/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20719126.0000 - mae: 2653.8472 - val_loss: 17927694.0000 - val_mae: 2629.6057\n",
      "Epoch 81/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20710648.0000 - mae: 2753.6292 - val_loss: 17738658.0000 - val_mae: 2409.6426\n",
      "Epoch 82/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20635280.0000 - mae: 2742.0806 - val_loss: 17702942.0000 - val_mae: 2329.8572\n",
      "Epoch 83/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20813944.0000 - mae: 2676.0205 - val_loss: 17915466.0000 - val_mae: 2288.3408\n",
      "Epoch 84/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21118316.0000 - mae: 2702.6211 - val_loss: 17627026.0000 - val_mae: 2541.8545\n",
      "Epoch 85/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20613632.0000 - mae: 2792.7839 - val_loss: 17533036.0000 - val_mae: 2348.9375\n",
      "Epoch 86/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20798716.0000 - mae: 2723.0693 - val_loss: 17633914.0000 - val_mae: 2294.9417\n",
      "Epoch 87/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20899512.0000 - mae: 2630.1384 - val_loss: 17599840.0000 - val_mae: 2316.2146\n",
      "Epoch 88/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20547824.0000 - mae: 2717.2695 - val_loss: 18186336.0000 - val_mae: 2826.7898\n",
      "Epoch 89/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20463716.0000 - mae: 2788.5552 - val_loss: 17580116.0000 - val_mae: 2253.7400\n",
      "Epoch 90/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20666140.0000 - mae: 2632.2336 - val_loss: 17810652.0000 - val_mae: 2640.0808\n",
      "Epoch 91/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20381392.0000 - mae: 2686.2129 - val_loss: 18101432.0000 - val_mae: 2707.3513\n",
      "Epoch 92/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20287424.0000 - mae: 2699.3503 - val_loss: 17547406.0000 - val_mae: 2355.8425\n",
      "Epoch 93/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20493798.0000 - mae: 2716.3831 - val_loss: 17570816.0000 - val_mae: 2541.7104\n",
      "Epoch 94/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20450414.0000 - mae: 2670.6790 - val_loss: 17529630.0000 - val_mae: 2442.2278\n",
      "Epoch 95/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20277348.0000 - mae: 2654.9648 - val_loss: 17539830.0000 - val_mae: 2533.8848\n",
      "Epoch 96/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 20267694.0000 - mae: 2645.0603 - val_loss: 17597830.0000 - val_mae: 2587.7849\n",
      "Epoch 97/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20207726.0000 - mae: 2618.8909 - val_loss: 17442916.0000 - val_mae: 2407.7012\n",
      "Epoch 98/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 20146718.0000 - mae: 2631.3826 - val_loss: 17454506.0000 - val_mae: 2471.7798\n",
      "Epoch 99/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20185626.0000 - mae: 2718.4204 - val_loss: 17490444.0000 - val_mae: 2456.7839\n",
      "Epoch 100/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20049310.0000 - mae: 2623.4492 - val_loss: 17365668.0000 - val_mae: 2355.7839\n",
      "Epoch 101/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20167208.0000 - mae: 2597.5056 - val_loss: 17447290.0000 - val_mae: 2437.5361\n",
      "Epoch 102/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20245062.0000 - mae: 2692.8447 - val_loss: 17703190.0000 - val_mae: 2611.6426\n",
      "Epoch 103/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20386584.0000 - mae: 2650.0427 - val_loss: 18161294.0000 - val_mae: 2785.2664\n",
      "Epoch 104/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 20331566.0000 - mae: 2677.2437 - val_loss: 17847746.0000 - val_mae: 2738.2205\n",
      "Epoch 105/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19965006.0000 - mae: 2663.4307 - val_loss: 17408326.0000 - val_mae: 2485.2925\n",
      "Epoch 106/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19937648.0000 - mae: 2668.0891 - val_loss: 17364672.0000 - val_mae: 2237.9941\n",
      "Epoch 107/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19884512.0000 - mae: 2678.9375 - val_loss: 17332810.0000 - val_mae: 2441.8149\n",
      "Epoch 108/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19829656.0000 - mae: 2584.2493 - val_loss: 17252250.0000 - val_mae: 2390.2703\n",
      "Epoch 109/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19892830.0000 - mae: 2598.7278 - val_loss: 17805524.0000 - val_mae: 2733.5813\n",
      "Epoch 110/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19831562.0000 - mae: 2589.5042 - val_loss: 18219334.0000 - val_mae: 2849.0505\n",
      "Epoch 111/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19866358.0000 - mae: 2663.1301 - val_loss: 17624270.0000 - val_mae: 2627.4163\n",
      "Epoch 112/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19918562.0000 - mae: 2646.6687 - val_loss: 17596456.0000 - val_mae: 2615.2551\n",
      "Epoch 113/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 19974662.0000 - mae: 2681.4888 - val_loss: 17363954.0000 - val_mae: 2454.1895\n",
      "Epoch 114/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19766204.0000 - mae: 2571.8928 - val_loss: 17253386.0000 - val_mae: 2477.8320\n",
      "Epoch 115/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19789344.0000 - mae: 2561.7290 - val_loss: 17425276.0000 - val_mae: 2595.8652\n",
      "Epoch 116/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19709346.0000 - mae: 2743.9573 - val_loss: 17171892.0000 - val_mae: 2401.9666\n",
      "Epoch 117/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19667430.0000 - mae: 2644.1316 - val_loss: 17224114.0000 - val_mae: 2266.2151\n",
      "Epoch 118/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19641740.0000 - mae: 2600.0811 - val_loss: 17237218.0000 - val_mae: 2334.9807\n",
      "Epoch 119/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 19580686.0000 - mae: 2562.6360 - val_loss: 17708558.0000 - val_mae: 2692.0505\n",
      "Epoch 120/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 19789202.0000 - mae: 2668.6099 - val_loss: 17574514.0000 - val_mae: 2191.3311\n",
      "Epoch 121/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19769116.0000 - mae: 2608.6069 - val_loss: 17555254.0000 - val_mae: 2660.5281\n",
      "Epoch 122/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 19536020.0000 - mae: 2688.5332 - val_loss: 17143800.0000 - val_mae: 2230.5591\n",
      "Epoch 123/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 19474862.0000 - mae: 2604.7639 - val_loss: 17655088.0000 - val_mae: 2621.2405\n",
      "Epoch 124/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 19478466.0000 - mae: 2513.4575 - val_loss: 17439052.0000 - val_mae: 2550.4070\n",
      "Epoch 125/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 19361788.0000 - mae: 2682.4458 - val_loss: 17196518.0000 - val_mae: 2376.4558\n",
      "Epoch 126/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19311016.0000 - mae: 2491.3037 - val_loss: 17404822.0000 - val_mae: 2591.5791\n",
      "Epoch 127/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19526490.0000 - mae: 2667.9829 - val_loss: 17073016.0000 - val_mae: 2368.2456\n",
      "Epoch 128/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19358236.0000 - mae: 2627.7791 - val_loss: 17232944.0000 - val_mae: 2355.7942\n",
      "Epoch 129/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19279640.0000 - mae: 2543.2061 - val_loss: 17251282.0000 - val_mae: 2457.3323\n",
      "Epoch 130/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19607158.0000 - mae: 2677.0317 - val_loss: 17133196.0000 - val_mae: 2351.8933\n",
      "Epoch 131/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19406462.0000 - mae: 2621.9070 - val_loss: 17144582.0000 - val_mae: 2293.0217\n",
      "Epoch 132/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19327504.0000 - mae: 2585.4536 - val_loss: 17494468.0000 - val_mae: 2661.1399\n",
      "Epoch 133/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19422658.0000 - mae: 2638.6301 - val_loss: 17981366.0000 - val_mae: 2801.0112\n",
      "Epoch 134/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19285286.0000 - mae: 2602.6582 - val_loss: 17251494.0000 - val_mae: 2492.4778\n",
      "Epoch 135/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 19305902.0000 - mae: 2569.3120 - val_loss: 17007302.0000 - val_mae: 2419.6553\n",
      "Epoch 136/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19105352.0000 - mae: 2628.2612 - val_loss: 17182010.0000 - val_mae: 2349.8948\n",
      "Epoch 137/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19317436.0000 - mae: 2592.5007 - val_loss: 17122294.0000 - val_mae: 2166.5811\n",
      "Epoch 138/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19176908.0000 - mae: 2567.8726 - val_loss: 16978706.0000 - val_mae: 2299.4026\n",
      "Epoch 139/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19344728.0000 - mae: 2594.7361 - val_loss: 17329952.0000 - val_mae: 2158.7178\n",
      "Epoch 140/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19257966.0000 - mae: 2635.5261 - val_loss: 16905758.0000 - val_mae: 2302.2974\n",
      "Epoch 141/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19059256.0000 - mae: 2580.7385 - val_loss: 16959102.0000 - val_mae: 2286.8508\n",
      "Epoch 142/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18902390.0000 - mae: 2571.7129 - val_loss: 16973964.0000 - val_mae: 2323.0186\n",
      "Epoch 143/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19046804.0000 - mae: 2475.3015 - val_loss: 17900110.0000 - val_mae: 2823.5222\n",
      "Epoch 144/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19169842.0000 - mae: 2651.7590 - val_loss: 17465406.0000 - val_mae: 2553.9185\n",
      "Epoch 145/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18760396.0000 - mae: 2523.1899 - val_loss: 17682158.0000 - val_mae: 2706.8350\n",
      "Epoch 146/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18846138.0000 - mae: 2563.2886 - val_loss: 17060246.0000 - val_mae: 2355.5398\n",
      "Epoch 147/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 19079600.0000 - mae: 2531.0767 - val_loss: 17123648.0000 - val_mae: 2464.7502\n",
      "Epoch 148/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 19001338.0000 - mae: 2623.7878 - val_loss: 17145638.0000 - val_mae: 2308.8892\n",
      "Epoch 149/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18994566.0000 - mae: 2494.3369 - val_loss: 17167452.0000 - val_mae: 2519.1335\n",
      "Epoch 150/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18874546.0000 - mae: 2548.5229 - val_loss: 18082368.0000 - val_mae: 2853.8171\n",
      "Epoch 151/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18933894.0000 - mae: 2561.9521 - val_loss: 17771432.0000 - val_mae: 2683.6516\n",
      "Epoch 152/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18894874.0000 - mae: 2640.6748 - val_loss: 17048768.0000 - val_mae: 2338.7507\n",
      "Epoch 153/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18767538.0000 - mae: 2509.2322 - val_loss: 17440414.0000 - val_mae: 2577.7529\n",
      "Epoch 154/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18712230.0000 - mae: 2491.5376 - val_loss: 17457078.0000 - val_mae: 2536.5889\n",
      "Epoch 155/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18835856.0000 - mae: 2518.6382 - val_loss: 17246044.0000 - val_mae: 2377.9089\n",
      "Epoch 156/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18877680.0000 - mae: 2585.4326 - val_loss: 17088570.0000 - val_mae: 2349.0183\n",
      "Epoch 157/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18781622.0000 - mae: 2544.1821 - val_loss: 17145730.0000 - val_mae: 2473.4282\n",
      "Epoch 158/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18675626.0000 - mae: 2603.6140 - val_loss: 17017900.0000 - val_mae: 2352.2864\n",
      "Epoch 159/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18844596.0000 - mae: 2509.8213 - val_loss: 16903016.0000 - val_mae: 2300.2700\n",
      "Epoch 160/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18612932.0000 - mae: 2459.2764 - val_loss: 17319348.0000 - val_mae: 2519.5322\n",
      "Epoch 161/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18922498.0000 - mae: 2578.5776 - val_loss: 18129064.0000 - val_mae: 2822.6394\n",
      "Epoch 162/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18732522.0000 - mae: 2573.9917 - val_loss: 17169216.0000 - val_mae: 2423.0547\n",
      "Epoch 163/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18829738.0000 - mae: 2490.6262 - val_loss: 17190336.0000 - val_mae: 2330.9749\n",
      "Epoch 164/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 18424310.0000 - mae: 2583.3672 - val_loss: 17033110.0000 - val_mae: 2265.8513\n",
      "Epoch 165/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18728776.0000 - mae: 2499.2498 - val_loss: 17108544.0000 - val_mae: 2424.5183\n",
      "Epoch 166/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18515750.0000 - mae: 2469.7729 - val_loss: 17532444.0000 - val_mae: 2569.1011\n",
      "Epoch 167/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18538820.0000 - mae: 2584.5745 - val_loss: 17118040.0000 - val_mae: 2513.7368\n",
      "Epoch 168/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18446902.0000 - mae: 2552.8589 - val_loss: 17382736.0000 - val_mae: 2480.2891\n",
      "Epoch 169/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 18459856.0000 - mae: 2499.2881 - val_loss: 18098856.0000 - val_mae: 2801.2747\n",
      "Epoch 170/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18714708.0000 - mae: 2561.2065 - val_loss: 17541654.0000 - val_mae: 2671.0959\n",
      "Epoch 171/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18502266.0000 - mae: 2544.1287 - val_loss: 17229460.0000 - val_mae: 2526.1990\n",
      "Epoch 172/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 18455256.0000 - mae: 2499.1570 - val_loss: 17155266.0000 - val_mae: 2140.8955\n",
      "Epoch 173/500\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 18497004.0000 - mae: 2520.6101 - val_loss: 17288976.0000 - val_mae: 2344.7136\n",
      "Epoch 174/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18310226.0000 - mae: 2580.0386 - val_loss: 17047198.0000 - val_mae: 2221.5969\n",
      "Epoch 175/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18450854.0000 - mae: 2439.7703 - val_loss: 17098160.0000 - val_mae: 2433.1450\n",
      "Epoch 176/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18377406.0000 - mae: 2556.8096 - val_loss: 17202542.0000 - val_mae: 2465.0823\n",
      "Epoch 177/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18239526.0000 - mae: 2461.5923 - val_loss: 17717384.0000 - val_mae: 2638.4971\n",
      "Epoch 178/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 18281164.0000 - mae: 2428.1921 - val_loss: 17309790.0000 - val_mae: 2501.0442\n",
      "Epoch 179/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18274490.0000 - mae: 2518.8301 - val_loss: 18302530.0000 - val_mae: 2858.0298\n",
      "Epoch 180/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18516394.0000 - mae: 2582.6448 - val_loss: 18164668.0000 - val_mae: 2856.2808\n",
      "Epoch 181/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18134382.0000 - mae: 2488.2092 - val_loss: 17140588.0000 - val_mae: 2373.5818\n",
      "Epoch 182/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18215316.0000 - mae: 2484.5664 - val_loss: 17389152.0000 - val_mae: 2487.8000\n",
      "Epoch 183/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18187178.0000 - mae: 2483.6877 - val_loss: 17240726.0000 - val_mae: 2329.0647\n",
      "Epoch 184/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18162252.0000 - mae: 2510.6699 - val_loss: 17193988.0000 - val_mae: 2474.6843\n",
      "Epoch 185/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 18239834.0000 - mae: 2529.1382 - val_loss: 17392058.0000 - val_mae: 2587.3198\n",
      "Epoch 186/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18108386.0000 - mae: 2506.4194 - val_loss: 16975462.0000 - val_mae: 2297.0703\n",
      "Epoch 187/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18013354.0000 - mae: 2515.8074 - val_loss: 17870672.0000 - val_mae: 2690.2441\n",
      "Epoch 188/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18180568.0000 - mae: 2494.1782 - val_loss: 17482534.0000 - val_mae: 2545.7781\n",
      "Epoch 189/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18070642.0000 - mae: 2509.1208 - val_loss: 17077248.0000 - val_mae: 2353.5708\n",
      "Epoch 190/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17966324.0000 - mae: 2539.8130 - val_loss: 17375778.0000 - val_mae: 2544.8076\n",
      "Epoch 191/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18122356.0000 - mae: 2498.1377 - val_loss: 17399942.0000 - val_mae: 2527.9880\n",
      "Epoch 192/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 18028758.0000 - mae: 2447.2825 - val_loss: 17072386.0000 - val_mae: 2316.8245\n",
      "Epoch 193/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 18089910.0000 - mae: 2460.5110 - val_loss: 17383110.0000 - val_mae: 2524.4653\n",
      "Epoch 194/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18107692.0000 - mae: 2510.9805 - val_loss: 18527702.0000 - val_mae: 2921.3430\n",
      "Epoch 195/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18092372.0000 - mae: 2565.9026 - val_loss: 17541752.0000 - val_mae: 2572.8352\n",
      "Epoch 196/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18049626.0000 - mae: 2459.1926 - val_loss: 16932774.0000 - val_mae: 2302.4253\n",
      "Epoch 197/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17986060.0000 - mae: 2505.5420 - val_loss: 17141860.0000 - val_mae: 2432.6956\n",
      "Epoch 198/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 18075056.0000 - mae: 2497.2388 - val_loss: 17221976.0000 - val_mae: 2318.7278\n",
      "Epoch 199/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17899792.0000 - mae: 2414.1104 - val_loss: 17688132.0000 - val_mae: 2662.4834\n",
      "Epoch 200/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17841190.0000 - mae: 2471.4192 - val_loss: 17095972.0000 - val_mae: 2382.9612\n",
      "Epoch 201/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17999750.0000 - mae: 2493.4485 - val_loss: 17507124.0000 - val_mae: 2563.1440\n",
      "Epoch 202/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 18034562.0000 - mae: 2540.2373 - val_loss: 17267736.0000 - val_mae: 2280.6589\n",
      "Epoch 203/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18381454.0000 - mae: 2544.1526 - val_loss: 17152804.0000 - val_mae: 2347.2847\n",
      "Epoch 204/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17918014.0000 - mae: 2511.2483 - val_loss: 16909974.0000 - val_mae: 2298.2524\n",
      "Epoch 205/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17753528.0000 - mae: 2405.9553 - val_loss: 17705678.0000 - val_mae: 2660.7639\n",
      "Epoch 206/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 17782250.0000 - mae: 2494.5076 - val_loss: 17712556.0000 - val_mae: 2666.6541\n",
      "Epoch 207/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17759990.0000 - mae: 2465.5544 - val_loss: 17736858.0000 - val_mae: 2686.1304\n",
      "Epoch 208/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 18025068.0000 - mae: 2586.6924 - val_loss: 17190966.0000 - val_mae: 2316.9517\n",
      "Epoch 209/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 17970054.0000 - mae: 2440.1514 - val_loss: 16982318.0000 - val_mae: 2310.9082\n",
      "Epoch 210/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17642788.0000 - mae: 2425.8728 - val_loss: 17386418.0000 - val_mae: 2468.6899\n",
      "Epoch 211/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17563928.0000 - mae: 2475.6125 - val_loss: 17538552.0000 - val_mae: 2536.1670\n",
      "Epoch 212/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17733916.0000 - mae: 2482.1692 - val_loss: 17252714.0000 - val_mae: 2501.8101\n",
      "Epoch 213/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17995384.0000 - mae: 2513.5120 - val_loss: 17138786.0000 - val_mae: 2248.7456\n",
      "Epoch 214/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17631472.0000 - mae: 2462.1846 - val_loss: 17225910.0000 - val_mae: 2293.5259\n",
      "Epoch 215/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17637592.0000 - mae: 2485.6582 - val_loss: 17032806.0000 - val_mae: 2391.2441\n",
      "Epoch 216/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17632408.0000 - mae: 2436.7278 - val_loss: 17241096.0000 - val_mae: 2482.5723\n",
      "Epoch 217/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17629288.0000 - mae: 2520.0105 - val_loss: 17097910.0000 - val_mae: 2449.3735\n",
      "Epoch 218/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17583498.0000 - mae: 2474.5774 - val_loss: 17365352.0000 - val_mae: 2478.2314\n",
      "Epoch 219/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17558748.0000 - mae: 2465.4016 - val_loss: 17739938.0000 - val_mae: 2691.4250\n",
      "Epoch 220/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17800678.0000 - mae: 2521.1514 - val_loss: 17411794.0000 - val_mae: 2523.4495\n",
      "Epoch 221/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17585762.0000 - mae: 2426.3005 - val_loss: 17476754.0000 - val_mae: 2258.1370\n",
      "Epoch 222/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 17758748.0000 - mae: 2458.0771 - val_loss: 17391470.0000 - val_mae: 2444.7363\n",
      "Epoch 223/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17438242.0000 - mae: 2443.6780 - val_loss: 18136838.0000 - val_mae: 2727.0220\n",
      "Epoch 224/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17557160.0000 - mae: 2429.1750 - val_loss: 17775424.0000 - val_mae: 2557.7798\n",
      "Epoch 225/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17492402.0000 - mae: 2541.2549 - val_loss: 17613746.0000 - val_mae: 2613.5017\n",
      "Epoch 226/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17488298.0000 - mae: 2470.3157 - val_loss: 17433386.0000 - val_mae: 2434.9165\n",
      "Epoch 227/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17536146.0000 - mae: 2505.1323 - val_loss: 17186990.0000 - val_mae: 2310.3379\n",
      "Epoch 228/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17578806.0000 - mae: 2442.9668 - val_loss: 17344262.0000 - val_mae: 2467.4514\n",
      "Epoch 229/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17464498.0000 - mae: 2408.8083 - val_loss: 17334198.0000 - val_mae: 2451.5452\n",
      "Epoch 230/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17393492.0000 - mae: 2437.7065 - val_loss: 18063254.0000 - val_mae: 2748.9746\n",
      "Epoch 231/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17510500.0000 - mae: 2498.3926 - val_loss: 17342346.0000 - val_mae: 2528.3494\n",
      "Epoch 232/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17266890.0000 - mae: 2420.9319 - val_loss: 18099648.0000 - val_mae: 2731.5791\n",
      "Epoch 233/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17411740.0000 - mae: 2482.1519 - val_loss: 17382760.0000 - val_mae: 2490.8408\n",
      "Epoch 234/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17301282.0000 - mae: 2434.6042 - val_loss: 17257056.0000 - val_mae: 2448.4629\n",
      "Epoch 235/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17290964.0000 - mae: 2458.8757 - val_loss: 17329692.0000 - val_mae: 2278.7454\n",
      "Epoch 236/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 17193242.0000 - mae: 2404.6396 - val_loss: 17656542.0000 - val_mae: 2505.3193\n",
      "Epoch 237/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17449150.0000 - mae: 2397.9824 - val_loss: 17278814.0000 - val_mae: 2368.2134\n",
      "Epoch 238/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17269046.0000 - mae: 2462.7217 - val_loss: 17870912.0000 - val_mae: 2667.7354\n",
      "Epoch 239/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17207918.0000 - mae: 2463.0447 - val_loss: 17432664.0000 - val_mae: 2334.0908\n",
      "Epoch 240/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17229126.0000 - mae: 2442.3940 - val_loss: 18076134.0000 - val_mae: 2715.3621\n",
      "Epoch 241/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17213298.0000 - mae: 2451.3628 - val_loss: 18008550.0000 - val_mae: 2709.4326\n",
      "Epoch 242/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 17288260.0000 - mae: 2449.5945 - val_loss: 17655124.0000 - val_mae: 2589.1372\n",
      "Epoch 243/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17244990.0000 - mae: 2423.5337 - val_loss: 17459526.0000 - val_mae: 2472.0166\n",
      "Epoch 244/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17055188.0000 - mae: 2409.5227 - val_loss: 18254066.0000 - val_mae: 2757.8018\n",
      "Epoch 245/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17277474.0000 - mae: 2521.2798 - val_loss: 17411298.0000 - val_mae: 2453.6848\n",
      "Epoch 246/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 17111070.0000 - mae: 2387.5396 - val_loss: 17314474.0000 - val_mae: 2471.5789\n",
      "Epoch 247/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17183648.0000 - mae: 2452.8623 - val_loss: 17554588.0000 - val_mae: 2537.2720\n",
      "Epoch 248/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17369374.0000 - mae: 2444.1511 - val_loss: 17874280.0000 - val_mae: 2586.4136\n",
      "Epoch 249/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17536008.0000 - mae: 2513.9204 - val_loss: 17731876.0000 - val_mae: 2390.3186\n",
      "Epoch 250/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 17254700.0000 - mae: 2503.0452 - val_loss: 17469906.0000 - val_mae: 2179.9729\n",
      "Epoch 251/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17234100.0000 - mae: 2455.7468 - val_loss: 17312320.0000 - val_mae: 2351.4792\n",
      "Epoch 252/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 16976798.0000 - mae: 2382.5930 - val_loss: 17552510.0000 - val_mae: 2387.7139\n",
      "Epoch 253/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16986802.0000 - mae: 2424.3293 - val_loss: 17259486.0000 - val_mae: 2323.7332\n",
      "Epoch 254/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 17016856.0000 - mae: 2422.0742 - val_loss: 17611818.0000 - val_mae: 2437.3621\n",
      "Epoch 255/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16979076.0000 - mae: 2421.9600 - val_loss: 18270998.0000 - val_mae: 2700.2009\n",
      "Epoch 256/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 17104916.0000 - mae: 2457.5391 - val_loss: 17411994.0000 - val_mae: 2386.9407\n",
      "Epoch 257/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 17218912.0000 - mae: 2449.4163 - val_loss: 17429942.0000 - val_mae: 2402.8816\n",
      "Epoch 258/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16805422.0000 - mae: 2438.8254 - val_loss: 17818656.0000 - val_mae: 2253.7390\n",
      "Epoch 259/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17239276.0000 - mae: 2463.7778 - val_loss: 17351346.0000 - val_mae: 2305.1633\n",
      "Epoch 260/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 17100354.0000 - mae: 2434.4170 - val_loss: 17841972.0000 - val_mae: 2351.6841\n",
      "Epoch 261/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16771286.0000 - mae: 2416.6597 - val_loss: 17484706.0000 - val_mae: 2172.8328\n",
      "Epoch 262/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 17164284.0000 - mae: 2443.3101 - val_loss: 17462754.0000 - val_mae: 2279.3057\n",
      "Epoch 263/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16692000.0000 - mae: 2362.2988 - val_loss: 17993700.0000 - val_mae: 2630.8816\n",
      "Epoch 264/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16908966.0000 - mae: 2403.2952 - val_loss: 17948446.0000 - val_mae: 2670.6606\n",
      "Epoch 265/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 17115186.0000 - mae: 2527.2085 - val_loss: 17671448.0000 - val_mae: 2453.5500\n",
      "Epoch 266/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 16900116.0000 - mae: 2416.2300 - val_loss: 17758534.0000 - val_mae: 2400.6638\n",
      "Epoch 267/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 16950522.0000 - mae: 2444.0276 - val_loss: 17800224.0000 - val_mae: 2480.1914\n",
      "Epoch 268/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 16740861.0000 - mae: 2378.3311 - val_loss: 17607732.0000 - val_mae: 2405.5369\n",
      "Epoch 269/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 16620312.0000 - mae: 2410.8552 - val_loss: 18328512.0000 - val_mae: 2656.9724\n",
      "Epoch 270/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 16830116.0000 - mae: 2450.1233 - val_loss: 17736780.0000 - val_mae: 2372.7056\n",
      "Epoch 271/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 16880188.0000 - mae: 2398.1985 - val_loss: 17574662.0000 - val_mae: 2489.1606\n",
      "Epoch 272/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 16741527.0000 - mae: 2432.0649 - val_loss: 17674932.0000 - val_mae: 2490.4778\n",
      "Epoch 273/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 16748642.0000 - mae: 2355.1104 - val_loss: 18875210.0000 - val_mae: 2851.6069\n",
      "Epoch 274/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16691964.0000 - mae: 2437.4163 - val_loss: 17388028.0000 - val_mae: 2444.1936\n",
      "Epoch 275/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16582362.0000 - mae: 2409.9150 - val_loss: 18269512.0000 - val_mae: 2737.2725\n",
      "Epoch 276/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16908586.0000 - mae: 2477.2793 - val_loss: 18012730.0000 - val_mae: 2589.8164\n",
      "Epoch 277/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16996486.0000 - mae: 2501.8481 - val_loss: 17443958.0000 - val_mae: 2469.5488\n",
      "Epoch 278/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16991848.0000 - mae: 2483.1262 - val_loss: 17743562.0000 - val_mae: 2175.7090\n",
      "Epoch 279/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16804056.0000 - mae: 2438.6865 - val_loss: 17614522.0000 - val_mae: 2454.3782\n",
      "Epoch 280/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16564300.0000 - mae: 2441.1680 - val_loss: 17619206.0000 - val_mae: 2272.8823\n",
      "Epoch 281/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 16472558.0000 - mae: 2378.1230 - val_loss: 17670950.0000 - val_mae: 2448.2158\n",
      "Epoch 282/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 16480904.0000 - mae: 2466.6260 - val_loss: 17358832.0000 - val_mae: 2195.5374\n",
      "Epoch 283/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16601192.0000 - mae: 2336.3882 - val_loss: 18031886.0000 - val_mae: 2552.9812\n",
      "Epoch 284/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16557872.0000 - mae: 2423.1226 - val_loss: 17889794.0000 - val_mae: 2596.6958\n",
      "Epoch 285/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16556605.0000 - mae: 2395.1599 - val_loss: 17916426.0000 - val_mae: 2589.6289\n",
      "Epoch 286/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16546179.0000 - mae: 2403.2910 - val_loss: 18244246.0000 - val_mae: 2709.9841\n",
      "Epoch 287/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16299879.0000 - mae: 2443.7517 - val_loss: 17707006.0000 - val_mae: 2567.3345\n",
      "Epoch 288/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16491197.0000 - mae: 2437.7419 - val_loss: 17440308.0000 - val_mae: 2221.7737\n",
      "Epoch 289/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16593079.0000 - mae: 2409.7693 - val_loss: 17417078.0000 - val_mae: 2222.5703\n",
      "Epoch 290/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 16743653.0000 - mae: 2420.4656 - val_loss: 18190806.0000 - val_mae: 2703.4202\n",
      "Epoch 291/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16334096.0000 - mae: 2393.9207 - val_loss: 17680980.0000 - val_mae: 2538.7444\n",
      "Epoch 292/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16197189.0000 - mae: 2388.4226 - val_loss: 17882220.0000 - val_mae: 2580.2163\n",
      "Epoch 293/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16273556.0000 - mae: 2421.7229 - val_loss: 17217140.0000 - val_mae: 2313.2595\n",
      "Epoch 294/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16391145.0000 - mae: 2385.8723 - val_loss: 17920188.0000 - val_mae: 2386.4565\n",
      "Epoch 295/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16396260.0000 - mae: 2354.0168 - val_loss: 17560710.0000 - val_mae: 2433.0864\n",
      "Epoch 296/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 16324098.0000 - mae: 2472.9714 - val_loss: 17721138.0000 - val_mae: 2499.1367\n",
      "Epoch 297/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16377629.0000 - mae: 2407.5889 - val_loss: 17531328.0000 - val_mae: 2460.6348\n",
      "Epoch 298/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16166911.0000 - mae: 2350.3269 - val_loss: 17910102.0000 - val_mae: 2464.5547\n",
      "Epoch 299/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16229824.0000 - mae: 2450.0869 - val_loss: 18128948.0000 - val_mae: 2643.6843\n",
      "Epoch 300/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 16400998.0000 - mae: 2405.8909 - val_loss: 17722434.0000 - val_mae: 2391.6873\n",
      "Epoch 301/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 16130366.0000 - mae: 2389.3276 - val_loss: 17993758.0000 - val_mae: 2533.2129\n",
      "Epoch 302/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 15951280.0000 - mae: 2348.1548 - val_loss: 17993416.0000 - val_mae: 2556.0811\n",
      "Epoch 303/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 16039534.0000 - mae: 2412.0278 - val_loss: 17750938.0000 - val_mae: 2429.6326\n",
      "Epoch 304/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15891567.0000 - mae: 2264.4839 - val_loss: 20237082.0000 - val_mae: 3158.3621\n",
      "Epoch 305/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16662230.0000 - mae: 2545.7722 - val_loss: 18046296.0000 - val_mae: 2588.4204\n",
      "Epoch 306/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15981457.0000 - mae: 2378.0696 - val_loss: 17755446.0000 - val_mae: 2442.3621\n",
      "Epoch 307/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15930858.0000 - mae: 2423.0059 - val_loss: 17678184.0000 - val_mae: 2287.0139\n",
      "Epoch 308/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16270275.0000 - mae: 2362.3936 - val_loss: 18093246.0000 - val_mae: 2711.9893\n",
      "Epoch 309/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15936529.0000 - mae: 2369.3535 - val_loss: 17997694.0000 - val_mae: 2516.7195\n",
      "Epoch 310/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15970137.0000 - mae: 2352.9888 - val_loss: 18036072.0000 - val_mae: 2468.6279\n",
      "Epoch 311/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 15995773.0000 - mae: 2448.9424 - val_loss: 18438082.0000 - val_mae: 2643.9839\n",
      "Epoch 312/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15867128.0000 - mae: 2318.7034 - val_loss: 18170062.0000 - val_mae: 2495.7769\n",
      "Epoch 313/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15724688.0000 - mae: 2437.3472 - val_loss: 17972020.0000 - val_mae: 2566.7520\n",
      "Epoch 314/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15962870.0000 - mae: 2392.3042 - val_loss: 18091318.0000 - val_mae: 2370.9221\n",
      "Epoch 315/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16355624.0000 - mae: 2471.9575 - val_loss: 17988632.0000 - val_mae: 2258.3093\n",
      "Epoch 316/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15808178.0000 - mae: 2432.6245 - val_loss: 17760426.0000 - val_mae: 2376.1169\n",
      "Epoch 317/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 16123889.0000 - mae: 2437.1470 - val_loss: 21277368.0000 - val_mae: 3315.9563\n",
      "Epoch 318/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 16033043.0000 - mae: 2492.5391 - val_loss: 17959448.0000 - val_mae: 2372.9014\n",
      "Epoch 319/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15728781.0000 - mae: 2330.2732 - val_loss: 19393306.0000 - val_mae: 2938.2908\n",
      "Epoch 320/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15813334.0000 - mae: 2446.5061 - val_loss: 17993064.0000 - val_mae: 2336.2991\n",
      "Epoch 321/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15714104.0000 - mae: 2328.9126 - val_loss: 18923008.0000 - val_mae: 2799.9365\n",
      "Epoch 322/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15653796.0000 - mae: 2370.4890 - val_loss: 19003886.0000 - val_mae: 2814.4360\n",
      "Epoch 323/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15635406.0000 - mae: 2315.5957 - val_loss: 18547180.0000 - val_mae: 2754.5728\n",
      "Epoch 324/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15869685.0000 - mae: 2440.4282 - val_loss: 17966038.0000 - val_mae: 2441.3501\n",
      "Epoch 325/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15595771.0000 - mae: 2260.4138 - val_loss: 18778074.0000 - val_mae: 2757.6687\n",
      "Epoch 326/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15853168.0000 - mae: 2383.4058 - val_loss: 18090544.0000 - val_mae: 2452.9792\n",
      "Epoch 327/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 15920272.0000 - mae: 2431.7847 - val_loss: 18720224.0000 - val_mae: 2768.2170\n",
      "Epoch 328/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 15536517.0000 - mae: 2353.2683 - val_loss: 18019030.0000 - val_mae: 2341.6746\n",
      "Epoch 329/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15651713.0000 - mae: 2324.2148 - val_loss: 18070204.0000 - val_mae: 2551.7600\n",
      "Epoch 330/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15689165.0000 - mae: 2405.2559 - val_loss: 18541914.0000 - val_mae: 2720.1379\n",
      "Epoch 331/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15908349.0000 - mae: 2478.5793 - val_loss: 18222784.0000 - val_mae: 2538.2234\n",
      "Epoch 332/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15377945.0000 - mae: 2293.4033 - val_loss: 18254110.0000 - val_mae: 2458.9663\n",
      "Epoch 333/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15521290.0000 - mae: 2335.7266 - val_loss: 18424040.0000 - val_mae: 2667.4836\n",
      "Epoch 334/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 15375865.0000 - mae: 2341.7068 - val_loss: 18660024.0000 - val_mae: 2711.1707\n",
      "Epoch 335/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15466216.0000 - mae: 2382.5542 - val_loss: 20126784.0000 - val_mae: 3018.8230\n",
      "Epoch 336/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15600974.0000 - mae: 2408.2317 - val_loss: 18244648.0000 - val_mae: 2564.9495\n",
      "Epoch 337/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15442179.0000 - mae: 2280.7759 - val_loss: 18963180.0000 - val_mae: 2811.5652\n",
      "Epoch 338/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15485988.0000 - mae: 2410.2124 - val_loss: 18471584.0000 - val_mae: 2340.7070\n",
      "Epoch 339/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15526752.0000 - mae: 2367.1218 - val_loss: 18181408.0000 - val_mae: 2294.2532\n",
      "Epoch 340/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 15567707.0000 - mae: 2410.9270 - val_loss: 18943584.0000 - val_mae: 2744.8933\n",
      "Epoch 341/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 15412632.0000 - mae: 2258.0239 - val_loss: 18768886.0000 - val_mae: 2788.4365\n",
      "Epoch 342/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15390132.0000 - mae: 2427.8286 - val_loss: 18529046.0000 - val_mae: 2536.5908\n",
      "Epoch 343/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15561507.0000 - mae: 2328.9810 - val_loss: 19735042.0000 - val_mae: 2953.2498\n",
      "Epoch 344/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15404458.0000 - mae: 2374.3398 - val_loss: 18742426.0000 - val_mae: 2374.1213\n",
      "Epoch 345/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15533930.0000 - mae: 2300.8726 - val_loss: 19506574.0000 - val_mae: 2932.3499\n",
      "Epoch 346/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 15538233.0000 - mae: 2375.7764 - val_loss: 18265970.0000 - val_mae: 2633.3364\n",
      "Epoch 347/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15244734.0000 - mae: 2392.5063 - val_loss: 18107110.0000 - val_mae: 2477.7827\n",
      "Epoch 348/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15221341.0000 - mae: 2285.5425 - val_loss: 19011572.0000 - val_mae: 2715.0879\n",
      "Epoch 349/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15041972.0000 - mae: 2306.8984 - val_loss: 18989282.0000 - val_mae: 2754.6011\n",
      "Epoch 350/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 15141493.0000 - mae: 2371.6145 - val_loss: 18079372.0000 - val_mae: 2376.4089\n",
      "Epoch 351/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 15364791.0000 - mae: 2356.5774 - val_loss: 18005098.0000 - val_mae: 2442.7554\n",
      "Epoch 352/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 15133261.0000 - mae: 2265.1467 - val_loss: 19756852.0000 - val_mae: 2920.4976\n",
      "Epoch 353/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 15173836.0000 - mae: 2320.5754 - val_loss: 18280958.0000 - val_mae: 2321.4253\n",
      "Epoch 354/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15230542.0000 - mae: 2329.7434 - val_loss: 19086102.0000 - val_mae: 2700.5303\n",
      "Epoch 355/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15331107.0000 - mae: 2430.0349 - val_loss: 18447962.0000 - val_mae: 2528.8203\n",
      "Epoch 356/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15204256.0000 - mae: 2269.4690 - val_loss: 19507460.0000 - val_mae: 2875.0879\n",
      "Epoch 357/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 15481533.0000 - mae: 2538.3450 - val_loss: 19232176.0000 - val_mae: 2716.1401\n",
      "Epoch 358/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14892618.0000 - mae: 2247.3308 - val_loss: 20489692.0000 - val_mae: 3097.8220\n",
      "Epoch 359/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 15319302.0000 - mae: 2438.8594 - val_loss: 18683892.0000 - val_mae: 2584.8086\n",
      "Epoch 360/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14825527.0000 - mae: 2315.6621 - val_loss: 18697244.0000 - val_mae: 2507.0349\n",
      "Epoch 361/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15043458.0000 - mae: 2333.0974 - val_loss: 18994506.0000 - val_mae: 2688.5830\n",
      "Epoch 362/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14964840.0000 - mae: 2310.3464 - val_loss: 18200328.0000 - val_mae: 2479.4197\n",
      "Epoch 363/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14953825.0000 - mae: 2360.2224 - val_loss: 18632870.0000 - val_mae: 2536.7197\n",
      "Epoch 364/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14942560.0000 - mae: 2271.3928 - val_loss: 18582930.0000 - val_mae: 2577.3872\n",
      "Epoch 365/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14761744.0000 - mae: 2359.6953 - val_loss: 19149496.0000 - val_mae: 2672.1738\n",
      "Epoch 366/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14978143.0000 - mae: 2313.4883 - val_loss: 18087652.0000 - val_mae: 2430.4370\n",
      "Epoch 367/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14768571.0000 - mae: 2250.3760 - val_loss: 19502238.0000 - val_mae: 2736.8901\n",
      "Epoch 368/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 14736609.0000 - mae: 2302.2913 - val_loss: 18276068.0000 - val_mae: 2407.4331\n",
      "Epoch 369/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14799542.0000 - mae: 2342.5027 - val_loss: 18676502.0000 - val_mae: 2437.9360\n",
      "Epoch 370/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15028666.0000 - mae: 2311.1094 - val_loss: 18236950.0000 - val_mae: 2516.4084\n",
      "Epoch 371/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14654610.0000 - mae: 2296.9387 - val_loss: 19588364.0000 - val_mae: 2773.4551\n",
      "Epoch 372/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 14648139.0000 - mae: 2326.1599 - val_loss: 18196768.0000 - val_mae: 2487.7634\n",
      "Epoch 373/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14701425.0000 - mae: 2280.5728 - val_loss: 18633642.0000 - val_mae: 2502.1306\n",
      "Epoch 374/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14965409.0000 - mae: 2383.3940 - val_loss: 18815802.0000 - val_mae: 2579.8254\n",
      "Epoch 375/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15332944.0000 - mae: 2338.7966 - val_loss: 19911990.0000 - val_mae: 2843.5498\n",
      "Epoch 376/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15085365.0000 - mae: 2429.4053 - val_loss: 18932824.0000 - val_mae: 2583.5435\n",
      "Epoch 377/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14779179.0000 - mae: 2303.8254 - val_loss: 18459232.0000 - val_mae: 2490.1326\n",
      "Epoch 378/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 15057188.0000 - mae: 2415.2522 - val_loss: 19078774.0000 - val_mae: 2582.2798\n",
      "Epoch 379/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14670184.0000 - mae: 2343.3298 - val_loss: 18363444.0000 - val_mae: 2437.7083\n",
      "Epoch 380/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14880966.0000 - mae: 2242.6643 - val_loss: 18917744.0000 - val_mae: 2685.4866\n",
      "Epoch 381/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14475412.0000 - mae: 2361.6113 - val_loss: 18525214.0000 - val_mae: 2417.9727\n",
      "Epoch 382/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14538696.0000 - mae: 2247.6621 - val_loss: 18738738.0000 - val_mae: 2574.4526\n",
      "Epoch 383/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14428856.0000 - mae: 2309.7332 - val_loss: 18975098.0000 - val_mae: 2681.2683\n",
      "Epoch 384/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14389398.0000 - mae: 2279.8486 - val_loss: 19778198.0000 - val_mae: 2835.3745\n",
      "Epoch 385/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14360080.0000 - mae: 2288.5637 - val_loss: 18628264.0000 - val_mae: 2408.0979\n",
      "Epoch 386/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14962330.0000 - mae: 2421.4126 - val_loss: 18821598.0000 - val_mae: 2572.8250\n",
      "Epoch 387/500\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 14299941.0000 - mae: 2254.7102 - val_loss: 19397830.0000 - val_mae: 2698.9983\n",
      "Epoch 388/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14330079.0000 - mae: 2284.2617 - val_loss: 18890846.0000 - val_mae: 2500.0991\n",
      "Epoch 389/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14313748.0000 - mae: 2278.4636 - val_loss: 21940112.0000 - val_mae: 3208.7578\n",
      "Epoch 390/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14930021.0000 - mae: 2390.0569 - val_loss: 19773706.0000 - val_mae: 2550.9956\n",
      "Epoch 391/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 14851076.0000 - mae: 2369.0591 - val_loss: 19443308.0000 - val_mae: 2666.8384\n",
      "Epoch 392/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14342502.0000 - mae: 2301.1377 - val_loss: 19513612.0000 - val_mae: 2624.6633\n",
      "Epoch 393/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14452919.0000 - mae: 2280.3862 - val_loss: 19226326.0000 - val_mae: 2755.6941\n",
      "Epoch 394/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14208643.0000 - mae: 2341.2183 - val_loss: 19251028.0000 - val_mae: 2664.4075\n",
      "Epoch 395/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 14354462.0000 - mae: 2295.3936 - val_loss: 20911600.0000 - val_mae: 3040.4946\n",
      "Epoch 396/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 14206456.0000 - mae: 2295.5952 - val_loss: 19346216.0000 - val_mae: 2744.3635\n",
      "Epoch 397/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14173180.0000 - mae: 2314.3848 - val_loss: 19364964.0000 - val_mae: 2652.7383\n",
      "Epoch 398/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14320003.0000 - mae: 2285.2080 - val_loss: 19298748.0000 - val_mae: 2670.0496\n",
      "Epoch 399/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14509663.0000 - mae: 2347.5342 - val_loss: 20251346.0000 - val_mae: 2835.1504\n",
      "Epoch 400/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 14486716.0000 - mae: 2349.0442 - val_loss: 19466562.0000 - val_mae: 2667.2173\n",
      "Epoch 401/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13933932.0000 - mae: 2228.2546 - val_loss: 20461776.0000 - val_mae: 2959.4666\n",
      "Epoch 402/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14356175.0000 - mae: 2300.9521 - val_loss: 19476632.0000 - val_mae: 2658.3804\n",
      "Epoch 403/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14376847.0000 - mae: 2344.7095 - val_loss: 20142910.0000 - val_mae: 2911.6609\n",
      "Epoch 404/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14196652.0000 - mae: 2296.3394 - val_loss: 19755760.0000 - val_mae: 2712.4607\n",
      "Epoch 405/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14162403.0000 - mae: 2297.8645 - val_loss: 19293902.0000 - val_mae: 2648.1094\n",
      "Epoch 406/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 14228905.0000 - mae: 2313.0759 - val_loss: 19073174.0000 - val_mae: 2582.6367\n",
      "Epoch 407/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13912121.0000 - mae: 2217.3005 - val_loss: 21115528.0000 - val_mae: 3110.6868\n",
      "Epoch 408/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14126382.0000 - mae: 2304.0776 - val_loss: 19355024.0000 - val_mae: 2645.4072\n",
      "Epoch 409/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 13890699.0000 - mae: 2286.9246 - val_loss: 20183032.0000 - val_mae: 2825.5540\n",
      "Epoch 410/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14100497.0000 - mae: 2286.6995 - val_loss: 19774996.0000 - val_mae: 2754.4785\n",
      "Epoch 411/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 13867690.0000 - mae: 2278.9177 - val_loss: 18977886.0000 - val_mae: 2561.6331\n",
      "Epoch 412/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 13914803.0000 - mae: 2258.6963 - val_loss: 19533382.0000 - val_mae: 2722.8506\n",
      "Epoch 413/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13891183.0000 - mae: 2307.7808 - val_loss: 19426176.0000 - val_mae: 2647.7808\n",
      "Epoch 414/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13884460.0000 - mae: 2239.3840 - val_loss: 19660926.0000 - val_mae: 2733.7812\n",
      "Epoch 415/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13931424.0000 - mae: 2258.1531 - val_loss: 20042520.0000 - val_mae: 2814.8862\n",
      "Epoch 416/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13832814.0000 - mae: 2301.5210 - val_loss: 19633506.0000 - val_mae: 2668.1118\n",
      "Epoch 417/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13949752.0000 - mae: 2297.5483 - val_loss: 20336920.0000 - val_mae: 2822.4165\n",
      "Epoch 418/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13827626.0000 - mae: 2205.3152 - val_loss: 20327024.0000 - val_mae: 2908.8750\n",
      "Epoch 419/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14008125.0000 - mae: 2345.2134 - val_loss: 21842364.0000 - val_mae: 3125.7803\n",
      "Epoch 420/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14325854.0000 - mae: 2324.8184 - val_loss: 20187450.0000 - val_mae: 2798.5710\n",
      "Epoch 421/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13766061.0000 - mae: 2321.2761 - val_loss: 20560330.0000 - val_mae: 2889.6587\n",
      "Epoch 422/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 13892129.0000 - mae: 2254.3782 - val_loss: 19917120.0000 - val_mae: 2754.8755\n",
      "Epoch 423/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 14004878.0000 - mae: 2395.4651 - val_loss: 19370276.0000 - val_mae: 2538.2905\n",
      "Epoch 424/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13763718.0000 - mae: 2233.5664 - val_loss: 20713892.0000 - val_mae: 2968.6226\n",
      "Epoch 425/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13534968.0000 - mae: 2257.8223 - val_loss: 19289220.0000 - val_mae: 2576.6968\n",
      "Epoch 426/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13798003.0000 - mae: 2293.7419 - val_loss: 19770142.0000 - val_mae: 2669.6404\n",
      "Epoch 427/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13568086.0000 - mae: 2238.0271 - val_loss: 19602408.0000 - val_mae: 2574.3291\n",
      "Epoch 428/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13745222.0000 - mae: 2282.5178 - val_loss: 20505642.0000 - val_mae: 2835.6306\n",
      "Epoch 429/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13564509.0000 - mae: 2298.5920 - val_loss: 19367694.0000 - val_mae: 2620.5930\n",
      "Epoch 430/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13729279.0000 - mae: 2260.9602 - val_loss: 19314440.0000 - val_mae: 2612.1907\n",
      "Epoch 431/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13963074.0000 - mae: 2296.7124 - val_loss: 20185520.0000 - val_mae: 2743.3923\n",
      "Epoch 432/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13674311.0000 - mae: 2297.6895 - val_loss: 19884068.0000 - val_mae: 2631.6379\n",
      "Epoch 433/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13504263.0000 - mae: 2281.9304 - val_loss: 20316568.0000 - val_mae: 2794.1318\n",
      "Epoch 434/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 13457423.0000 - mae: 2240.8064 - val_loss: 21250630.0000 - val_mae: 2989.4009\n",
      "Epoch 435/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13422603.0000 - mae: 2247.3965 - val_loss: 21489228.0000 - val_mae: 3038.2214\n",
      "Epoch 436/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13492575.0000 - mae: 2298.2095 - val_loss: 21322946.0000 - val_mae: 2928.7720\n",
      "Epoch 437/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13421004.0000 - mae: 2247.2412 - val_loss: 20664042.0000 - val_mae: 2929.2063\n",
      "Epoch 438/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13285973.0000 - mae: 2210.8242 - val_loss: 22965526.0000 - val_mae: 3274.0286\n",
      "Epoch 439/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13612260.0000 - mae: 2324.5110 - val_loss: 20225270.0000 - val_mae: 2661.5227\n",
      "Epoch 440/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 13322309.0000 - mae: 2294.5928 - val_loss: 20132648.0000 - val_mae: 2698.7017\n",
      "Epoch 441/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13360868.0000 - mae: 2252.6582 - val_loss: 19572064.0000 - val_mae: 2659.8271\n",
      "Epoch 442/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 13418303.0000 - mae: 2272.9207 - val_loss: 22395068.0000 - val_mae: 3146.1016\n",
      "Epoch 443/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13275980.0000 - mae: 2238.0610 - val_loss: 21604992.0000 - val_mae: 3106.4490\n",
      "Epoch 444/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13439962.0000 - mae: 2267.2354 - val_loss: 20670270.0000 - val_mae: 2801.7151\n",
      "Epoch 445/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13167271.0000 - mae: 2266.4609 - val_loss: 22480450.0000 - val_mae: 3099.1960\n",
      "Epoch 446/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 13518488.0000 - mae: 2222.3789 - val_loss: 21471800.0000 - val_mae: 2986.5645\n",
      "Epoch 447/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13404641.0000 - mae: 2319.6060 - val_loss: 20423152.0000 - val_mae: 2604.6467\n",
      "Epoch 448/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13589830.0000 - mae: 2278.7217 - val_loss: 19968982.0000 - val_mae: 2694.3806\n",
      "Epoch 449/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 13213042.0000 - mae: 2237.5339 - val_loss: 21670838.0000 - val_mae: 3041.3723\n",
      "Epoch 450/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13015038.0000 - mae: 2271.9836 - val_loss: 20342948.0000 - val_mae: 2719.0483\n",
      "Epoch 451/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 13062474.0000 - mae: 2223.6487 - val_loss: 20827908.0000 - val_mae: 2802.6326\n",
      "Epoch 452/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 13440096.0000 - mae: 2276.1868 - val_loss: 22169410.0000 - val_mae: 3123.6140\n",
      "Epoch 453/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 13391751.0000 - mae: 2240.2505 - val_loss: 22099390.0000 - val_mae: 2993.5586\n",
      "Epoch 454/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 13160922.0000 - mae: 2312.6301 - val_loss: 20248518.0000 - val_mae: 2639.5979\n",
      "Epoch 455/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 13313474.0000 - mae: 2246.2339 - val_loss: 20724458.0000 - val_mae: 2895.5425\n",
      "Epoch 456/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 13007245.0000 - mae: 2228.5688 - val_loss: 20639994.0000 - val_mae: 2853.7200\n",
      "Epoch 457/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 13026631.0000 - mae: 2251.7957 - val_loss: 21121632.0000 - val_mae: 2885.6116\n",
      "Epoch 458/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 13103241.0000 - mae: 2221.9192 - val_loss: 20989034.0000 - val_mae: 2924.3003\n",
      "Epoch 459/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 13149300.0000 - mae: 2236.2766 - val_loss: 21275210.0000 - val_mae: 2931.6096\n",
      "Epoch 460/500\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 12891565.0000 - mae: 2195.3906 - val_loss: 24048204.0000 - val_mae: 3373.7368\n",
      "Epoch 461/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 13140381.0000 - mae: 2225.3191 - val_loss: 22387590.0000 - val_mae: 3167.6841\n",
      "Epoch 462/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 12574297.0000 - mae: 2293.1741 - val_loss: 20525774.0000 - val_mae: 2658.6167\n",
      "Epoch 463/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 13121861.0000 - mae: 2227.8252 - val_loss: 20891270.0000 - val_mae: 2834.8711\n",
      "Epoch 464/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 12958172.0000 - mae: 2254.6045 - val_loss: 21172432.0000 - val_mae: 2820.0183\n",
      "Epoch 465/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 13211428.0000 - mae: 2241.2278 - val_loss: 21608630.0000 - val_mae: 3006.9524\n",
      "Epoch 466/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 12973167.0000 - mae: 2289.8596 - val_loss: 21350764.0000 - val_mae: 2922.9871\n",
      "Epoch 467/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 12789544.0000 - mae: 2247.4658 - val_loss: 20931764.0000 - val_mae: 2791.6953\n",
      "Epoch 468/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 13114639.0000 - mae: 2234.6133 - val_loss: 21009860.0000 - val_mae: 2869.5884\n",
      "Epoch 469/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12775876.0000 - mae: 2236.1411 - val_loss: 22630988.0000 - val_mae: 3140.6208\n",
      "Epoch 470/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12977882.0000 - mae: 2245.5835 - val_loss: 21058544.0000 - val_mae: 2838.8008\n",
      "Epoch 471/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 12644324.0000 - mae: 2208.4756 - val_loss: 21050736.0000 - val_mae: 2875.1577\n",
      "Epoch 472/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12698165.0000 - mae: 2186.9456 - val_loss: 22936162.0000 - val_mae: 3187.6804\n",
      "Epoch 473/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12868692.0000 - mae: 2283.2422 - val_loss: 20510232.0000 - val_mae: 2713.4995\n",
      "Epoch 474/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12771810.0000 - mae: 2206.3716 - val_loss: 21573828.0000 - val_mae: 3040.9038\n",
      "Epoch 475/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 12848196.0000 - mae: 2273.9824 - val_loss: 21458776.0000 - val_mae: 2900.9221\n",
      "Epoch 476/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12635496.0000 - mae: 2210.4749 - val_loss: 21590756.0000 - val_mae: 2907.5481\n",
      "Epoch 477/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 12682233.0000 - mae: 2223.3640 - val_loss: 21264660.0000 - val_mae: 2908.3362\n",
      "Epoch 478/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12672965.0000 - mae: 2223.8459 - val_loss: 22160210.0000 - val_mae: 3051.4373\n",
      "Epoch 479/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12508089.0000 - mae: 2197.1445 - val_loss: 22708450.0000 - val_mae: 3080.7056\n",
      "Epoch 480/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12556979.0000 - mae: 2213.9185 - val_loss: 20943796.0000 - val_mae: 2814.3799\n",
      "Epoch 481/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12682522.0000 - mae: 2223.5342 - val_loss: 22133230.0000 - val_mae: 3054.5088\n",
      "Epoch 482/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12386911.0000 - mae: 2230.1404 - val_loss: 22009656.0000 - val_mae: 2889.0144\n",
      "Epoch 483/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12519354.0000 - mae: 2185.9604 - val_loss: 22924358.0000 - val_mae: 3250.7000\n",
      "Epoch 484/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12696422.0000 - mae: 2240.7869 - val_loss: 22494350.0000 - val_mae: 3043.2327\n",
      "Epoch 485/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 12558887.0000 - mae: 2226.4233 - val_loss: 23486738.0000 - val_mae: 3218.6780\n",
      "Epoch 486/500\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 12823873.0000 - mae: 2303.3416 - val_loss: 21563466.0000 - val_mae: 2899.7083\n",
      "Epoch 487/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12510278.0000 - mae: 2182.6206 - val_loss: 22946424.0000 - val_mae: 3112.9885\n",
      "Epoch 488/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12436284.0000 - mae: 2258.1819 - val_loss: 21514250.0000 - val_mae: 2866.1003\n",
      "Epoch 489/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12620843.0000 - mae: 2224.2903 - val_loss: 21266094.0000 - val_mae: 2801.1633\n",
      "Epoch 490/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12585525.0000 - mae: 2270.8289 - val_loss: 22038512.0000 - val_mae: 2855.2366\n",
      "Epoch 491/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12475746.0000 - mae: 2227.0239 - val_loss: 20761906.0000 - val_mae: 2712.7305\n",
      "Epoch 492/500\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 12196300.0000 - mae: 2173.4346 - val_loss: 21978400.0000 - val_mae: 2890.1379\n",
      "Epoch 493/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12385984.0000 - mae: 2202.6216 - val_loss: 20887818.0000 - val_mae: 2758.8298\n",
      "Epoch 494/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12408193.0000 - mae: 2160.7993 - val_loss: 22031332.0000 - val_mae: 2986.7314\n",
      "Epoch 495/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12210069.0000 - mae: 2205.7388 - val_loss: 21940348.0000 - val_mae: 2986.6641\n",
      "Epoch 496/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12364918.0000 - mae: 2238.2930 - val_loss: 22065034.0000 - val_mae: 2837.8611\n",
      "Epoch 497/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12148426.0000 - mae: 2157.3948 - val_loss: 22343052.0000 - val_mae: 3056.7786\n",
      "Epoch 498/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12140343.0000 - mae: 2198.7136 - val_loss: 22543036.0000 - val_mae: 2987.0208\n",
      "Epoch 499/500\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 12083308.0000 - mae: 2191.3894 - val_loss: 24306452.0000 - val_mae: 3327.5508\n",
      "Epoch 500/500\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 12563724.0000 - mae: 2276.3772 - val_loss: 21501228.0000 - val_mae: 2848.4741\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20697fadf90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Create a model\n",
    "model_2 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, input_shape = (11,), activation = \"relu\", name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "        tf.keras.layers.Dense(256, activation = \"relu\", name = \"hidden_layer_3\"),\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", name = \"hidden_layer_4\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = \"model_1\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_2.compile(\n",
    "    loss = tf.keras.losses.MSE,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model_2.fit(X_train_prepared, y_train, validation_data = (X_val_prepared, y_val), epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 11ms/step - loss: 34223200.0000 - mae: 3596.5256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[34223200.0, 3596.525634765625]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(X_test_prepared, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So looks like we've started overfitting! Maybe we need to reduce model complexity. Lets try stripping off a layer and reducing the number of neurons in the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "30/30 [==============================] - 1s 15ms/step - loss: 315476288.0000 - mae: 13067.7422 - val_loss: 331092128.0000 - val_mae: 13456.7412\n",
      "Epoch 2/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 315264224.0000 - mae: 13059.7920 - val_loss: 330594016.0000 - val_mae: 13438.6025\n",
      "Epoch 3/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 314102016.0000 - mae: 13015.9639 - val_loss: 328090400.0000 - val_mae: 13348.7393\n",
      "Epoch 4/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 309398912.0000 - mae: 12840.2686 - val_loss: 319452288.0000 - val_mae: 13037.9355\n",
      "Epoch 5/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 295732672.0000 - mae: 12324.1494 - val_loss: 297690304.0000 - val_mae: 12228.0410\n",
      "Epoch 6/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 266265296.0000 - mae: 11173.4795 - val_loss: 256402880.0000 - val_mae: 10631.1465\n",
      "Epoch 7/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 219015504.0000 - mae: 9379.1680 - val_loss: 200383312.0000 - val_mae: 8572.5029\n",
      "Epoch 8/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 167053104.0000 - mae: 7802.1528 - val_loss: 152894752.0000 - val_mae: 7680.6821\n",
      "Epoch 9/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 133954016.0000 - mae: 7887.6558 - val_loss: 128813928.0000 - val_mae: 8271.5938\n",
      "Epoch 10/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 118813336.0000 - mae: 8238.9463 - val_loss: 118773360.0000 - val_mae: 8384.8115\n",
      "Epoch 11/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 110525016.0000 - mae: 8225.2900 - val_loss: 110927016.0000 - val_mae: 8237.3662\n",
      "Epoch 12/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 103496128.0000 - mae: 8037.9653 - val_loss: 103835888.0000 - val_mae: 8008.6509\n",
      "Epoch 13/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 97016288.0000 - mae: 7814.7388 - val_loss: 97434800.0000 - val_mae: 7752.6353\n",
      "Epoch 14/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 90821368.0000 - mae: 7532.7124 - val_loss: 91290392.0000 - val_mae: 7445.6123\n",
      "Epoch 15/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 84963144.0000 - mae: 7280.4668 - val_loss: 84982688.0000 - val_mae: 7227.1245\n",
      "Epoch 16/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 79285168.0000 - mae: 7070.2124 - val_loss: 78953432.0000 - val_mae: 6907.6987\n",
      "Epoch 17/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 73540992.0000 - mae: 6693.0942 - val_loss: 73038704.0000 - val_mae: 6538.5186\n",
      "Epoch 18/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 68081376.0000 - mae: 6414.9937 - val_loss: 67231128.0000 - val_mae: 6250.0884\n",
      "Epoch 19/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 62847320.0000 - mae: 6127.0537 - val_loss: 61865252.0000 - val_mae: 6008.9941\n",
      "Epoch 20/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 57934576.0000 - mae: 5899.1958 - val_loss: 56837764.0000 - val_mae: 5744.0425\n",
      "Epoch 21/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 53335528.0000 - mae: 5657.8047 - val_loss: 52146620.0000 - val_mae: 5522.4907\n",
      "Epoch 22/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 49200968.0000 - mae: 5470.9790 - val_loss: 47893120.0000 - val_mae: 5348.3345\n",
      "Epoch 23/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 45595444.0000 - mae: 5195.7275 - val_loss: 44354688.0000 - val_mae: 5027.6509\n",
      "Epoch 24/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 42627576.0000 - mae: 4948.1245 - val_loss: 41176040.0000 - val_mae: 4863.6177\n",
      "Epoch 25/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 40116752.0000 - mae: 4794.8379 - val_loss: 39001760.0000 - val_mae: 4678.5635\n",
      "Epoch 26/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 38245236.0000 - mae: 4598.7700 - val_loss: 37199060.0000 - val_mae: 4502.4321\n",
      "Epoch 27/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 37094316.0000 - mae: 4411.7119 - val_loss: 35837340.0000 - val_mae: 4268.8452\n",
      "Epoch 28/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 35879424.0000 - mae: 4312.8174 - val_loss: 35137364.0000 - val_mae: 4268.9541\n",
      "Epoch 29/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 35310148.0000 - mae: 4219.6885 - val_loss: 34247728.0000 - val_mae: 4100.3452\n",
      "Epoch 30/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 34854516.0000 - mae: 4070.1616 - val_loss: 33692728.0000 - val_mae: 3997.6665\n",
      "Epoch 31/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 34651168.0000 - mae: 4076.0601 - val_loss: 33497356.0000 - val_mae: 3971.8638\n",
      "Epoch 32/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 34236128.0000 - mae: 3938.1187 - val_loss: 33092742.0000 - val_mae: 3861.0662\n",
      "Epoch 33/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 34054264.0000 - mae: 3891.5391 - val_loss: 32846482.0000 - val_mae: 3851.4900\n",
      "Epoch 34/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 33877372.0000 - mae: 3870.4834 - val_loss: 32602860.0000 - val_mae: 3828.2532\n",
      "Epoch 35/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 33775132.0000 - mae: 3930.8833 - val_loss: 32695246.0000 - val_mae: 3873.7791\n",
      "Epoch 36/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 33446658.0000 - mae: 3842.7004 - val_loss: 32080070.0000 - val_mae: 3754.0647\n",
      "Epoch 37/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 33384942.0000 - mae: 3787.2996 - val_loss: 31919216.0000 - val_mae: 3757.5024\n",
      "Epoch 38/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 33210766.0000 - mae: 3837.5278 - val_loss: 31804442.0000 - val_mae: 3768.9697\n",
      "Epoch 39/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 33021442.0000 - mae: 3790.1150 - val_loss: 31537602.0000 - val_mae: 3714.5149\n",
      "Epoch 40/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 32792804.0000 - mae: 3752.4961 - val_loss: 31345322.0000 - val_mae: 3697.0042\n",
      "Epoch 41/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 32649986.0000 - mae: 3712.6113 - val_loss: 31097612.0000 - val_mae: 3659.4978\n",
      "Epoch 42/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 32438416.0000 - mae: 3716.5706 - val_loss: 30960550.0000 - val_mae: 3689.6292\n",
      "Epoch 43/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 32231676.0000 - mae: 3715.5039 - val_loss: 30643264.0000 - val_mae: 3646.9341\n",
      "Epoch 44/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 32188164.0000 - mae: 3701.8213 - val_loss: 30440838.0000 - val_mae: 3619.3591\n",
      "Epoch 45/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 31981150.0000 - mae: 3642.3401 - val_loss: 30214714.0000 - val_mae: 3617.6687\n",
      "Epoch 46/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 31750616.0000 - mae: 3655.0605 - val_loss: 30044238.0000 - val_mae: 3625.5361\n",
      "Epoch 47/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 31670484.0000 - mae: 3697.0393 - val_loss: 29814604.0000 - val_mae: 3589.7324\n",
      "Epoch 48/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 31351566.0000 - mae: 3600.8108 - val_loss: 29408004.0000 - val_mae: 3529.7942\n",
      "Epoch 49/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 31180648.0000 - mae: 3574.5278 - val_loss: 29198896.0000 - val_mae: 3520.2249\n",
      "Epoch 50/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 30979084.0000 - mae: 3562.0142 - val_loss: 29099560.0000 - val_mae: 3557.3318\n",
      "Epoch 51/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 30792444.0000 - mae: 3568.1536 - val_loss: 28790422.0000 - val_mae: 3491.6731\n",
      "Epoch 52/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 30620252.0000 - mae: 3503.2183 - val_loss: 28469512.0000 - val_mae: 3472.2109\n",
      "Epoch 53/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 30442108.0000 - mae: 3578.3333 - val_loss: 28270480.0000 - val_mae: 3479.0935\n",
      "Epoch 54/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 30219706.0000 - mae: 3476.4868 - val_loss: 27962326.0000 - val_mae: 3421.3315\n",
      "Epoch 55/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 30149576.0000 - mae: 3519.8987 - val_loss: 27690670.0000 - val_mae: 3391.9685\n",
      "Epoch 56/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 29859666.0000 - mae: 3435.7676 - val_loss: 27367550.0000 - val_mae: 3364.1538\n",
      "Epoch 57/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 29608808.0000 - mae: 3432.2114 - val_loss: 27117432.0000 - val_mae: 3337.4824\n",
      "Epoch 58/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 29716886.0000 - mae: 3512.2080 - val_loss: 26988170.0000 - val_mae: 3336.0493\n",
      "Epoch 59/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 29290834.0000 - mae: 3331.6567 - val_loss: 26647984.0000 - val_mae: 3283.3381\n",
      "Epoch 60/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 29068148.0000 - mae: 3351.2700 - val_loss: 26464786.0000 - val_mae: 3298.0078\n",
      "Epoch 61/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 28845780.0000 - mae: 3366.9934 - val_loss: 26275782.0000 - val_mae: 3327.6455\n",
      "Epoch 62/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 28738832.0000 - mae: 3394.6172 - val_loss: 26098928.0000 - val_mae: 3322.0417\n",
      "Epoch 63/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 28516372.0000 - mae: 3369.1099 - val_loss: 25794818.0000 - val_mae: 3240.3994\n",
      "Epoch 64/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 28346422.0000 - mae: 3345.6907 - val_loss: 25574928.0000 - val_mae: 3220.4614\n",
      "Epoch 65/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 28151482.0000 - mae: 3265.4263 - val_loss: 25345070.0000 - val_mae: 3189.8213\n",
      "Epoch 66/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 27999052.0000 - mae: 3296.7317 - val_loss: 25201564.0000 - val_mae: 3218.7927\n",
      "Epoch 67/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 27810794.0000 - mae: 3260.6179 - val_loss: 24996784.0000 - val_mae: 3189.9714\n",
      "Epoch 68/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 27671132.0000 - mae: 3268.9387 - val_loss: 24842782.0000 - val_mae: 3200.5288\n",
      "Epoch 69/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 27576486.0000 - mae: 3252.6633 - val_loss: 24622966.0000 - val_mae: 3182.1953\n",
      "Epoch 70/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 27407644.0000 - mae: 3248.0227 - val_loss: 24395482.0000 - val_mae: 3156.4907\n",
      "Epoch 71/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 27193008.0000 - mae: 3212.5745 - val_loss: 24142042.0000 - val_mae: 3091.7012\n",
      "Epoch 72/200\n",
      "30/30 [==============================] - 0s 12ms/step - loss: 27016816.0000 - mae: 3222.3918 - val_loss: 24039084.0000 - val_mae: 3129.6946\n",
      "Epoch 73/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 26902384.0000 - mae: 3202.4114 - val_loss: 23792144.0000 - val_mae: 3050.7671\n",
      "Epoch 74/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 26778534.0000 - mae: 3162.6304 - val_loss: 23665856.0000 - val_mae: 3064.7061\n",
      "Epoch 75/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 26595556.0000 - mae: 3199.2236 - val_loss: 23487274.0000 - val_mae: 3060.7578\n",
      "Epoch 76/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 26495490.0000 - mae: 3157.1975 - val_loss: 23390958.0000 - val_mae: 3067.0002\n",
      "Epoch 77/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 26337050.0000 - mae: 3192.0642 - val_loss: 23250216.0000 - val_mae: 3076.5181\n",
      "Epoch 78/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 26249666.0000 - mae: 3207.7087 - val_loss: 22988132.0000 - val_mae: 3003.1689\n",
      "Epoch 79/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 26149414.0000 - mae: 3178.6177 - val_loss: 22917694.0000 - val_mae: 3046.3828\n",
      "Epoch 80/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 26038858.0000 - mae: 3135.3411 - val_loss: 22765532.0000 - val_mae: 2987.2539\n",
      "Epoch 81/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 25885170.0000 - mae: 3134.6501 - val_loss: 22581932.0000 - val_mae: 2979.2356\n",
      "Epoch 82/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25770732.0000 - mae: 3115.8237 - val_loss: 22453776.0000 - val_mae: 2957.0242\n",
      "Epoch 83/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25683398.0000 - mae: 3122.3174 - val_loss: 22303430.0000 - val_mae: 2927.9565\n",
      "Epoch 84/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 25545788.0000 - mae: 3050.2510 - val_loss: 22217814.0000 - val_mae: 2954.7458\n",
      "Epoch 85/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25455350.0000 - mae: 3108.9805 - val_loss: 22075698.0000 - val_mae: 2966.8191\n",
      "Epoch 86/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25441592.0000 - mae: 3085.3394 - val_loss: 21920834.0000 - val_mae: 2925.1279\n",
      "Epoch 87/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25232082.0000 - mae: 3086.9189 - val_loss: 21871140.0000 - val_mae: 2949.0166\n",
      "Epoch 88/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25167500.0000 - mae: 3108.3994 - val_loss: 21736000.0000 - val_mae: 2919.6213\n",
      "Epoch 89/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 25059916.0000 - mae: 2997.1392 - val_loss: 21544640.0000 - val_mae: 2860.8435\n",
      "Epoch 90/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24970682.0000 - mae: 3073.1118 - val_loss: 21488716.0000 - val_mae: 2898.8206\n",
      "Epoch 91/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24899460.0000 - mae: 3094.4341 - val_loss: 21360468.0000 - val_mae: 2885.8149\n",
      "Epoch 92/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 24784118.0000 - mae: 3051.8877 - val_loss: 21319702.0000 - val_mae: 2908.9407\n",
      "Epoch 93/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 24828864.0000 - mae: 3141.9192 - val_loss: 21184226.0000 - val_mae: 2885.1963\n",
      "Epoch 94/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 24656550.0000 - mae: 2961.1689 - val_loss: 21036180.0000 - val_mae: 2829.7793\n",
      "Epoch 95/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24568510.0000 - mae: 3050.7002 - val_loss: 21087940.0000 - val_mae: 2915.5803\n",
      "Epoch 96/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24507268.0000 - mae: 3027.7798 - val_loss: 20841798.0000 - val_mae: 2791.2874\n",
      "Epoch 97/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24441000.0000 - mae: 2943.6118 - val_loss: 20703588.0000 - val_mae: 2797.9597\n",
      "Epoch 98/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 24343476.0000 - mae: 2977.8367 - val_loss: 20820618.0000 - val_mae: 2889.8374\n",
      "Epoch 99/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 24354666.0000 - mae: 3114.6150 - val_loss: 20676460.0000 - val_mae: 2866.2725\n",
      "Epoch 100/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 24278822.0000 - mae: 3067.9172 - val_loss: 20572734.0000 - val_mae: 2824.3120\n",
      "Epoch 101/200\n",
      "30/30 [==============================] - 0s 11ms/step - loss: 24193492.0000 - mae: 2969.3862 - val_loss: 20585888.0000 - val_mae: 2885.2878\n",
      "Epoch 102/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 24129758.0000 - mae: 3004.8103 - val_loss: 20411584.0000 - val_mae: 2821.8452\n",
      "Epoch 103/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 24076326.0000 - mae: 3072.7566 - val_loss: 20459730.0000 - val_mae: 2908.0916\n",
      "Epoch 104/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 24016270.0000 - mae: 2989.9163 - val_loss: 20274926.0000 - val_mae: 2770.1865\n",
      "Epoch 105/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23939848.0000 - mae: 3006.1877 - val_loss: 20250516.0000 - val_mae: 2834.3013\n",
      "Epoch 106/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23888690.0000 - mae: 2983.1545 - val_loss: 20172064.0000 - val_mae: 2806.2498\n",
      "Epoch 107/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23863906.0000 - mae: 3012.3271 - val_loss: 20001108.0000 - val_mae: 2750.3113\n",
      "Epoch 108/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 23822586.0000 - mae: 2872.2297 - val_loss: 19952784.0000 - val_mae: 2779.1699\n",
      "Epoch 109/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 23925108.0000 - mae: 3045.6299 - val_loss: 19988240.0000 - val_mae: 2807.3909\n",
      "Epoch 110/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 23688840.0000 - mae: 2959.5220 - val_loss: 19984326.0000 - val_mae: 2843.4246\n",
      "Epoch 111/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23762020.0000 - mae: 2999.8442 - val_loss: 19845164.0000 - val_mae: 2763.6436\n",
      "Epoch 112/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23633624.0000 - mae: 2998.4089 - val_loss: 19795106.0000 - val_mae: 2797.3186\n",
      "Epoch 113/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23562002.0000 - mae: 2905.7576 - val_loss: 19642130.0000 - val_mae: 2739.8850\n",
      "Epoch 114/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23522282.0000 - mae: 2946.2607 - val_loss: 19652948.0000 - val_mae: 2785.1821\n",
      "Epoch 115/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23509132.0000 - mae: 2922.5859 - val_loss: 19617082.0000 - val_mae: 2765.3696\n",
      "Epoch 116/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23468598.0000 - mae: 2984.7188 - val_loss: 19548214.0000 - val_mae: 2753.0374\n",
      "Epoch 117/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 23450186.0000 - mae: 2865.3928 - val_loss: 19481548.0000 - val_mae: 2726.7017\n",
      "Epoch 118/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23390038.0000 - mae: 2971.0637 - val_loss: 19514354.0000 - val_mae: 2778.0168\n",
      "Epoch 119/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23316216.0000 - mae: 2960.1802 - val_loss: 19383988.0000 - val_mae: 2714.7358\n",
      "Epoch 120/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23290710.0000 - mae: 2893.2527 - val_loss: 19350284.0000 - val_mae: 2706.4497\n",
      "Epoch 121/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23259732.0000 - mae: 2883.6924 - val_loss: 19445960.0000 - val_mae: 2791.4661\n",
      "Epoch 122/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23256422.0000 - mae: 2896.3965 - val_loss: 19282796.0000 - val_mae: 2717.0925\n",
      "Epoch 123/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23198846.0000 - mae: 2957.6680 - val_loss: 19220124.0000 - val_mae: 2700.3892\n",
      "Epoch 124/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23126600.0000 - mae: 2930.7183 - val_loss: 19268860.0000 - val_mae: 2743.5325\n",
      "Epoch 125/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23123748.0000 - mae: 2919.6782 - val_loss: 19138984.0000 - val_mae: 2681.9465\n",
      "Epoch 126/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 23170702.0000 - mae: 2961.6707 - val_loss: 19145868.0000 - val_mae: 2723.5420\n",
      "Epoch 127/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 23111970.0000 - mae: 2862.1084 - val_loss: 19136236.0000 - val_mae: 2706.2878\n",
      "Epoch 128/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 23057330.0000 - mae: 2949.8958 - val_loss: 19109402.0000 - val_mae: 2718.7402\n",
      "Epoch 129/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22986708.0000 - mae: 2945.3264 - val_loss: 19003726.0000 - val_mae: 2660.7639\n",
      "Epoch 130/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22975484.0000 - mae: 2855.5198 - val_loss: 19100814.0000 - val_mae: 2769.2424\n",
      "Epoch 131/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22999704.0000 - mae: 3046.0432 - val_loss: 18963646.0000 - val_mae: 2687.0872\n",
      "Epoch 132/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22918464.0000 - mae: 2799.4822 - val_loss: 18937896.0000 - val_mae: 2674.1406\n",
      "Epoch 133/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22892076.0000 - mae: 2912.1924 - val_loss: 18907278.0000 - val_mae: 2669.7021\n",
      "Epoch 134/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22844038.0000 - mae: 2944.7234 - val_loss: 18921328.0000 - val_mae: 2707.9065\n",
      "Epoch 135/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22917614.0000 - mae: 2867.7466 - val_loss: 18999226.0000 - val_mae: 2751.1914\n",
      "Epoch 136/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22822732.0000 - mae: 2894.9385 - val_loss: 18889606.0000 - val_mae: 2716.8357\n",
      "Epoch 137/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22740304.0000 - mae: 2932.9873 - val_loss: 18851426.0000 - val_mae: 2700.2229\n",
      "Epoch 138/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22712884.0000 - mae: 2894.5967 - val_loss: 18865494.0000 - val_mae: 2717.2891\n",
      "Epoch 139/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22707010.0000 - mae: 2885.9675 - val_loss: 18855688.0000 - val_mae: 2701.9775\n",
      "Epoch 140/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22755962.0000 - mae: 2797.8999 - val_loss: 18758212.0000 - val_mae: 2636.8940\n",
      "Epoch 141/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22734476.0000 - mae: 2983.8333 - val_loss: 18800356.0000 - val_mae: 2670.7080\n",
      "Epoch 142/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22741746.0000 - mae: 2851.8618 - val_loss: 18827814.0000 - val_mae: 2727.2708\n",
      "Epoch 143/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22608746.0000 - mae: 2864.4231 - val_loss: 18754968.0000 - val_mae: 2635.4573\n",
      "Epoch 144/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22558588.0000 - mae: 2848.9656 - val_loss: 18739232.0000 - val_mae: 2698.7505\n",
      "Epoch 145/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22616692.0000 - mae: 2884.5547 - val_loss: 18717798.0000 - val_mae: 2698.3391\n",
      "Epoch 146/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22648722.0000 - mae: 2812.5337 - val_loss: 18654608.0000 - val_mae: 2675.2463\n",
      "Epoch 147/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22519086.0000 - mae: 2838.1567 - val_loss: 18636570.0000 - val_mae: 2650.4517\n",
      "Epoch 148/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22481256.0000 - mae: 2890.3066 - val_loss: 18626548.0000 - val_mae: 2665.2842\n",
      "Epoch 149/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22476620.0000 - mae: 2779.0918 - val_loss: 18522884.0000 - val_mae: 2630.1985\n",
      "Epoch 150/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22426796.0000 - mae: 2902.8389 - val_loss: 18533698.0000 - val_mae: 2654.6221\n",
      "Epoch 151/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 22400670.0000 - mae: 2846.4229 - val_loss: 18519766.0000 - val_mae: 2666.7783\n",
      "Epoch 152/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22395074.0000 - mae: 2901.0645 - val_loss: 18517694.0000 - val_mae: 2650.5344\n",
      "Epoch 153/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22334704.0000 - mae: 2812.5942 - val_loss: 18515852.0000 - val_mae: 2637.5071\n",
      "Epoch 154/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22362540.0000 - mae: 2876.0686 - val_loss: 18472704.0000 - val_mae: 2607.9558\n",
      "Epoch 155/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22325502.0000 - mae: 2806.5808 - val_loss: 18416988.0000 - val_mae: 2583.9727\n",
      "Epoch 156/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22289650.0000 - mae: 2856.0957 - val_loss: 18483410.0000 - val_mae: 2689.1609\n",
      "Epoch 157/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22281858.0000 - mae: 2876.2815 - val_loss: 18470402.0000 - val_mae: 2655.9968\n",
      "Epoch 158/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22236804.0000 - mae: 2849.7542 - val_loss: 18476330.0000 - val_mae: 2654.7839\n",
      "Epoch 159/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22285714.0000 - mae: 2771.2766 - val_loss: 18443840.0000 - val_mae: 2646.1636\n",
      "Epoch 160/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22242322.0000 - mae: 2859.4058 - val_loss: 18408540.0000 - val_mae: 2612.2100\n",
      "Epoch 161/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22197498.0000 - mae: 2852.7432 - val_loss: 18359088.0000 - val_mae: 2597.0579\n",
      "Epoch 162/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22171862.0000 - mae: 2828.8901 - val_loss: 18415170.0000 - val_mae: 2622.2505\n",
      "Epoch 163/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22154032.0000 - mae: 2812.4565 - val_loss: 18320508.0000 - val_mae: 2606.8013\n",
      "Epoch 164/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22105408.0000 - mae: 2824.9231 - val_loss: 18403294.0000 - val_mae: 2633.6094\n",
      "Epoch 165/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 22129210.0000 - mae: 2801.5989 - val_loss: 18319992.0000 - val_mae: 2601.0999\n",
      "Epoch 166/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22127466.0000 - mae: 2796.1868 - val_loss: 18352476.0000 - val_mae: 2637.4473\n",
      "Epoch 167/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22105062.0000 - mae: 2829.6519 - val_loss: 18384248.0000 - val_mae: 2656.1267\n",
      "Epoch 168/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22092318.0000 - mae: 2927.2964 - val_loss: 18287650.0000 - val_mae: 2534.3928\n",
      "Epoch 169/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22093300.0000 - mae: 2747.3901 - val_loss: 18306316.0000 - val_mae: 2609.9119\n",
      "Epoch 170/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22011146.0000 - mae: 2776.7981 - val_loss: 18225504.0000 - val_mae: 2525.1968\n",
      "Epoch 171/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 22039530.0000 - mae: 2855.3010 - val_loss: 18202602.0000 - val_mae: 2587.3711\n",
      "Epoch 172/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21961914.0000 - mae: 2747.8296 - val_loss: 18280202.0000 - val_mae: 2613.6609\n",
      "Epoch 173/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21957700.0000 - mae: 2916.0884 - val_loss: 18250468.0000 - val_mae: 2577.8984\n",
      "Epoch 174/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 22020192.0000 - mae: 2841.6833 - val_loss: 18226344.0000 - val_mae: 2586.8552\n",
      "Epoch 175/200\n",
      "30/30 [==============================] - 0s 9ms/step - loss: 21913808.0000 - mae: 2766.8945 - val_loss: 18172980.0000 - val_mae: 2533.8569\n",
      "Epoch 176/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21950952.0000 - mae: 2769.1404 - val_loss: 18208520.0000 - val_mae: 2566.1902\n",
      "Epoch 177/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21875200.0000 - mae: 2817.7358 - val_loss: 18178398.0000 - val_mae: 2573.5049\n",
      "Epoch 178/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21822336.0000 - mae: 2765.3374 - val_loss: 18220324.0000 - val_mae: 2568.9788\n",
      "Epoch 179/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21849890.0000 - mae: 2725.6047 - val_loss: 18217332.0000 - val_mae: 2607.4668\n",
      "Epoch 180/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21872596.0000 - mae: 2864.1348 - val_loss: 18307422.0000 - val_mae: 2668.6494\n",
      "Epoch 181/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 21775776.0000 - mae: 2727.3547 - val_loss: 18124782.0000 - val_mae: 2514.7925\n",
      "Epoch 182/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21795088.0000 - mae: 2841.1755 - val_loss: 18145280.0000 - val_mae: 2580.4763\n",
      "Epoch 183/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21714920.0000 - mae: 2829.0076 - val_loss: 18222762.0000 - val_mae: 2628.4026\n",
      "Epoch 184/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21787950.0000 - mae: 2710.5525 - val_loss: 18149746.0000 - val_mae: 2601.2493\n",
      "Epoch 185/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21744578.0000 - mae: 2763.7405 - val_loss: 18296262.0000 - val_mae: 2669.7566\n",
      "Epoch 186/200\n",
      "30/30 [==============================] - 0s 10ms/step - loss: 21759672.0000 - mae: 2795.3755 - val_loss: 18080144.0000 - val_mae: 2507.6890\n",
      "Epoch 187/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 21632920.0000 - mae: 2765.1829 - val_loss: 18306564.0000 - val_mae: 2664.7544\n",
      "Epoch 188/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21651348.0000 - mae: 2791.4910 - val_loss: 18143522.0000 - val_mae: 2564.0752\n",
      "Epoch 189/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21645768.0000 - mae: 2768.2815 - val_loss: 18084474.0000 - val_mae: 2518.7910\n",
      "Epoch 190/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21599132.0000 - mae: 2702.4373 - val_loss: 18103662.0000 - val_mae: 2582.7444\n",
      "Epoch 191/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21659132.0000 - mae: 2843.4600 - val_loss: 17994866.0000 - val_mae: 2436.7051\n",
      "Epoch 192/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21623906.0000 - mae: 2709.7288 - val_loss: 18110698.0000 - val_mae: 2592.8833\n",
      "Epoch 193/200\n",
      "30/30 [==============================] - 0s 6ms/step - loss: 21700554.0000 - mae: 2883.3162 - val_loss: 18064676.0000 - val_mae: 2469.7632\n",
      "Epoch 194/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21571636.0000 - mae: 2686.0137 - val_loss: 18065164.0000 - val_mae: 2523.1536\n",
      "Epoch 195/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21542930.0000 - mae: 2789.5530 - val_loss: 17985506.0000 - val_mae: 2468.7947\n",
      "Epoch 196/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21572224.0000 - mae: 2662.3303 - val_loss: 17962762.0000 - val_mae: 2543.7075\n",
      "Epoch 197/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21495230.0000 - mae: 2799.6721 - val_loss: 18039432.0000 - val_mae: 2554.8291\n",
      "Epoch 198/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21519498.0000 - mae: 2673.2996 - val_loss: 18111406.0000 - val_mae: 2611.1589\n",
      "Epoch 199/200\n",
      "30/30 [==============================] - 0s 7ms/step - loss: 21501244.0000 - mae: 2838.2493 - val_loss: 18118126.0000 - val_mae: 2524.5210\n",
      "Epoch 200/200\n",
      "30/30 [==============================] - 0s 8ms/step - loss: 21478008.0000 - mae: 2686.3259 - val_loss: 18023172.0000 - val_mae: 2540.1589\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a model\n",
    "model_3 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, input_shape = (11,), activation = \"relu\", name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(64, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", name = \"hidden_layer_3\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = \"model_3\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_3.compile(\n",
    "    loss = tf.keras.losses.MSE,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "history = model_3.fit(X_train_prepared, y_train, validation_data = (X_val_prepared, y_val), epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 7ms/step - loss: 26560570.0000 - mae: 3130.6587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[26560570.0, 3130.65869140625]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(X_test_prepared, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yupp this looks much better. The MAE over test data looks similar to train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.ylabel = \"Loss\"\n",
    "plt.xlabel = \"Epochs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** How long should you train for?\n",
    "\n",
    "It depends. Really... it depends on the problem you're working on. However, many people have asked this question before... so TensorFlow has a solution! Its called the [EarlyStopping Callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) which is a TensorFlow component you can add to your model to stop training once its loss stops improving a certain metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data (normalization and standardization)\n",
    "\n",
    "In terms of scaling values, neural networks tend to prefer normalization.\n",
    "\n",
    "If you're not sure, try both\n",
    "\n",
    "|Scaling type|What it does|Scikit-learn Function|When to use|\n",
    "|------------|------------|---------------------|-----------|\n",
    "|Scale (also referred to as normalization)|Converts all values to between 0 and 1 whilst preserving the original distribution|MinMaxScaler|Use as default scaler with neural networks|\n",
    "|Standardization|Removes the mean and divides each value by the standard deviation|StandardScaler|Transform a feature to have close to normal distribution (caution: reduces effect of outliers)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "num_pipeline = Pipeline(\n",
    "    [\n",
    "        ('std_scaling_layer', MinMaxScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "cat_pipeline = Pipeline(\n",
    "    [\n",
    "        ('one_hot_encoding', OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        ('num_transform_layer', num_pipeline, numerical_columns),\n",
    "        ('cat_transform_layer', cat_pipeline, categorical_columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_train_normal = full_pipeline.fit_transform(X_train)\n",
    "X_test_normal = full_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! Our data has been normalized and one hot encoded. Now lets build a neural network model on it and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 1s 12ms/step - loss: 322407808.0000 - mae: 13344.4971\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 322007840.0000 - mae: 13329.8242\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 319568992.0000 - mae: 13241.5254\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 310328000.0000 - mae: 12895.2773\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 286156928.0000 - mae: 11960.0176\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 7ms/step - loss: 241570240.0000 - mae: 10272.9268\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 186038064.0000 - mae: 8507.1807\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 144599296.0000 - mae: 8006.8550\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 129114648.0000 - mae: 8450.7891\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 121422360.0000 - mae: 8438.4580\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 115201736.0000 - mae: 8226.6543\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 109323120.0000 - mae: 8077.8643\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 103848544.0000 - mae: 7913.7026\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 98188176.0000 - mae: 7648.6523\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 92621688.0000 - mae: 7433.4595\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 87103824.0000 - mae: 7279.0039\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 81408536.0000 - mae: 6910.0059\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 75628984.0000 - mae: 6851.4302\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 69831032.0000 - mae: 6548.5718\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 64463284.0000 - mae: 6320.6621\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 59144672.0000 - mae: 5959.6577\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 54405452.0000 - mae: 5829.5430\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 50316940.0000 - mae: 5523.8281\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 47002488.0000 - mae: 5240.1655\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 44631132.0000 - mae: 5206.4473\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 42622488.0000 - mae: 4868.4507\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 41439512.0000 - mae: 4691.1919\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 40494212.0000 - mae: 4579.6484\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 40009044.0000 - mae: 4542.5298\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 39472124.0000 - mae: 4381.0088\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 39195208.0000 - mae: 4295.7734\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 39000436.0000 - mae: 4297.7827\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 38758356.0000 - mae: 4237.5972\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 38628472.0000 - mae: 4177.1982\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 38507524.0000 - mae: 4104.2534\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 38650704.0000 - mae: 4250.5078\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 38355072.0000 - mae: 4031.3123\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 38115920.0000 - mae: 4132.6514\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 38136228.0000 - mae: 4073.8723\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 38020240.0000 - mae: 4070.6565\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 37865248.0000 - mae: 4110.0864\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37845064.0000 - mae: 4064.6477\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37719096.0000 - mae: 4094.6677\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37643780.0000 - mae: 4018.3201\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 37658372.0000 - mae: 4039.6599\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37504520.0000 - mae: 3999.2197\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37416168.0000 - mae: 4035.3369\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37370088.0000 - mae: 4038.9319\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37418180.0000 - mae: 4066.6599\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 37295888.0000 - mae: 4024.7812\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 37269288.0000 - mae: 4079.3457\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37172796.0000 - mae: 3942.3235\n",
      "Epoch 53/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37063456.0000 - mae: 4024.9331\n",
      "Epoch 54/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 37059824.0000 - mae: 4032.6033\n",
      "Epoch 55/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 36959448.0000 - mae: 3984.4912\n",
      "Epoch 56/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 36944640.0000 - mae: 3981.3323\n",
      "Epoch 57/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 36930588.0000 - mae: 4048.1033\n",
      "Epoch 58/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 36894212.0000 - mae: 3985.9836\n",
      "Epoch 59/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36756920.0000 - mae: 4008.3450\n",
      "Epoch 60/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36689836.0000 - mae: 3960.9365\n",
      "Epoch 61/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36665812.0000 - mae: 3996.7141\n",
      "Epoch 62/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36700860.0000 - mae: 3950.0864\n",
      "Epoch 63/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 36671808.0000 - mae: 4137.9565\n",
      "Epoch 64/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36471756.0000 - mae: 3964.0854\n",
      "Epoch 65/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36463280.0000 - mae: 3987.8672\n",
      "Epoch 66/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36367488.0000 - mae: 3983.0859\n",
      "Epoch 67/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36416268.0000 - mae: 3914.7063\n",
      "Epoch 68/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 36251476.0000 - mae: 4009.5566\n",
      "Epoch 69/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 36164532.0000 - mae: 3987.6929\n",
      "Epoch 70/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 36299956.0000 - mae: 3980.5471\n",
      "Epoch 71/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 36072880.0000 - mae: 3931.7864\n",
      "Epoch 72/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 35986888.0000 - mae: 3942.6626\n",
      "Epoch 73/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 35935152.0000 - mae: 3986.9556\n",
      "Epoch 74/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 35831752.0000 - mae: 3944.9990\n",
      "Epoch 75/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 35732408.0000 - mae: 3965.5317\n",
      "Epoch 76/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 35740852.0000 - mae: 3942.1028\n",
      "Epoch 77/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 35662848.0000 - mae: 3997.3994\n",
      "Epoch 78/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 35683084.0000 - mae: 3938.8074\n",
      "Epoch 79/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 35585124.0000 - mae: 3931.8672\n",
      "Epoch 80/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 35421440.0000 - mae: 3939.2275\n",
      "Epoch 81/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 35354812.0000 - mae: 3968.3401\n",
      "Epoch 82/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 35393828.0000 - mae: 3899.2673\n",
      "Epoch 83/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 35242384.0000 - mae: 3931.5837\n",
      "Epoch 84/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 35146228.0000 - mae: 3925.2373\n",
      "Epoch 85/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 35104368.0000 - mae: 3903.3428\n",
      "Epoch 86/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 35081260.0000 - mae: 3947.2439\n",
      "Epoch 87/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 34916268.0000 - mae: 3898.2024\n",
      "Epoch 88/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 34866700.0000 - mae: 3892.2739\n",
      "Epoch 89/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 34947900.0000 - mae: 3938.7976\n",
      "Epoch 90/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 34778568.0000 - mae: 3885.5637\n",
      "Epoch 91/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 34629136.0000 - mae: 3880.6265\n",
      "Epoch 92/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 34590092.0000 - mae: 3922.8318\n",
      "Epoch 93/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 34508836.0000 - mae: 3826.3264\n",
      "Epoch 94/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 34375324.0000 - mae: 3916.0647\n",
      "Epoch 95/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 34284808.0000 - mae: 3863.3662\n",
      "Epoch 96/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 34330860.0000 - mae: 3886.8650\n",
      "Epoch 97/100\n",
      "34/34 [==============================] - 0s 9ms/step - loss: 34212156.0000 - mae: 3852.5952\n",
      "Epoch 98/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 34005660.0000 - mae: 3830.2510\n",
      "Epoch 99/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 33998844.0000 - mae: 3865.1846\n",
      "Epoch 100/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 33911304.0000 - mae: 3845.7439\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create a model\n",
    "model_3 = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(64, input_shape = (11,), activation = \"relu\", name = \"hidden_layer_1\"),\n",
    "        tf.keras.layers.Dense(64, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "        tf.keras.layers.Dense(32, activation = \"relu\", name = \"hidden_layer_3\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = \"model_3\"\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model_3.compile(\n",
    "    loss = tf.keras.losses.MSE,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "history = model_3.fit(X_train_normal, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 11ms/step - loss: 31169486.0000 - mae: 3855.4866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[31169486.0, 3855.486572265625]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.evaluate(X_test_normal, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful! We've reduced overfitting by a lot! Although we've sacrificed some accuracy in the process. However, we could in theory get more data and expect better results from this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98ba7f9ca0a7d3f9faf83a09faac2df1e0ca0e1c9a5db868ca666908d95c6454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
