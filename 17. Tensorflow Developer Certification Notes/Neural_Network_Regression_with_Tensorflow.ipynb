{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jun 20 09:41:37 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.96       Driver Version: 512.96       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   55C    P8     5W /  N/A |   4446MiB /  6144MiB |      7%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1928    C+G   ...e\\Current\\LogiOverlay.exe    N/A      |\n",
      "|    0   N/A  N/A      6732    C+G   ...erver\\YourPhoneServer.exe    N/A      |\n",
      "|    0   N/A  N/A     15316    C+G   ...mmandCenterBackground.exe    N/A      |\n",
      "|    0   N/A  N/A     20752      C   ...thon\\Python310\\python.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll cover:\n",
    "* Architecture of a neural network regression model\n",
    "* Input shapes and output shapes a regression model (features and labels)\n",
    "* Creating custom data to view and fit\n",
    "* Steps in modelling\n",
    "* Creating a model, compiling a model, fitting a model, evaluating a model\n",
    "* Different evaluation methods\n",
    "* Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression inputs and outputs\n",
    "\n",
    "`Inputs` -> `Machine Learning Model` -> `Outputs`\n",
    "\n",
    "We're essentially trying to figure out the relationship between inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of Neural Networks\n",
    "\n",
    "`Input` -> `Input Layer` -> `Hidden Layer(s)` -> `Output Layer`-> `Output`\n",
    "\n",
    "|Hyperparameter|Typical Value|\n",
    "|--------------|-------------|\n",
    "|Input Layer Shape|Same shape as number of features (eg 3 for # bedrooms, # car spaces in housing price prediction)|\n",
    "|Hidden Layer(s)|Problem specific, minimum = 1, maximum = $\\infty$|\n",
    "|Neurons per hidden layer|Problem specific; generally 10 to 100|\n",
    "|Output layer shape|Same shape as desired prediction shape (eg 1 for house price)|\n",
    "|Hidden activation|Using ReLU (rectified linear unit)|\n",
    "|Output activation|None, ReLU, logistic/tanh|\n",
    "|Loss function|MSE(mean square error) or MAE (mean absolute error)/Huber (combination of MAE/MSE) if outliers|\n",
    "|Optimizer|SGD (stochastic gradient descent), Adam|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model specimen\n",
    "model = tf.keras.Sequential(<br>\n",
    "    [<br>\n",
    "        tf.keras.Input(shape = (3,)),<br>\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),<br>\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),<br>\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),<br>\n",
    "        tf.keras.layers.Dense(1, activation = None)<br>\n",
    "    ]<br>\n",
    ")<br>\n",
    "#### Compiling the model\n",
    "model.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(lr = 1e-3), metrics = [\"mae\"])\n",
    "#### Fit the model\n",
    "model.fit(X_train, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Regression Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the features\n",
    "X = np.array([-7.0, -4.0, -1.0, 2.0, 5.0, 8.0, 11.0, 14.0])\n",
    "\n",
    "# Create the labels\n",
    "y = np.array([3.0, 6.0, 9.0, 12.0, 15.0, 18.0, 21.0, 24.0])\n",
    "\n",
    "# Visualise it\n",
    "plt.scatter(X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Output Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'bedroom', b'bathroom', b'garage'], dtype=object)>,\n",
       " <tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700])>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a demo tensor for our housing price prediction problem\n",
    "\n",
    "house_info = tf.constant([\"bedroom\",\"bathroom\", \"garage\"])\n",
    "house_price = tf.constant([939700])\n",
    "house_info, house_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([8, 1]), TensorShape([8]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "X = X[..., tf.newaxis]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps in Modelling with Tensorflow\n",
    "\n",
    "1. **Creating a model** - define the input and output layers, as well as the hidden layers of a deep learning model.\n",
    "2. **Compiling a model** - define the loss function (in other words, the function which tells our model how wrong it is) and the optimizer (tells our model how to improve the patterns its learning) and evaluation metrics (what we can use to interpret the performance of our model)\n",
    "3. **Fitting a model** - letting the model try to find patterns between X & y (features and labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 11.5048 - mae: 11.5048\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.3723 - mae: 11.3723\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.2398 - mae: 11.2398\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 11.1073 - mae: 11.1073\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.9748 - mae: 10.9748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2486e422230>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create a model using the Sequential API\n",
    "model  = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae, # Mean absolute error\n",
    "    optimizer = tf.keras.optimizers.SGD(), # Stochastic Gradient Descent\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 5)\n",
    "\n",
    "# Alternatively we can also do\n",
    "# model = tf.keras.Sequential()\n",
    "# model.add(tf.keras.layers.Dense(1, shape = (1,)))\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
       " array([[-7.],\n",
       "        [-4.],\n",
       "        [-1.],\n",
       "        [ 2.],\n",
       "        [ 5.],\n",
       "        [ 8.],\n",
       "        [11.],\n",
       "        [14.]])>,\n",
       " <tf.Tensor: shape=(8,), dtype=float64, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.])>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out X and y\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000249737B8820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "# Try and make a prediction using our model\n",
    "y_pred = model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.71602]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred + 11 # Add the mean absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still off! The model should have predicted 27 but we can always improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our Model\n",
    "\n",
    "We can improve the model, by altering the steps we took to create a model\n",
    "\n",
    "1. **Creating a model** - here we might add more layers, increase the number of hidden units (all called neurons) within each of the hidden layers, change the activation function of each layer.\n",
    "2. **Compiling a model** - here we might change the loss function, optimization function or perhaps the **learning rate** of the optimization function\n",
    "3. **Fitting a model** - here we might fit a model for more **epochs** (leave it training for longer) or on more data (give the model more examples to learn from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 11.2219 - mae: 11.2219\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 11.0894 - mae: 11.0894\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.9569 - mae: 10.9569\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 10.8244 - mae: 10.8244\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 10.6919 - mae: 10.6919\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.5594 - mae: 10.5594\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.4269 - mae: 10.4269\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.2944 - mae: 10.2944\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 10.1619 - mae: 10.1619\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.0294 - mae: 10.0294\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.8969 - mae: 9.8969\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.7644 - mae: 9.7644\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.6319 - mae: 9.6319\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.4994 - mae: 9.4994\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.3669 - mae: 9.3669\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.2344 - mae: 9.2344\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.1019 - mae: 9.1019\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.9694 - mae: 8.9694\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.8369 - mae: 8.8369\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.7044 - mae: 8.7044\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.5719 - mae: 8.5719\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.4394 - mae: 8.4394\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.3069 - mae: 8.3069\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1744 - mae: 8.1744\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.0419 - mae: 8.0419\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.9094 - mae: 7.9094\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 7.7769 - mae: 7.7769\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.6444 - mae: 7.6444\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.5119 - mae: 7.5119\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.3794 - mae: 7.3794\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.2750 - mae: 7.2750\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2694 - mae: 7.2694\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2638 - mae: 7.2638\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2581 - mae: 7.2581\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.2525 - mae: 7.2525\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2469 - mae: 7.2469\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2413 - mae: 7.2413\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 7.2356 - mae: 7.2356\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.2300 - mae: 7.2300\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.2244 - mae: 7.2244\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.2188 - mae: 7.2188\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2131 - mae: 7.2131\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2075 - mae: 7.2075\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.2019 - mae: 7.2019\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1962 - mae: 7.1962\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1906 - mae: 7.1906\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.1850 - mae: 7.1850\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1794 - mae: 7.1794\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1737 - mae: 7.1737\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1681 - mae: 7.1681\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1625 - mae: 7.1625\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1569 - mae: 7.1569\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.1512 - mae: 7.1512\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1456 - mae: 7.1456\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1400 - mae: 7.1400\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 7.1344 - mae: 7.1344\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1287 - mae: 7.1287\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1231 - mae: 7.1231\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.1175 - mae: 7.1175\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1119 - mae: 7.1119\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1062 - mae: 7.1062\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.1006 - mae: 7.1006\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.0950 - mae: 7.0950\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0894 - mae: 7.0894\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0838 - mae: 7.0838\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0781 - mae: 7.0781\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0725 - mae: 7.0725\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0669 - mae: 7.0669\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0613 - mae: 7.0613\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0556 - mae: 7.0556\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0500 - mae: 7.0500\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0444 - mae: 7.0444\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0388 - mae: 7.0388\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.0331 - mae: 7.0331\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0275 - mae: 7.0275\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.0219 - mae: 7.0219\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0163 - mae: 7.0163\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0106 - mae: 7.0106\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0050 - mae: 7.0050\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9994 - mae: 6.9994\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9938 - mae: 6.9938\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9881 - mae: 6.9881\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9825 - mae: 6.9825\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9769 - mae: 6.9769\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9713 - mae: 6.9713\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.9656 - mae: 6.9656\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9600 - mae: 6.9600\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 6.9544 - mae: 6.9544\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9488 - mae: 6.9488\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.9431 - mae: 6.9431\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9375 - mae: 6.9375\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9319 - mae: 6.9319\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9262 - mae: 6.9262\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9206 - mae: 6.9206\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9150 - mae: 6.9150\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.9094 - mae: 6.9094\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9038 - mae: 6.9038\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.8981 - mae: 6.8981\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8925 - mae: 6.8925\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8869 - mae: 6.8869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24958f5be80>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Increase the epochs\n",
    "# 1. Create a model using the Sequential API\n",
    "model  = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae, # Mean absolute error\n",
    "    optimizer = tf.keras.optimizers.SGD(), # Stochastic Gradient Descent\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
       " array([[-7.],\n",
       "        [-4.],\n",
       "        [-1.],\n",
       "        [ 2.],\n",
       "        [ 5.],\n",
       "        [ 8.],\n",
       "        [11.],\n",
       "        [14.]])>,\n",
       " <tf.Tensor: shape=(8,), dtype=float64, numpy=array([ 3.,  6.,  9., 12., 15., 18., 21., 24.])>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remind ourselves of the data\n",
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[29.739855]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see if our model's prediction has improved\n",
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yes! Its pretty close to 27!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 12.3193 - mae: 12.3193\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.7804 - mae: 11.7804\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.2324 - mae: 11.2324\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.6601 - mae: 10.6601\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.0632 - mae: 10.0632\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.4503 - mae: 9.4503\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.7991 - mae: 8.7991\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1072 - mae: 8.1072\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.3691 - mae: 7.3691\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.5758 - mae: 6.5758\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.7205 - mae: 5.7205\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7947 - mae: 4.7947\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3581 - mae: 4.3581\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3134 - mae: 4.3134\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 4.2550 - mae: 4.2550\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.2442 - mae: 4.2442\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 4.1520 - mae: 4.1520\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1739 - mae: 4.1739\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.0681 - mae: 4.0681\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.0807 - mae: 4.0807\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9954 - mae: 3.9954\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9739 - mae: 3.9739\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9208 - mae: 3.9208\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.9047 - mae: 3.9047\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9267 - mae: 3.9267\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8797 - mae: 3.8797\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9341 - mae: 3.9341\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8678 - mae: 3.8678\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.9274 - mae: 3.9274\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8751 - mae: 3.8751\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9080 - mae: 3.9080\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8893 - mae: 3.8893\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8834 - mae: 3.8834\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8969 - mae: 3.8969\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8581 - mae: 3.8581\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9046 - mae: 3.9046\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8386 - mae: 3.8386\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.9054 - mae: 3.9054\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8482 - mae: 3.8482\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8862 - mae: 3.8862\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8605 - mae: 3.8605\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.8608 - mae: 3.8608\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8683 - mae: 3.8683\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8352 - mae: 3.8352\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8762 - mae: 3.8762\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8106 - mae: 3.8106\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8821 - mae: 3.8821\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8234 - mae: 3.8234\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8626 - mae: 3.8626\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8328 - mae: 3.8328\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.8369 - mae: 3.8369\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8408 - mae: 3.8408\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8111 - mae: 3.8111\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8489 - mae: 3.8489\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7850 - mae: 3.7850\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8585 - mae: 3.8585\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7982 - mae: 3.7982\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8377 - mae: 3.8377\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.8062 - mae: 3.8062\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8117 - mae: 3.8117\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8144 - mae: 3.8144\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7856 - mae: 3.7856\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8227 - mae: 3.8227\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7593 - mae: 3.7593\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8352 - mae: 3.8352\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7725 - mae: 3.7725\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8115 - mae: 3.8115\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7807 - mae: 3.7807\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7853 - mae: 3.7853\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7891 - mae: 3.7891\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7588 - mae: 3.7588\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7975 - mae: 3.7975\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7337 - mae: 3.7337\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.8105 - mae: 3.8105\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7478 - mae: 3.7478\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7840 - mae: 3.7840\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7563 - mae: 3.7563\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7575 - mae: 3.7575\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7648 - mae: 3.7648\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7307 - mae: 3.7307\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7735 - mae: 3.7735\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7125 - mae: 3.7125\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7820 - mae: 3.7820\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7242 - mae: 3.7242\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.7552 - mae: 3.7552\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7329 - mae: 3.7329\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.7284 - mae: 3.7284\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7416 - mae: 3.7416\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7013 - mae: 3.7013\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7505 - mae: 3.7505\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.6921 - mae: 3.6921\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7522 - mae: 3.7522\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.7016 - mae: 3.7016\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7251 - mae: 3.7251\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7105 - mae: 3.7105\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6979 - mae: 3.6979\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.7194 - mae: 3.7194\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.6705 - mae: 3.6705\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7299 - mae: 3.7299\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.6711 - mae: 3.6711\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x249763e7ac0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets test by adding in a hidden layer\n",
    "\n",
    "# 1. Create the model \n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[31.22314]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to see if its performance has improved.\n",
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohkay.....this seems a bit off...but we're getting there. This might be overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 12.7339 - mae: 12.7339\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.6498 - mae: 12.6498\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 12.5666 - mae: 12.5666\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 12.4824 - mae: 12.4824\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.3987 - mae: 12.3987\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 12.3151 - mae: 12.3151\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 12.2313 - mae: 12.2313\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 12.1475 - mae: 12.1475\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 12.0636 - mae: 12.0636\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.9797 - mae: 11.9797\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.8960 - mae: 11.8960\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.8131 - mae: 11.8131\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.7301 - mae: 11.7301\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.6470 - mae: 11.6470\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 11.5639 - mae: 11.5639\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.4806 - mae: 11.4806\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.3973 - mae: 11.3973\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.3140 - mae: 11.3140\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.2306 - mae: 11.2306\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 11.1471 - mae: 11.1471\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.0633 - mae: 11.0633\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.9795 - mae: 10.9795\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.8956 - mae: 10.8956\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.8120 - mae: 10.8120\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.7287 - mae: 10.7287\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.6454 - mae: 10.6454\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.5619 - mae: 10.5619\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 10.4782 - mae: 10.4782\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.3943 - mae: 10.3943\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 10.3101 - mae: 10.3101\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 10.2256 - mae: 10.2256\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.1409 - mae: 10.1409\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 10.0650 - mae: 10.0650\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0023 - mae: 10.0023\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.9390 - mae: 9.9390\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 9.8751 - mae: 9.8751\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.8108 - mae: 9.8108\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.7458 - mae: 9.7458\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 9.6804 - mae: 9.6804\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.6145 - mae: 9.6145\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.5482 - mae: 9.5482\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 9.4813 - mae: 9.4813\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.4138 - mae: 9.4138\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 9.3459 - mae: 9.3459\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 9.2776 - mae: 9.2776\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 9.2088 - mae: 9.2088\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.1395 - mae: 9.1395\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.0698 - mae: 9.0698\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.9997 - mae: 8.9997\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.9291 - mae: 8.9291\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 8.8583 - mae: 8.8583\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.7871 - mae: 8.7871\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 8.7154 - mae: 8.7154\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.6433 - mae: 8.6433\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.5707 - mae: 8.5707\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 8.4975 - mae: 8.4975\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.4240 - mae: 8.4240\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.3498 - mae: 8.3498\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.2752 - mae: 8.2752\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.2000 - mae: 8.2000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.1244 - mae: 8.1244\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.0483 - mae: 8.0483\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.9716 - mae: 7.9716\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.8944 - mae: 7.8944\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.8167 - mae: 7.8167\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.7385 - mae: 7.7385\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.6598 - mae: 7.6598\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.5806 - mae: 7.5806\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.5011 - mae: 7.5011\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.4209 - mae: 7.4209\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 7.3402 - mae: 7.3402\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.2589 - mae: 7.2589\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.1770 - mae: 7.1770\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 7.0944 - mae: 7.0944\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 7.0113 - mae: 7.0113\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.9275 - mae: 6.9275\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.8432 - mae: 6.8432\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.7582 - mae: 6.7582\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.6726 - mae: 6.6726\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.5864 - mae: 6.5864\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 6.4996 - mae: 6.4996\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.4122 - mae: 6.4122\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.3241 - mae: 6.3241\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.2354 - mae: 6.2354\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.1461 - mae: 6.1461\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 6.0561 - mae: 6.0561\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.9655 - mae: 5.9655\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.8742 - mae: 5.8742\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.7823 - mae: 5.7823\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.6897 - mae: 5.6897\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.5965 - mae: 5.5965\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.5026 - mae: 5.5026\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.4080 - mae: 5.4080\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.3128 - mae: 5.3128\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.2168 - mae: 5.2168\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 5.1201 - mae: 5.1201\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 5.0227 - mae: 5.0227\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9286 - mae: 4.9286\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.8307 - mae: 4.8307\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.7286 - mae: 4.7286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497d708940>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try the Adam optimizer this time\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(100, activation = \"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[26.75023]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah this is worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 12.4190 - mae: 12.4190\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 11.8429 - mae: 11.8429\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 11.2581 - mae: 11.2581\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 10.6721 - mae: 10.6721\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 10.0804 - mae: 10.0804\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 9.5489 - mae: 9.5489\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 9.1225 - mae: 9.1225\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 8.6844 - mae: 8.6844\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 8.2325 - mae: 8.2325\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 7.7671 - mae: 7.7671\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 7.2883 - mae: 7.2883\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 6.7943 - mae: 6.7943\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 6.2844 - mae: 6.2844\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 5.7579 - mae: 5.7579\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 5.2143 - mae: 5.2143\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 4.6535 - mae: 4.6535\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0754 - mae: 4.0754\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.8555 - mae: 3.8555\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.7786 - mae: 3.7786\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.8772 - mae: 3.8772\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.9592 - mae: 3.9592\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0442 - mae: 4.0442\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.2009 - mae: 4.2009\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.2947 - mae: 4.2947\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3319 - mae: 4.3319\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3194 - mae: 4.3194\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.2630 - mae: 4.2630\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.1675 - mae: 4.1675\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 4.0397 - mae: 4.0397\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.8833 - mae: 3.8833\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7864 - mae: 3.7864\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.7056 - mae: 3.7056\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.6225 - mae: 3.6225\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.5392 - mae: 3.5392\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 3.4557 - mae: 3.4557\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.3690 - mae: 3.3690\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3832 - mae: 3.3832\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 3.3844 - mae: 3.3844\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.3621 - mae: 3.3621\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.3699 - mae: 3.3699\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.3355 - mae: 3.3355\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.2558 - mae: 3.2558\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 3.1418 - mae: 3.1418\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0561 - mae: 3.0561\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 3.0588 - mae: 3.0588\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0587 - mae: 3.0587\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 3.0463 - mae: 3.0463\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 3.0225 - mae: 3.0225\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.9879 - mae: 2.9879\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.9434 - mae: 2.9434\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.8864 - mae: 2.8864\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.8234 - mae: 2.8234\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.7595 - mae: 2.7595\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.6876 - mae: 2.6876\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.6068 - mae: 2.6068\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5252 - mae: 2.5252\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4880 - mae: 2.4880\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4156 - mae: 2.4156\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3245 - mae: 2.3245\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2534 - mae: 2.2534\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1764 - mae: 2.1764\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1018 - mae: 2.1018\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0224 - mae: 2.0224\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.9605 - mae: 1.9605\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8925 - mae: 1.8925\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7844 - mae: 1.7844\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.7111 - mae: 1.7111\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.6392 - mae: 1.6392\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.5528 - mae: 1.5528\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.4482 - mae: 1.4482\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3288 - mae: 1.3288\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.2027 - mae: 1.2027\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0693 - mae: 1.0693\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9998 - mae: 0.9998\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8935 - mae: 0.8935\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7530 - mae: 0.7530\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5703 - mae: 0.5703\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4596 - mae: 0.4596\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3370 - mae: 0.3370\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1705 - mae: 0.1705\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3356 - mae: 0.3356\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2114 - mae: 0.2114\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2668 - mae: 0.2668\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.3382 - mae: 0.3382\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4465 - mae: 0.4465\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4333 - mae: 0.4333\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4359 - mae: 0.4359\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3670 - mae: 0.3670\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3806 - mae: 0.3806\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3484 - mae: 0.3484\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.3203 - mae: 0.3203\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2763 - mae: 0.2763\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.3990 - mae: 0.3990\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2378 - mae: 0.2378\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4056 - mae: 0.4056\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5224 - mae: 0.5224\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.4573 - mae: 0.4573\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1685 - mae: 0.1685\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.3968 - mae: 0.3968\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5774 - mae: 0.5774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497d7d36d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to reduce the number of neurons in the hidden layer. Maybe it will end up improving things for us?\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(50, activation = \"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation = None)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X, y, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[28.953987]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([[17.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our best model so far! We're sitting almost at 27!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving a model - Methods\n",
    "\n",
    "Common ways to improve a deep model:\n",
    "* Adding layers\n",
    "* Increase the number of hidden units\n",
    "* Change the activation functions\n",
    "* Change the optimization function\n",
    "* Change the learning rate\n",
    "* Fitting on more data\n",
    "* Fitting for longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a model\n",
    "In practice, a typical workflow you'll go through when building neural networks is:\n",
    "\n",
    "```\n",
    "Build a model -> fit it -> evaluate it -> tweak a model -> fit it -> evaluate it -> tweak a model -> fit it .....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to evaluation....there are 3 words you should memorize:\n",
    ">\"Visualize, visualize, visualize\"\n",
    "\n",
    "Its a good idea to visualize:\n",
    "* The data - what data are we working with? What does it look like\n",
    "* The model itself - what does our model look like?\n",
    "* The training of a model - how does a model perform while it learns?\n",
    "* The predictions of the model - how do the predictions of a model line up against the ground truth (the original labels)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a bigger dataset\n",
    "\n",
    "X = tf.cast(tf.range(-100, 100, 4), dtype=tf.float32)\n",
    "\n",
    "# Make labels for the dataset\n",
    "y = X + 10 + tf.random.uniform(X.shape, -10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2497d898220>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 3 sets ...\n",
    "\n",
    "* **Training set** - the model learns from this data, which is typically 70-80%\n",
    "* **Validation set** - the model gets tuned on this data, which is typically 10-15% of the data available\n",
    "* **Test set** - the model gets evaluated on this data to test what it has learnt, this set is typically 10-15% of the total data available\n",
    "\n",
    "Analogy<br>\n",
    "Course materials -> Practice exam  -> Final exam<br>\n",
    "[train_set]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[validation_set]&nbsp;&nbsp;&nbsp;[test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three sets...\n",
    "random_permutation = np.random.permutation(X.numpy().shape[0])\n",
    "test_set_idx, val_set_idx, train_set_idx = random_permutation[:len(random_permutation)//10], random_permutation[len(random_permutation)//10:int(0.3*len(random_permutation))], random_permutation[int(0.3*len(random_permutation)):]\n",
    "\n",
    "X_train, X_val, X_test = tf.convert_to_tensor(X.numpy()[train_set_idx])[..., tf.newaxis], tf.convert_to_tensor(X.numpy()[val_set_idx])[..., tf.newaxis], tf.convert_to_tensor(X.numpy()[test_set_idx])[..., tf.newaxis]\n",
    "y_train, y_val, y_test = tf.convert_to_tensor(y.numpy()[train_set_idx]), tf.convert_to_tensor(y.numpy()[val_set_idx]), tf.convert_to_tensor(y.numpy()[test_set_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the data\n",
    "\n",
    "Now we've got our data in training and test sets....lets visualise it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10,7))\n",
    "# Plot the training data in blue\n",
    "plt.scatter(X_train, y_train, c=\"b\", label = \"training_data\")\n",
    "# Plot the val data in green\n",
    "plt.scatter(X_val, y_val, c = \"g\", label = \"validation_data\")\n",
    "# Plot the test data in red\n",
    "plt.scatter(X_test, y_test, c = \"r\", label = \"test_data\")\n",
    "# Show a legend\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1,activation = None, name = \"Dense_Layer\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Pratik Sanghavi\\Desktop\\Projects\\Machine-Learning\\17. Tensorflow Developer Certification Notes\\Neural_Network_Regression_with_Tensorflow.ipynb Cell 44'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Pratik%20Sanghavi/Desktop/Projects/Machine-Learning/17.%20Tensorflow%20Developer%20Certification%20Notes/Neural_Network_Regression_with_Tensorflow.ipynb#ch0000043?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msummary()\n",
      "File \u001b[1;32mc:\\Users\\Pratik Sanghavi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2869\u001b[0m, in \u001b[0;36mModel.summary\u001b[1;34m(self, line_length, positions, print_fn, expand_nested, show_trainable)\u001b[0m\n\u001b[0;32m   2847\u001b[0m \u001b[39m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[0;32m   2848\u001b[0m \n\u001b[0;32m   2849\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2866\u001b[0m \u001b[39m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[0;32m   2867\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2868\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m-> 2869\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2870\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2871\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   2872\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m   2873\u001b[0m layer_utils\u001b[39m.\u001b[39mprint_summary(\n\u001b[0;32m   2874\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   2875\u001b[0m     line_length\u001b[39m=\u001b[39mline_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2878\u001b[0m     expand_nested\u001b[39m=\u001b[39mexpand_nested,\n\u001b[0;32m   2879\u001b[0m     show_trainable\u001b[39m=\u001b[39mshow_trainable)\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because input shape to the model is not defined. Lets create a model with a defined input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1,input_shape = (1,), activation = None, name = \"Dense_Layer\")\n",
    "])\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_Layer (Dense)         (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presto! We have our model instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total params - total number of parameters in the model\n",
    "* Trainable params - these are the parameters (patterns) the model can update as it trains\n",
    "* Non-trainable params - these parameteres aren't updated during training (this is typical when you have parameters from other models during transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "import pydot_ng as pydot\n",
    "import graphviz\n",
    "pydot.find_graphviz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 - 0s - loss: 21.0919 - mae: 21.0919 - val_loss: 7.6659 - val_mae: 7.6659 - 119ms/epoch - 59ms/step\n",
      "Epoch 2/100\n",
      "2/2 - 0s - loss: 10.6932 - mae: 10.6932 - val_loss: 11.0732 - val_mae: 11.0732 - 82ms/epoch - 41ms/step\n",
      "Epoch 3/100\n",
      "2/2 - 0s - loss: 10.4076 - mae: 10.4076 - val_loss: 19.2308 - val_mae: 19.2308 - 60ms/epoch - 30ms/step\n",
      "Epoch 4/100\n",
      "2/2 - 0s - loss: 24.7479 - mae: 24.7479 - val_loss: 7.0820 - val_mae: 7.0820 - 59ms/epoch - 29ms/step\n",
      "Epoch 5/100\n",
      "2/2 - 0s - loss: 9.4267 - mae: 9.4267 - val_loss: 7.6226 - val_mae: 7.6226 - 60ms/epoch - 30ms/step\n",
      "Epoch 6/100\n",
      "2/2 - 0s - loss: 10.8548 - mae: 10.8548 - val_loss: 10.0043 - val_mae: 10.0043 - 61ms/epoch - 31ms/step\n",
      "Epoch 7/100\n",
      "2/2 - 0s - loss: 10.7805 - mae: 10.7805 - val_loss: 10.1858 - val_mae: 10.1858 - 54ms/epoch - 27ms/step\n",
      "Epoch 8/100\n",
      "2/2 - 0s - loss: 9.6701 - mae: 9.6701 - val_loss: 7.5864 - val_mae: 7.5864 - 60ms/epoch - 30ms/step\n",
      "Epoch 9/100\n",
      "2/2 - 0s - loss: 9.8537 - mae: 9.8537 - val_loss: 36.9268 - val_mae: 36.9268 - 51ms/epoch - 25ms/step\n",
      "Epoch 10/100\n",
      "2/2 - 0s - loss: 34.7312 - mae: 34.7312 - val_loss: 21.3380 - val_mae: 21.3380 - 56ms/epoch - 28ms/step\n",
      "Epoch 11/100\n",
      "2/2 - 0s - loss: 19.1501 - mae: 19.1501 - val_loss: 34.2043 - val_mae: 34.2043 - 60ms/epoch - 30ms/step\n",
      "Epoch 12/100\n",
      "2/2 - 0s - loss: 31.6341 - mae: 31.6341 - val_loss: 7.0301 - val_mae: 7.0301 - 57ms/epoch - 29ms/step\n",
      "Epoch 13/100\n",
      "2/2 - 0s - loss: 9.3065 - mae: 9.3065 - val_loss: 28.9005 - val_mae: 28.9005 - 48ms/epoch - 24ms/step\n",
      "Epoch 14/100\n",
      "2/2 - 0s - loss: 27.7367 - mae: 27.7367 - val_loss: 13.6343 - val_mae: 13.6343 - 70ms/epoch - 35ms/step\n",
      "Epoch 15/100\n",
      "2/2 - 0s - loss: 12.2846 - mae: 12.2846 - val_loss: 24.0605 - val_mae: 24.0605 - 60ms/epoch - 30ms/step\n",
      "Epoch 16/100\n",
      "2/2 - 0s - loss: 22.7740 - mae: 22.7740 - val_loss: 7.0246 - val_mae: 7.0246 - 60ms/epoch - 30ms/step\n",
      "Epoch 17/100\n",
      "2/2 - 0s - loss: 9.7780 - mae: 9.7780 - val_loss: 22.1850 - val_mae: 22.1850 - 57ms/epoch - 28ms/step\n",
      "Epoch 18/100\n",
      "2/2 - 0s - loss: 19.7918 - mae: 19.7918 - val_loss: 25.4722 - val_mae: 25.4722 - 46ms/epoch - 23ms/step\n",
      "Epoch 19/100\n",
      "2/2 - 0s - loss: 24.4506 - mae: 24.4506 - val_loss: 7.4028 - val_mae: 7.4028 - 57ms/epoch - 29ms/step\n",
      "Epoch 20/100\n",
      "2/2 - 0s - loss: 11.0872 - mae: 11.0872 - val_loss: 7.2908 - val_mae: 7.2908 - 58ms/epoch - 29ms/step\n",
      "Epoch 21/100\n",
      "2/2 - 0s - loss: 10.6798 - mae: 10.6798 - val_loss: 26.5735 - val_mae: 26.5735 - 60ms/epoch - 30ms/step\n",
      "Epoch 22/100\n",
      "2/2 - 0s - loss: 32.0041 - mae: 32.0041 - val_loss: 13.4528 - val_mae: 13.4528 - 57ms/epoch - 29ms/step\n",
      "Epoch 23/100\n",
      "2/2 - 0s - loss: 11.9175 - mae: 11.9175 - val_loss: 10.2665 - val_mae: 10.2665 - 48ms/epoch - 24ms/step\n",
      "Epoch 24/100\n",
      "2/2 - 0s - loss: 10.9065 - mae: 10.9065 - val_loss: 21.5800 - val_mae: 21.5800 - 56ms/epoch - 28ms/step\n",
      "Epoch 25/100\n",
      "2/2 - 0s - loss: 20.1593 - mae: 20.1593 - val_loss: 26.1780 - val_mae: 26.1780 - 42ms/epoch - 21ms/step\n",
      "Epoch 26/100\n",
      "2/2 - 0s - loss: 25.3472 - mae: 25.3472 - val_loss: 14.8040 - val_mae: 14.8040 - 52ms/epoch - 26ms/step\n",
      "Epoch 27/100\n",
      "2/2 - 0s - loss: 12.9855 - mae: 12.9855 - val_loss: 9.4758 - val_mae: 9.4758 - 58ms/epoch - 29ms/step\n",
      "Epoch 28/100\n",
      "2/2 - 0s - loss: 9.2944 - mae: 9.2944 - val_loss: 14.0676 - val_mae: 14.0676 - 59ms/epoch - 29ms/step\n",
      "Epoch 29/100\n",
      "2/2 - 0s - loss: 19.3000 - mae: 19.3000 - val_loss: 10.4771 - val_mae: 10.4771 - 58ms/epoch - 29ms/step\n",
      "Epoch 30/100\n",
      "2/2 - 0s - loss: 13.9623 - mae: 13.9623 - val_loss: 27.7402 - val_mae: 27.7402 - 61ms/epoch - 30ms/step\n",
      "Epoch 31/100\n",
      "2/2 - 0s - loss: 35.2243 - mae: 35.2243 - val_loss: 9.5826 - val_mae: 9.5826 - 56ms/epoch - 28ms/step\n",
      "Epoch 32/100\n",
      "2/2 - 0s - loss: 13.2138 - mae: 13.2138 - val_loss: 22.0842 - val_mae: 22.0842 - 44ms/epoch - 22ms/step\n",
      "Epoch 33/100\n",
      "2/2 - 0s - loss: 19.9044 - mae: 19.9044 - val_loss: 7.2206 - val_mae: 7.2206 - 44ms/epoch - 22ms/step\n",
      "Epoch 34/100\n",
      "2/2 - 0s - loss: 9.9706 - mae: 9.9706 - val_loss: 8.3471 - val_mae: 8.3471 - 56ms/epoch - 28ms/step\n",
      "Epoch 35/100\n",
      "2/2 - 0s - loss: 13.4433 - mae: 13.4433 - val_loss: 22.1725 - val_mae: 22.1725 - 44ms/epoch - 22ms/step\n",
      "Epoch 36/100\n",
      "2/2 - 0s - loss: 26.7751 - mae: 26.7751 - val_loss: 11.0127 - val_mae: 11.0127 - 59ms/epoch - 30ms/step\n",
      "Epoch 37/100\n",
      "2/2 - 0s - loss: 10.4215 - mae: 10.4215 - val_loss: 13.0556 - val_mae: 13.0556 - 50ms/epoch - 25ms/step\n",
      "Epoch 38/100\n",
      "2/2 - 0s - loss: 17.4617 - mae: 17.4617 - val_loss: 6.7667 - val_mae: 6.7667 - 54ms/epoch - 27ms/step\n",
      "Epoch 39/100\n",
      "2/2 - 0s - loss: 9.2009 - mae: 9.2009 - val_loss: 11.6820 - val_mae: 11.6820 - 65ms/epoch - 33ms/step\n",
      "Epoch 40/100\n",
      "2/2 - 0s - loss: 15.9893 - mae: 15.9893 - val_loss: 26.9387 - val_mae: 26.9387 - 62ms/epoch - 31ms/step\n",
      "Epoch 41/100\n",
      "2/2 - 0s - loss: 31.7842 - mae: 31.7842 - val_loss: 20.0272 - val_mae: 20.0272 - 54ms/epoch - 27ms/step\n",
      "Epoch 42/100\n",
      "2/2 - 0s - loss: 19.3218 - mae: 19.3218 - val_loss: 7.1814 - val_mae: 7.1814 - 55ms/epoch - 27ms/step\n",
      "Epoch 43/100\n",
      "2/2 - 0s - loss: 10.1486 - mae: 10.1486 - val_loss: 14.1990 - val_mae: 14.1990 - 61ms/epoch - 30ms/step\n",
      "Epoch 44/100\n",
      "2/2 - 0s - loss: 11.8786 - mae: 11.8786 - val_loss: 9.4667 - val_mae: 9.4667 - 63ms/epoch - 32ms/step\n",
      "Epoch 45/100\n",
      "2/2 - 0s - loss: 13.0893 - mae: 13.0893 - val_loss: 18.6276 - val_mae: 18.6276 - 74ms/epoch - 37ms/step\n",
      "Epoch 46/100\n",
      "2/2 - 0s - loss: 21.4610 - mae: 21.4610 - val_loss: 38.6007 - val_mae: 38.6007 - 63ms/epoch - 31ms/step\n",
      "Epoch 47/100\n",
      "2/2 - 0s - loss: 36.7199 - mae: 36.7199 - val_loss: 7.1144 - val_mae: 7.1144 - 60ms/epoch - 30ms/step\n",
      "Epoch 48/100\n",
      "2/2 - 0s - loss: 9.6397 - mae: 9.6397 - val_loss: 11.4183 - val_mae: 11.4183 - 59ms/epoch - 29ms/step\n",
      "Epoch 49/100\n",
      "2/2 - 0s - loss: 14.3117 - mae: 14.3117 - val_loss: 19.7880 - val_mae: 19.7880 - 73ms/epoch - 37ms/step\n",
      "Epoch 50/100\n",
      "2/2 - 0s - loss: 24.0011 - mae: 24.0011 - val_loss: 6.8031 - val_mae: 6.8031 - 60ms/epoch - 30ms/step\n",
      "Epoch 51/100\n",
      "2/2 - 0s - loss: 9.0940 - mae: 9.0940 - val_loss: 16.7400 - val_mae: 16.7400 - 62ms/epoch - 31ms/step\n",
      "Epoch 52/100\n",
      "2/2 - 0s - loss: 15.9464 - mae: 15.9464 - val_loss: 8.9878 - val_mae: 8.9878 - 62ms/epoch - 31ms/step\n",
      "Epoch 53/100\n",
      "2/2 - 0s - loss: 9.2741 - mae: 9.2741 - val_loss: 8.0834 - val_mae: 8.0834 - 70ms/epoch - 35ms/step\n",
      "Epoch 54/100\n",
      "2/2 - 0s - loss: 9.4902 - mae: 9.4902 - val_loss: 35.7572 - val_mae: 35.7572 - 49ms/epoch - 24ms/step\n",
      "Epoch 55/100\n",
      "2/2 - 0s - loss: 34.9905 - mae: 34.9905 - val_loss: 6.6602 - val_mae: 6.6602 - 59ms/epoch - 30ms/step\n",
      "Epoch 56/100\n",
      "2/2 - 0s - loss: 9.2123 - mae: 9.2123 - val_loss: 32.0667 - val_mae: 32.0667 - 47ms/epoch - 24ms/step\n",
      "Epoch 57/100\n",
      "2/2 - 0s - loss: 29.1383 - mae: 29.1383 - val_loss: 20.9338 - val_mae: 20.9338 - 60ms/epoch - 30ms/step\n",
      "Epoch 58/100\n",
      "2/2 - 0s - loss: 24.8407 - mae: 24.8407 - val_loss: 6.7361 - val_mae: 6.7361 - 62ms/epoch - 31ms/step\n",
      "Epoch 59/100\n",
      "2/2 - 0s - loss: 8.8204 - mae: 8.8204 - val_loss: 28.1745 - val_mae: 28.1745 - 59ms/epoch - 30ms/step\n",
      "Epoch 60/100\n",
      "2/2 - 0s - loss: 25.7229 - mae: 25.7229 - val_loss: 6.7393 - val_mae: 6.7393 - 57ms/epoch - 29ms/step\n",
      "Epoch 61/100\n",
      "2/2 - 0s - loss: 9.1477 - mae: 9.1477 - val_loss: 6.5967 - val_mae: 6.5967 - 66ms/epoch - 33ms/step\n",
      "Epoch 62/100\n",
      "2/2 - 0s - loss: 8.9287 - mae: 8.9287 - val_loss: 7.0336 - val_mae: 7.0336 - 72ms/epoch - 36ms/step\n",
      "Epoch 63/100\n",
      "2/2 - 0s - loss: 10.6341 - mae: 10.6341 - val_loss: 11.6111 - val_mae: 11.6111 - 64ms/epoch - 32ms/step\n",
      "Epoch 64/100\n",
      "2/2 - 0s - loss: 16.1037 - mae: 16.1037 - val_loss: 15.6624 - val_mae: 15.6624 - 54ms/epoch - 27ms/step\n",
      "Epoch 65/100\n",
      "2/2 - 0s - loss: 19.1475 - mae: 19.1475 - val_loss: 6.7421 - val_mae: 6.7421 - 46ms/epoch - 23ms/step\n",
      "Epoch 66/100\n",
      "2/2 - 0s - loss: 9.0966 - mae: 9.0966 - val_loss: 32.2280 - val_mae: 32.2280 - 62ms/epoch - 31ms/step\n",
      "Epoch 67/100\n",
      "2/2 - 0s - loss: 30.8946 - mae: 30.8946 - val_loss: 17.7685 - val_mae: 17.7685 - 60ms/epoch - 30ms/step\n",
      "Epoch 68/100\n",
      "2/2 - 0s - loss: 15.8074 - mae: 15.8074 - val_loss: 27.5897 - val_mae: 27.5897 - 59ms/epoch - 30ms/step\n",
      "Epoch 69/100\n",
      "2/2 - 0s - loss: 26.6428 - mae: 26.6428 - val_loss: 30.7760 - val_mae: 30.7760 - 60ms/epoch - 30ms/step\n",
      "Epoch 70/100\n",
      "2/2 - 0s - loss: 27.8489 - mae: 27.8489 - val_loss: 16.2560 - val_mae: 16.2560 - 55ms/epoch - 28ms/step\n",
      "Epoch 71/100\n",
      "2/2 - 0s - loss: 16.3597 - mae: 16.3597 - val_loss: 20.0877 - val_mae: 20.0877 - 50ms/epoch - 25ms/step\n",
      "Epoch 72/100\n",
      "2/2 - 0s - loss: 16.6844 - mae: 16.6844 - val_loss: 18.3195 - val_mae: 18.3195 - 51ms/epoch - 25ms/step\n",
      "Epoch 73/100\n",
      "2/2 - 0s - loss: 23.7152 - mae: 23.7152 - val_loss: 6.4814 - val_mae: 6.4814 - 53ms/epoch - 26ms/step\n",
      "Epoch 74/100\n",
      "2/2 - 0s - loss: 9.0529 - mae: 9.0529 - val_loss: 19.3488 - val_mae: 19.3488 - 51ms/epoch - 25ms/step\n",
      "Epoch 75/100\n",
      "2/2 - 0s - loss: 22.3613 - mae: 22.3613 - val_loss: 37.1487 - val_mae: 37.1487 - 61ms/epoch - 31ms/step\n",
      "Epoch 76/100\n",
      "2/2 - 0s - loss: 34.8810 - mae: 34.8810 - val_loss: 13.0000 - val_mae: 13.0000 - 76ms/epoch - 38ms/step\n",
      "Epoch 77/100\n",
      "2/2 - 0s - loss: 16.5735 - mae: 16.5735 - val_loss: 6.4801 - val_mae: 6.4801 - 73ms/epoch - 37ms/step\n",
      "Epoch 78/100\n",
      "2/2 - 0s - loss: 8.8851 - mae: 8.8851 - val_loss: 23.3028 - val_mae: 23.3028 - 56ms/epoch - 28ms/step\n",
      "Epoch 79/100\n",
      "2/2 - 0s - loss: 29.1397 - mae: 29.1397 - val_loss: 6.4674 - val_mae: 6.4674 - 45ms/epoch - 22ms/step\n",
      "Epoch 80/100\n",
      "2/2 - 0s - loss: 8.7136 - mae: 8.7136 - val_loss: 16.6795 - val_mae: 16.6795 - 60ms/epoch - 30ms/step\n",
      "Epoch 81/100\n",
      "2/2 - 0s - loss: 14.4406 - mae: 14.4406 - val_loss: 12.4772 - val_mae: 12.4772 - 59ms/epoch - 29ms/step\n",
      "Epoch 82/100\n",
      "2/2 - 0s - loss: 16.8255 - mae: 16.8255 - val_loss: 13.3499 - val_mae: 13.3499 - 60ms/epoch - 30ms/step\n",
      "Epoch 83/100\n",
      "2/2 - 0s - loss: 18.8904 - mae: 18.8904 - val_loss: 18.6972 - val_mae: 18.6972 - 55ms/epoch - 28ms/step\n",
      "Epoch 84/100\n",
      "2/2 - 0s - loss: 21.2434 - mae: 21.2434 - val_loss: 42.5332 - val_mae: 42.5332 - 47ms/epoch - 24ms/step\n",
      "Epoch 85/100\n",
      "2/2 - 0s - loss: 39.6001 - mae: 39.6001 - val_loss: 18.8385 - val_mae: 18.8385 - 58ms/epoch - 29ms/step\n",
      "Epoch 86/100\n",
      "2/2 - 0s - loss: 23.1227 - mae: 23.1227 - val_loss: 13.1907 - val_mae: 13.1907 - 46ms/epoch - 23ms/step\n",
      "Epoch 87/100\n",
      "2/2 - 0s - loss: 14.0929 - mae: 14.0929 - val_loss: 20.0272 - val_mae: 20.0272 - 53ms/epoch - 27ms/step\n",
      "Epoch 88/100\n",
      "2/2 - 0s - loss: 19.3532 - mae: 19.3532 - val_loss: 7.4879 - val_mae: 7.4879 - 40ms/epoch - 20ms/step\n",
      "Epoch 89/100\n",
      "2/2 - 0s - loss: 8.6947 - mae: 8.6947 - val_loss: 12.0914 - val_mae: 12.0914 - 55ms/epoch - 28ms/step\n",
      "Epoch 90/100\n",
      "2/2 - 0s - loss: 16.7868 - mae: 16.7868 - val_loss: 7.0506 - val_mae: 7.0506 - 57ms/epoch - 29ms/step\n",
      "Epoch 91/100\n",
      "2/2 - 0s - loss: 10.0106 - mae: 10.0106 - val_loss: 20.4651 - val_mae: 20.4651 - 57ms/epoch - 29ms/step\n",
      "Epoch 92/100\n",
      "2/2 - 0s - loss: 23.7325 - mae: 23.7325 - val_loss: 22.2657 - val_mae: 22.2657 - 47ms/epoch - 23ms/step\n",
      "Epoch 93/100\n",
      "2/2 - 0s - loss: 19.8397 - mae: 19.8397 - val_loss: 16.5585 - val_mae: 16.5585 - 72ms/epoch - 36ms/step\n",
      "Epoch 94/100\n",
      "2/2 - 0s - loss: 13.5399 - mae: 13.5399 - val_loss: 6.3398 - val_mae: 6.3398 - 51ms/epoch - 26ms/step\n",
      "Epoch 95/100\n",
      "2/2 - 0s - loss: 8.6418 - mae: 8.6418 - val_loss: 6.9250 - val_mae: 6.9250 - 67ms/epoch - 34ms/step\n",
      "Epoch 96/100\n",
      "2/2 - 0s - loss: 8.5972 - mae: 8.5972 - val_loss: 21.5465 - val_mae: 21.5465 - 50ms/epoch - 25ms/step\n",
      "Epoch 97/100\n",
      "2/2 - 0s - loss: 25.1709 - mae: 25.1709 - val_loss: 6.3764 - val_mae: 6.3764 - 60ms/epoch - 30ms/step\n",
      "Epoch 98/100\n",
      "2/2 - 0s - loss: 8.9929 - mae: 8.9929 - val_loss: 11.7024 - val_mae: 11.7024 - 67ms/epoch - 34ms/step\n",
      "Epoch 99/100\n",
      "2/2 - 0s - loss: 15.6724 - mae: 15.6724 - val_loss: 6.3731 - val_mae: 6.3731 - 53ms/epoch - 26ms/step\n",
      "Epoch 100/100\n",
      "2/2 - 0s - loss: 8.8148 - mae: 8.8148 - val_loss: 11.2143 - val_mae: 11.2143 - 60ms/epoch - 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bcfff80a30>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets fit our model to the training data\n",
    "model.fit(X_train,y_train, validation_data=(X_val, y_val), epochs = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a model that builds automatically by defining the input_shape argument\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10,input_shape = (1,), activation = \"relu\", name = \"hidden_layer\"),\n",
    "    tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "], name = \"model_1\")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.SGD(),\n",
    "    metrics = [\"mae\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer (Dense)        (None, 10)                20        \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31\n",
      "Trainable params: 31\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 - 0s - loss: 118.8339 - mae: 118.8339 - val_loss: 64.7255 - val_mae: 64.7255 - 377ms/epoch - 188ms/step\n",
      "Epoch 2/100\n",
      "2/2 - 0s - loss: 72.4998 - mae: 72.4998 - val_loss: 22.4055 - val_mae: 22.4055 - 40ms/epoch - 20ms/step\n",
      "Epoch 3/100\n",
      "2/2 - 0s - loss: 26.6928 - mae: 26.6928 - val_loss: 14.2413 - val_mae: 14.2413 - 40ms/epoch - 20ms/step\n",
      "Epoch 4/100\n",
      "2/2 - 0s - loss: 17.9859 - mae: 17.9859 - val_loss: 10.7150 - val_mae: 10.7150 - 70ms/epoch - 35ms/step\n",
      "Epoch 5/100\n",
      "2/2 - 0s - loss: 11.2340 - mae: 11.2340 - val_loss: 12.2345 - val_mae: 12.2345 - 55ms/epoch - 27ms/step\n",
      "Epoch 6/100\n",
      "2/2 - 0s - loss: 12.3554 - mae: 12.3554 - val_loss: 19.0321 - val_mae: 19.0321 - 36ms/epoch - 18ms/step\n",
      "Epoch 7/100\n",
      "2/2 - 0s - loss: 22.3680 - mae: 22.3680 - val_loss: 18.6058 - val_mae: 18.6058 - 38ms/epoch - 19ms/step\n",
      "Epoch 8/100\n",
      "2/2 - 0s - loss: 20.4914 - mae: 20.4914 - val_loss: 22.5020 - val_mae: 22.5020 - 49ms/epoch - 24ms/step\n",
      "Epoch 9/100\n",
      "2/2 - 0s - loss: 23.4925 - mae: 23.4925 - val_loss: 20.4313 - val_mae: 20.4313 - 33ms/epoch - 16ms/step\n",
      "Epoch 10/100\n",
      "2/2 - 0s - loss: 25.4870 - mae: 25.4870 - val_loss: 23.6705 - val_mae: 23.6705 - 38ms/epoch - 19ms/step\n",
      "Epoch 11/100\n",
      "2/2 - 0s - loss: 28.3565 - mae: 28.3565 - val_loss: 11.0046 - val_mae: 11.0046 - 36ms/epoch - 18ms/step\n",
      "Epoch 12/100\n",
      "2/2 - 0s - loss: 13.0493 - mae: 13.0493 - val_loss: 17.8856 - val_mae: 17.8856 - 37ms/epoch - 19ms/step\n",
      "Epoch 13/100\n",
      "2/2 - 0s - loss: 19.0164 - mae: 19.0164 - val_loss: 24.6863 - val_mae: 24.6863 - 58ms/epoch - 29ms/step\n",
      "Epoch 14/100\n",
      "2/2 - 0s - loss: 28.0951 - mae: 28.0951 - val_loss: 10.5980 - val_mae: 10.5980 - 44ms/epoch - 22ms/step\n",
      "Epoch 15/100\n",
      "2/2 - 0s - loss: 11.2259 - mae: 11.2259 - val_loss: 21.1987 - val_mae: 21.1987 - 36ms/epoch - 18ms/step\n",
      "Epoch 16/100\n",
      "2/2 - 0s - loss: 23.8946 - mae: 23.8946 - val_loss: 19.4162 - val_mae: 19.4162 - 44ms/epoch - 22ms/step\n",
      "Epoch 17/100\n",
      "2/2 - 0s - loss: 19.9789 - mae: 19.9789 - val_loss: 21.6779 - val_mae: 21.6779 - 43ms/epoch - 21ms/step\n",
      "Epoch 18/100\n",
      "2/2 - 0s - loss: 24.7119 - mae: 24.7119 - val_loss: 14.6725 - val_mae: 14.6725 - 31ms/epoch - 15ms/step\n",
      "Epoch 19/100\n",
      "2/2 - 0s - loss: 17.4357 - mae: 17.4357 - val_loss: 13.2404 - val_mae: 13.2404 - 32ms/epoch - 16ms/step\n",
      "Epoch 20/100\n",
      "2/2 - 0s - loss: 16.5489 - mae: 16.5489 - val_loss: 11.7093 - val_mae: 11.7093 - 35ms/epoch - 18ms/step\n",
      "Epoch 21/100\n",
      "2/2 - 0s - loss: 11.9612 - mae: 11.9612 - val_loss: 12.2344 - val_mae: 12.2344 - 49ms/epoch - 24ms/step\n",
      "Epoch 22/100\n",
      "2/2 - 0s - loss: 14.2690 - mae: 14.2690 - val_loss: 16.0022 - val_mae: 16.0022 - 30ms/epoch - 15ms/step\n",
      "Epoch 23/100\n",
      "2/2 - 0s - loss: 18.6115 - mae: 18.6115 - val_loss: 10.3183 - val_mae: 10.3183 - 31ms/epoch - 15ms/step\n",
      "Epoch 24/100\n",
      "2/2 - 0s - loss: 13.9547 - mae: 13.9547 - val_loss: 19.2430 - val_mae: 19.2430 - 40ms/epoch - 20ms/step\n",
      "Epoch 25/100\n",
      "2/2 - 0s - loss: 23.8016 - mae: 23.8016 - val_loss: 14.3220 - val_mae: 14.3220 - 37ms/epoch - 18ms/step\n",
      "Epoch 26/100\n",
      "2/2 - 0s - loss: 18.2779 - mae: 18.2779 - val_loss: 11.3038 - val_mae: 11.3038 - 44ms/epoch - 22ms/step\n",
      "Epoch 27/100\n",
      "2/2 - 0s - loss: 11.9194 - mae: 11.9194 - val_loss: 11.8568 - val_mae: 11.8568 - 44ms/epoch - 22ms/step\n",
      "Epoch 28/100\n",
      "2/2 - 0s - loss: 13.1659 - mae: 13.1659 - val_loss: 11.8608 - val_mae: 11.8608 - 33ms/epoch - 17ms/step\n",
      "Epoch 29/100\n",
      "2/2 - 0s - loss: 12.4750 - mae: 12.4750 - val_loss: 10.8198 - val_mae: 10.8198 - 33ms/epoch - 16ms/step\n",
      "Epoch 30/100\n",
      "2/2 - 0s - loss: 10.9355 - mae: 10.9355 - val_loss: 16.6105 - val_mae: 16.6105 - 35ms/epoch - 18ms/step\n",
      "Epoch 31/100\n",
      "2/2 - 0s - loss: 20.9166 - mae: 20.9166 - val_loss: 19.8180 - val_mae: 19.8180 - 40ms/epoch - 20ms/step\n",
      "Epoch 32/100\n",
      "2/2 - 0s - loss: 25.1899 - mae: 25.1899 - val_loss: 10.4920 - val_mae: 10.4920 - 38ms/epoch - 19ms/step\n",
      "Epoch 33/100\n",
      "2/2 - 0s - loss: 11.1113 - mae: 11.1113 - val_loss: 13.4298 - val_mae: 13.4298 - 30ms/epoch - 15ms/step\n",
      "Epoch 34/100\n",
      "2/2 - 0s - loss: 16.8585 - mae: 16.8585 - val_loss: 15.8095 - val_mae: 15.8095 - 35ms/epoch - 17ms/step\n",
      "Epoch 35/100\n",
      "2/2 - 0s - loss: 19.4629 - mae: 19.4629 - val_loss: 24.5145 - val_mae: 24.5145 - 32ms/epoch - 16ms/step\n",
      "Epoch 36/100\n",
      "2/2 - 0s - loss: 27.7471 - mae: 27.7471 - val_loss: 19.2628 - val_mae: 19.2628 - 31ms/epoch - 15ms/step\n",
      "Epoch 37/100\n",
      "2/2 - 0s - loss: 21.0091 - mae: 21.0091 - val_loss: 9.7627 - val_mae: 9.7627 - 30ms/epoch - 15ms/step\n",
      "Epoch 38/100\n",
      "2/2 - 0s - loss: 10.9134 - mae: 10.9134 - val_loss: 9.7640 - val_mae: 9.7640 - 32ms/epoch - 16ms/step\n",
      "Epoch 39/100\n",
      "2/2 - 0s - loss: 11.0352 - mae: 11.0352 - val_loss: 10.5621 - val_mae: 10.5621 - 26ms/epoch - 13ms/step\n",
      "Epoch 40/100\n",
      "2/2 - 0s - loss: 12.8333 - mae: 12.8333 - val_loss: 12.9952 - val_mae: 12.9952 - 29ms/epoch - 14ms/step\n",
      "Epoch 41/100\n",
      "2/2 - 0s - loss: 13.4925 - mae: 13.4925 - val_loss: 9.7412 - val_mae: 9.7412 - 36ms/epoch - 18ms/step\n",
      "Epoch 42/100\n",
      "2/2 - 0s - loss: 11.3614 - mae: 11.3614 - val_loss: 23.1147 - val_mae: 23.1147 - 34ms/epoch - 17ms/step\n",
      "Epoch 43/100\n",
      "2/2 - 0s - loss: 26.9990 - mae: 26.9990 - val_loss: 13.0483 - val_mae: 13.0483 - 41ms/epoch - 21ms/step\n",
      "Epoch 44/100\n",
      "2/2 - 0s - loss: 16.8761 - mae: 16.8761 - val_loss: 12.9780 - val_mae: 12.9780 - 27ms/epoch - 13ms/step\n",
      "Epoch 45/100\n",
      "2/2 - 0s - loss: 15.6315 - mae: 15.6315 - val_loss: 10.6073 - val_mae: 10.6073 - 35ms/epoch - 17ms/step\n",
      "Epoch 46/100\n",
      "2/2 - 0s - loss: 11.0607 - mae: 11.0607 - val_loss: 14.0778 - val_mae: 14.0778 - 32ms/epoch - 16ms/step\n",
      "Epoch 47/100\n",
      "2/2 - 0s - loss: 16.7716 - mae: 16.7716 - val_loss: 9.6049 - val_mae: 9.6049 - 28ms/epoch - 14ms/step\n",
      "Epoch 48/100\n",
      "2/2 - 0s - loss: 11.1348 - mae: 11.1348 - val_loss: 26.0088 - val_mae: 26.0088 - 29ms/epoch - 15ms/step\n",
      "Epoch 49/100\n",
      "2/2 - 0s - loss: 29.7052 - mae: 29.7052 - val_loss: 10.0206 - val_mae: 10.0206 - 32ms/epoch - 16ms/step\n",
      "Epoch 50/100\n",
      "2/2 - 0s - loss: 11.5177 - mae: 11.5177 - val_loss: 17.4996 - val_mae: 17.4996 - 29ms/epoch - 14ms/step\n",
      "Epoch 51/100\n",
      "2/2 - 0s - loss: 20.3270 - mae: 20.3270 - val_loss: 9.6470 - val_mae: 9.6470 - 31ms/epoch - 16ms/step\n",
      "Epoch 52/100\n",
      "2/2 - 0s - loss: 10.7292 - mae: 10.7292 - val_loss: 16.3492 - val_mae: 16.3492 - 25ms/epoch - 12ms/step\n",
      "Epoch 53/100\n",
      "2/2 - 0s - loss: 19.6332 - mae: 19.6332 - val_loss: 13.9227 - val_mae: 13.9227 - 27ms/epoch - 14ms/step\n",
      "Epoch 54/100\n",
      "2/2 - 0s - loss: 17.2011 - mae: 17.2011 - val_loss: 9.5235 - val_mae: 9.5235 - 24ms/epoch - 12ms/step\n",
      "Epoch 55/100\n",
      "2/2 - 0s - loss: 11.0855 - mae: 11.0855 - val_loss: 27.9063 - val_mae: 27.9063 - 31ms/epoch - 15ms/step\n",
      "Epoch 56/100\n",
      "2/2 - 0s - loss: 31.5856 - mae: 31.5856 - val_loss: 10.0508 - val_mae: 10.0508 - 30ms/epoch - 15ms/step\n",
      "Epoch 57/100\n",
      "2/2 - 0s - loss: 11.3009 - mae: 11.3009 - val_loss: 9.4670 - val_mae: 9.4670 - 30ms/epoch - 15ms/step\n",
      "Epoch 58/100\n",
      "2/2 - 0s - loss: 10.6583 - mae: 10.6583 - val_loss: 9.5192 - val_mae: 9.5192 - 29ms/epoch - 14ms/step\n",
      "Epoch 59/100\n",
      "2/2 - 0s - loss: 10.7271 - mae: 10.7271 - val_loss: 11.0205 - val_mae: 11.0205 - 33ms/epoch - 17ms/step\n",
      "Epoch 60/100\n",
      "2/2 - 0s - loss: 11.2720 - mae: 11.2720 - val_loss: 11.4990 - val_mae: 11.4990 - 36ms/epoch - 18ms/step\n",
      "Epoch 61/100\n",
      "2/2 - 0s - loss: 14.7178 - mae: 14.7178 - val_loss: 10.0389 - val_mae: 10.0389 - 40ms/epoch - 20ms/step\n",
      "Epoch 62/100\n",
      "2/2 - 0s - loss: 13.4129 - mae: 13.4129 - val_loss: 31.7013 - val_mae: 31.7013 - 32ms/epoch - 16ms/step\n",
      "Epoch 63/100\n",
      "2/2 - 0s - loss: 35.0240 - mae: 35.0240 - val_loss: 20.1637 - val_mae: 20.1637 - 30ms/epoch - 15ms/step\n",
      "Epoch 64/100\n",
      "2/2 - 0s - loss: 22.0368 - mae: 22.0368 - val_loss: 23.5945 - val_mae: 23.5945 - 29ms/epoch - 14ms/step\n",
      "Epoch 65/100\n",
      "2/2 - 0s - loss: 25.0446 - mae: 25.0446 - val_loss: 10.1823 - val_mae: 10.1823 - 27ms/epoch - 14ms/step\n",
      "Epoch 66/100\n",
      "2/2 - 0s - loss: 12.9265 - mae: 12.9265 - val_loss: 10.2593 - val_mae: 10.2593 - 28ms/epoch - 14ms/step\n",
      "Epoch 67/100\n",
      "2/2 - 0s - loss: 12.1902 - mae: 12.1902 - val_loss: 15.1864 - val_mae: 15.1864 - 28ms/epoch - 14ms/step\n",
      "Epoch 68/100\n",
      "2/2 - 0s - loss: 17.3468 - mae: 17.3468 - val_loss: 10.0791 - val_mae: 10.0791 - 32ms/epoch - 16ms/step\n",
      "Epoch 69/100\n",
      "2/2 - 0s - loss: 10.7676 - mae: 10.7676 - val_loss: 11.1824 - val_mae: 11.1824 - 28ms/epoch - 14ms/step\n",
      "Epoch 70/100\n",
      "2/2 - 0s - loss: 11.5412 - mae: 11.5412 - val_loss: 9.9480 - val_mae: 9.9480 - 29ms/epoch - 15ms/step\n",
      "Epoch 71/100\n",
      "2/2 - 0s - loss: 10.5646 - mae: 10.5646 - val_loss: 10.9116 - val_mae: 10.9116 - 25ms/epoch - 12ms/step\n",
      "Epoch 72/100\n",
      "2/2 - 0s - loss: 13.4185 - mae: 13.4185 - val_loss: 11.4621 - val_mae: 11.4621 - 30ms/epoch - 15ms/step\n",
      "Epoch 73/100\n",
      "2/2 - 0s - loss: 13.7753 - mae: 13.7753 - val_loss: 23.1728 - val_mae: 23.1728 - 29ms/epoch - 15ms/step\n",
      "Epoch 74/100\n",
      "2/2 - 0s - loss: 24.3479 - mae: 24.3479 - val_loss: 34.6920 - val_mae: 34.6920 - 31ms/epoch - 15ms/step\n",
      "Epoch 75/100\n",
      "2/2 - 0s - loss: 35.5748 - mae: 35.5748 - val_loss: 9.2773 - val_mae: 9.2773 - 26ms/epoch - 13ms/step\n",
      "Epoch 76/100\n",
      "2/2 - 0s - loss: 10.4824 - mae: 10.4824 - val_loss: 9.5706 - val_mae: 9.5706 - 30ms/epoch - 15ms/step\n",
      "Epoch 77/100\n",
      "2/2 - 0s - loss: 10.6234 - mae: 10.6234 - val_loss: 17.0078 - val_mae: 17.0078 - 32ms/epoch - 16ms/step\n",
      "Epoch 78/100\n",
      "2/2 - 0s - loss: 20.9841 - mae: 20.9841 - val_loss: 15.9836 - val_mae: 15.9836 - 35ms/epoch - 17ms/step\n",
      "Epoch 79/100\n",
      "2/2 - 0s - loss: 18.1987 - mae: 18.1987 - val_loss: 13.9273 - val_mae: 13.9273 - 36ms/epoch - 18ms/step\n",
      "Epoch 80/100\n",
      "2/2 - 0s - loss: 17.1022 - mae: 17.1022 - val_loss: 11.9644 - val_mae: 11.9644 - 30ms/epoch - 15ms/step\n",
      "Epoch 81/100\n",
      "2/2 - 0s - loss: 13.2399 - mae: 13.2399 - val_loss: 9.2849 - val_mae: 9.2849 - 29ms/epoch - 15ms/step\n",
      "Epoch 82/100\n",
      "2/2 - 0s - loss: 11.0375 - mae: 11.0375 - val_loss: 9.9470 - val_mae: 9.9470 - 29ms/epoch - 15ms/step\n",
      "Epoch 83/100\n",
      "2/2 - 0s - loss: 12.3678 - mae: 12.3678 - val_loss: 11.0190 - val_mae: 11.0190 - 30ms/epoch - 15ms/step\n",
      "Epoch 84/100\n",
      "2/2 - 0s - loss: 13.6945 - mae: 13.6945 - val_loss: 10.7671 - val_mae: 10.7671 - 28ms/epoch - 14ms/step\n",
      "Epoch 85/100\n",
      "2/2 - 0s - loss: 12.5089 - mae: 12.5089 - val_loss: 15.0898 - val_mae: 15.0898 - 37ms/epoch - 19ms/step\n",
      "Epoch 86/100\n",
      "2/2 - 0s - loss: 17.8598 - mae: 17.8598 - val_loss: 9.9208 - val_mae: 9.9208 - 32ms/epoch - 16ms/step\n",
      "Epoch 87/100\n",
      "2/2 - 0s - loss: 10.4206 - mae: 10.4206 - val_loss: 9.1545 - val_mae: 9.1545 - 32ms/epoch - 16ms/step\n",
      "Epoch 88/100\n",
      "2/2 - 0s - loss: 10.4539 - mae: 10.4539 - val_loss: 12.2718 - val_mae: 12.2718 - 27ms/epoch - 14ms/step\n",
      "Epoch 89/100\n",
      "2/2 - 0s - loss: 14.0246 - mae: 14.0246 - val_loss: 12.8670 - val_mae: 12.8670 - 30ms/epoch - 15ms/step\n",
      "Epoch 90/100\n",
      "2/2 - 0s - loss: 17.0586 - mae: 17.0586 - val_loss: 9.5130 - val_mae: 9.5130 - 35ms/epoch - 18ms/step\n",
      "Epoch 91/100\n",
      "2/2 - 0s - loss: 10.3430 - mae: 10.3430 - val_loss: 9.1851 - val_mae: 9.1851 - 35ms/epoch - 17ms/step\n",
      "Epoch 92/100\n",
      "2/2 - 0s - loss: 10.2251 - mae: 10.2251 - val_loss: 18.1703 - val_mae: 18.1703 - 30ms/epoch - 15ms/step\n",
      "Epoch 93/100\n",
      "2/2 - 0s - loss: 19.8371 - mae: 19.8371 - val_loss: 9.4953 - val_mae: 9.4953 - 30ms/epoch - 15ms/step\n",
      "Epoch 94/100\n",
      "2/2 - 0s - loss: 10.3123 - mae: 10.3123 - val_loss: 9.1799 - val_mae: 9.1799 - 35ms/epoch - 18ms/step\n",
      "Epoch 95/100\n",
      "2/2 - 0s - loss: 11.0320 - mae: 11.0320 - val_loss: 9.1358 - val_mae: 9.1358 - 37ms/epoch - 19ms/step\n",
      "Epoch 96/100\n",
      "2/2 - 0s - loss: 10.1459 - mae: 10.1459 - val_loss: 23.6136 - val_mae: 23.6136 - 26ms/epoch - 13ms/step\n",
      "Epoch 97/100\n",
      "2/2 - 0s - loss: 25.2519 - mae: 25.2519 - val_loss: 13.1011 - val_mae: 13.1011 - 30ms/epoch - 15ms/step\n",
      "Epoch 98/100\n",
      "2/2 - 0s - loss: 15.6688 - mae: 15.6688 - val_loss: 9.0370 - val_mae: 9.0370 - 29ms/epoch - 15ms/step\n",
      "Epoch 99/100\n",
      "2/2 - 0s - loss: 10.4011 - mae: 10.4011 - val_loss: 16.1230 - val_mae: 16.1230 - 37ms/epoch - 19ms/step\n",
      "Epoch 100/100\n",
      "2/2 - 0s - loss: 20.7977 - mae: 20.7977 - val_loss: 16.1216 - val_mae: 16.1216 - 35ms/epoch - 18ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497d9dcf40>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets fit our model to the training data\n",
    "model.fit(X_train,y_train, validation_data=(X_val, y_val), epochs = 100, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising our Model's predictions\n",
    "To visualise predictions its a good idea to plot them against the ground truth labels.\n",
    "\n",
    "Often you'll see this in the form of y_test or y_true versus y_pred (ground truth vs your model's predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-53.24751  ],\n",
       "       [ -4.239314 ],\n",
       "       [-66.8609   ],\n",
       "       [-31.466093 ],\n",
       "       [ -6.9619923]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([-60.212627 ,  11.738316 , -95.907974 , -29.188904 ,   3.1377392],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you feel like you're going to reuse some kind of functionality in the future its a good idea to turn it into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create a plotting function\n",
    "def plot_predictions(\n",
    "    train_data = X_train,\n",
    "    train_labels = y_train,\n",
    "    test_data = X_test,\n",
    "    test_labels = y_test,\n",
    "    predictions = y_pred\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots training data, test data and compares predictions to ground truth\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,7))\n",
    "    # Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c =\"b\", label = \"Training data\")\n",
    "    # Plot testing data in green\n",
    "    plt.scatter(test_data, test_labels, c = \"g\", label = \"Testing data\")\n",
    "    # Plot model's predictions in red\n",
    "    plt.scatter(test_data, predictions, c = \"r\", label = \"Predictions\")\n",
    "    # Show the legend\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Model's Predictions with Regression Evaluation Metrics\n",
    "\n",
    "Depending on the problem you're working on, there will be different evaluation metrics to evaluate your model's performance. Since we're working on a regression, two of the main metrics:\n",
    "* MAE - mean absolute error, \"on average, how wrong is each of the model's predictions\"\n",
    "* MSE - mean squared error, \"square the average errors\"\n",
    "\n",
    "|Metric Name|Metric Formula|Tensorflow Code|When to use|\n",
    "|-----------|--------------|---------------|-----------|\n",
    "|Mean absolute error (MAE)|$MAE=\\frac{\\sum_{i=1}^nabs(y_i - x_i)}{n}$|tf.keras.losses.MAE() or tf.metrics.mean_absolute_error()|As a great starter metric for any regression problem|\n",
    "|Mean square error (MSE)|$MSE=\\frac{\\sum_{i=1}^n(y_i - x_i)^2}{n}$|tf.keras.losses.MSE() or tf.metrics.mean_square_error()|When larger errors are more significant than smaller errors|\n",
    "|Huber|$L_g(y, f(x)) = \\frac{(y-f(x))^2}{2} ? abs(y-f(x)) \\le \\delta; \\delta abs(y-f(x)) - \\frac{\\delta^2}{2} otherwise $|tf.keras.losses.Huber()|Combination of MSE and MAE. Less sensitive to outliers than MSE|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 16.1216 - mae: 16.1216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16.121566772460938, 16.121566772460938]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=12.873349>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean absolute error\n",
    "\n",
    "y_pred, y_test\n",
    "mae = tf.keras.losses.MAE(y_test, tf.squeeze(tf.constant(y_pred)))\n",
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=250.94405>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mean squared error\n",
    "mse = tf.keras.losses.MSE(y_test, tf.squeeze(tf.constant(y_pred)))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some functions to reuse MAE and MSE\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.keras.losses.MAE(y_true, tf.squeeze(tf.constant(y_pred)))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return tf.keras.losses.MSE(y_true, tf.squeeze(tf.constant(y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running experiments to improve our model\n",
    "\n",
    "```\n",
    "Build a model -> fit it -> evaluate it -> tweak it -> fit it -> evaluate it -> tweak it ....\n",
    "```\n",
    "\n",
    "Machine Learning Explorer's Motto:\n",
    "\n",
    "> \"Visualise, visualise, visualise\"\n",
    "\n",
    "Machine Learning Practitioner's Motto:\n",
    "\n",
    "> \"Experiment, experiment, experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get more data - get more examples for your model to train on (more opportunities to learn patterns or relationships between features and labels)\n",
    "2. Make your model larger (using a more complex model) - this might come in the form of more layers or more hidden units in each layer\n",
    "3. Train for longer - give your model more of a chance to find patterns in the data\n",
    "\n",
    "Lets do 3 modelling experiments:\n",
    "1. `model_1` - same as the original model, 1 layer, trained for 100 epochs\n",
    "2. `model_2` - 2 layers, trained for 100 epochs\n",
    "3. `model_3` - 2 layers, trained for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 164ms/step - loss: 26.4073 - mae: 26.4073 - val_loss: 20.0428 - val_mae: 20.0428\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 25.4800 - mae: 25.4800 - val_loss: 19.1579 - val_mae: 19.1579\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 24.5592 - mae: 24.5592 - val_loss: 18.3908 - val_mae: 18.3908\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 23.6680 - mae: 23.6680 - val_loss: 17.6559 - val_mae: 17.6559\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 22.7454 - mae: 22.7454 - val_loss: 16.9095 - val_mae: 16.9095\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 21.8626 - mae: 21.8626 - val_loss: 16.1556 - val_mae: 16.1556\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 20.9029 - mae: 20.9029 - val_loss: 15.3932 - val_mae: 15.3932\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 19.9968 - mae: 19.9968 - val_loss: 14.6443 - val_mae: 14.6443\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 19.0654 - mae: 19.0654 - val_loss: 13.9784 - val_mae: 13.9784\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 18.1167 - mae: 18.1167 - val_loss: 13.3192 - val_mae: 13.3192\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 17.1974 - mae: 17.1974 - val_loss: 12.6681 - val_mae: 12.6681\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 16.2352 - mae: 16.2352 - val_loss: 12.0113 - val_mae: 12.0113\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 15.3677 - mae: 15.3677 - val_loss: 11.3760 - val_mae: 11.3760\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 14.4784 - mae: 14.4784 - val_loss: 10.9394 - val_mae: 10.9394\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 13.8282 - mae: 13.8282 - val_loss: 10.7144 - val_mae: 10.7144\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 13.2136 - mae: 13.2136 - val_loss: 10.4896 - val_mae: 10.4896\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 12.6390 - mae: 12.6390 - val_loss: 10.2671 - val_mae: 10.2671\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 12.2003 - mae: 12.2003 - val_loss: 10.0503 - val_mae: 10.0503\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 11.7661 - mae: 11.7661 - val_loss: 9.9983 - val_mae: 9.9983\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 11.4602 - mae: 11.4602 - val_loss: 9.9571 - val_mae: 9.9571\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 11.2552 - mae: 11.2552 - val_loss: 9.9175 - val_mae: 9.9175\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 11.1169 - mae: 11.1169 - val_loss: 9.8788 - val_mae: 9.8788\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 11.0611 - mae: 11.0611 - val_loss: 9.8396 - val_mae: 9.8396\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 11.0446 - mae: 11.0446 - val_loss: 9.8030 - val_mae: 9.8030\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 10.9642 - mae: 10.9642 - val_loss: 9.7706 - val_mae: 9.7706\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10.9188 - mae: 10.9188 - val_loss: 9.7406 - val_mae: 9.7406\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 10.8893 - mae: 10.8893 - val_loss: 9.7110 - val_mae: 9.7110\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 10.8634 - mae: 10.8634 - val_loss: 9.6811 - val_mae: 9.6811\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 10.8399 - mae: 10.8399 - val_loss: 9.6509 - val_mae: 9.6509\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10.8266 - mae: 10.8266 - val_loss: 9.6228 - val_mae: 9.6228\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 10.7941 - mae: 10.7941 - val_loss: 9.5970 - val_mae: 9.5970\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 10.7729 - mae: 10.7729 - val_loss: 9.5712 - val_mae: 9.5712\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 10.7488 - mae: 10.7488 - val_loss: 9.5450 - val_mae: 9.5450\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 10.7236 - mae: 10.7236 - val_loss: 9.5172 - val_mae: 9.5172\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 10.6946 - mae: 10.6946 - val_loss: 9.4872 - val_mae: 9.4872\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 10.6708 - mae: 10.6708 - val_loss: 9.5167 - val_mae: 9.5167\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 10.6559 - mae: 10.6559 - val_loss: 9.5837 - val_mae: 9.5837\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 10.6305 - mae: 10.6305 - val_loss: 9.6343 - val_mae: 9.6343\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 10.6098 - mae: 10.6098 - val_loss: 9.6667 - val_mae: 9.6667\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 10.5812 - mae: 10.5812 - val_loss: 9.6888 - val_mae: 9.6888\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 10.5711 - mae: 10.5711 - val_loss: 9.6868 - val_mae: 9.6868\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 10.5391 - mae: 10.5391 - val_loss: 9.6412 - val_mae: 9.6412\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 10.5187 - mae: 10.5187 - val_loss: 9.5749 - val_mae: 9.5749\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 10.5051 - mae: 10.5051 - val_loss: 9.5101 - val_mae: 9.5101\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 10.4901 - mae: 10.4901 - val_loss: 9.4612 - val_mae: 9.4612\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 10.4719 - mae: 10.4719 - val_loss: 9.4345 - val_mae: 9.4345\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 10.4511 - mae: 10.4511 - val_loss: 9.4225 - val_mae: 9.4225\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 10.4341 - mae: 10.4341 - val_loss: 9.3925 - val_mae: 9.3925\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 10.4119 - mae: 10.4119 - val_loss: 9.3411 - val_mae: 9.3411\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 10.3975 - mae: 10.3975 - val_loss: 9.3036 - val_mae: 9.3036\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 10.3758 - mae: 10.3758 - val_loss: 9.2800 - val_mae: 9.2800\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 10.3569 - mae: 10.3569 - val_loss: 9.2669 - val_mae: 9.2669\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 10.3366 - mae: 10.3366 - val_loss: 9.2661 - val_mae: 9.2661\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 10.3155 - mae: 10.3155 - val_loss: 9.2694 - val_mae: 9.2694\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 10.3022 - mae: 10.3022 - val_loss: 9.2516 - val_mae: 9.2516\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 10.2748 - mae: 10.2748 - val_loss: 9.1987 - val_mae: 9.1987\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 10.2683 - mae: 10.2683 - val_loss: 9.1591 - val_mae: 9.1591\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 10.2419 - mae: 10.2419 - val_loss: 9.1414 - val_mae: 9.1414\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 10.2227 - mae: 10.2227 - val_loss: 9.1268 - val_mae: 9.1268\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 10.2033 - mae: 10.2033 - val_loss: 9.1146 - val_mae: 9.1146\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 10.1832 - mae: 10.1832 - val_loss: 9.1062 - val_mae: 9.1062\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 10.1658 - mae: 10.1658 - val_loss: 9.0933 - val_mae: 9.0933\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 10.1435 - mae: 10.1435 - val_loss: 9.0677 - val_mae: 9.0677\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 10.1255 - mae: 10.1255 - val_loss: 9.0635 - val_mae: 9.0635\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 10.1069 - mae: 10.1069 - val_loss: 9.0700 - val_mae: 9.0700\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 10.0861 - mae: 10.0861 - val_loss: 9.0545 - val_mae: 9.0545\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 10.0667 - mae: 10.0667 - val_loss: 9.0334 - val_mae: 9.0334\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 10.0484 - mae: 10.0484 - val_loss: 9.0257 - val_mae: 9.0257\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 10.0292 - mae: 10.0292 - val_loss: 9.0302 - val_mae: 9.0302\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 10.0121 - mae: 10.0121 - val_loss: 9.0340 - val_mae: 9.0340\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 9.9923 - mae: 9.9923 - val_loss: 9.0173 - val_mae: 9.0173\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 9.9727 - mae: 9.9727 - val_loss: 8.9932 - val_mae: 8.9932\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 9.9546 - mae: 9.9546 - val_loss: 8.9841 - val_mae: 8.9841\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 9.9325 - mae: 9.9325 - val_loss: 9.0094 - val_mae: 9.0094\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 9.9214 - mae: 9.9214 - val_loss: 9.0449 - val_mae: 9.0449\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.8933 - mae: 9.8933 - val_loss: 9.0613 - val_mae: 9.0613\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 9.8794 - mae: 9.8794 - val_loss: 9.0579 - val_mae: 9.0579\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 9.8555 - mae: 9.8555 - val_loss: 9.0426 - val_mae: 9.0426\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 9.8362 - mae: 9.8362 - val_loss: 9.0489 - val_mae: 9.0489\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 9.8172 - mae: 9.8172 - val_loss: 9.0677 - val_mae: 9.0677\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 9.7999 - mae: 9.7999 - val_loss: 9.0775 - val_mae: 9.0775\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 9.7798 - mae: 9.7798 - val_loss: 9.0684 - val_mae: 9.0684\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 9.7615 - mae: 9.7615 - val_loss: 9.0494 - val_mae: 9.0494\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 9.7441 - mae: 9.7441 - val_loss: 9.0405 - val_mae: 9.0405\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 9.7255 - mae: 9.7255 - val_loss: 9.0603 - val_mae: 9.0603\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 9.7002 - mae: 9.7002 - val_loss: 9.1165 - val_mae: 9.1165\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 9.6867 - mae: 9.6867 - val_loss: 9.1830 - val_mae: 9.1830\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 9.6997 - mae: 9.6997 - val_loss: 9.2160 - val_mae: 9.2160\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.7129 - mae: 9.7129 - val_loss: 9.2460 - val_mae: 9.2460\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 9.7376 - mae: 9.7376 - val_loss: 9.2905 - val_mae: 9.2905\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.7645 - mae: 9.7645 - val_loss: 9.2672 - val_mae: 9.2672\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 9.7297 - mae: 9.7297 - val_loss: 9.1833 - val_mae: 9.1833\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 9.6735 - mae: 9.6735 - val_loss: 9.0923 - val_mae: 9.0923\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 9.6031 - mae: 9.6031 - val_loss: 9.0095 - val_mae: 9.0095\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.5497 - mae: 9.5497 - val_loss: 8.9217 - val_mae: 8.9217\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 9.5525 - mae: 9.5525 - val_loss: 8.8598 - val_mae: 8.8598\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 9.5295 - mae: 9.5295 - val_loss: 8.8366 - val_mae: 8.8366\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 9.5121 - mae: 9.5121 - val_loss: 8.8297 - val_mae: 8.8297\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 9.4990 - mae: 9.4990 - val_loss: 8.8122 - val_mae: 8.8122\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.4787 - mae: 9.4787 - val_loss: 8.7619 - val_mae: 8.7619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497eb6c3a0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(1, activation = None, input_shape = (1, ), name = \"output_layer\")\n",
    "    ], name = 'model_1'\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = ['mae']\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make and plot predictions for model_1\n",
    "y_preds_1 = model.predict(X_test)\n",
    "plot_predictions(predictions = y_preds_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=16.769896>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=306.29105>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_1 = mae(y_test, y_preds_1)\n",
    "mse_1 = mse(y_test, y_preds_1)\n",
    "mae_1,mse_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 0s 133ms/step - loss: 48.9170 - mae: 48.9170 - val_loss: 30.0751 - val_mae: 30.0751\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 35.1800 - mae: 35.1800 - val_loss: 17.3734 - val_mae: 17.3734\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 21.3764 - mae: 21.3764 - val_loss: 6.3092 - val_mae: 6.3092\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 8.4734 - mae: 8.4734 - val_loss: 8.8267 - val_mae: 8.8267\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 11.3824 - mae: 11.3824 - val_loss: 14.1676 - val_mae: 14.1676\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 16.2367 - mae: 16.2367 - val_loss: 12.7001 - val_mae: 12.7001\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 14.5700 - mae: 14.5700 - val_loss: 8.8944 - val_mae: 8.8944\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 12.0028 - mae: 12.0028 - val_loss: 5.7233 - val_mae: 5.7233\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 10.1785 - mae: 10.1785 - val_loss: 5.7765 - val_mae: 5.7765\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 8.2552 - mae: 8.2552 - val_loss: 6.7994 - val_mae: 6.7994\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 9.6232 - mae: 9.6232 - val_loss: 8.5537 - val_mae: 8.5537\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 10.6407 - mae: 10.6407 - val_loss: 10.0930 - val_mae: 10.0930\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 11.4048 - mae: 11.4048 - val_loss: 8.7935 - val_mae: 8.7935\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 9.7438 - mae: 9.7438 - val_loss: 6.1834 - val_mae: 6.1834\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 7.2700 - mae: 7.2700 - val_loss: 4.6946 - val_mae: 4.6946\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6.6516 - mae: 6.6516 - val_loss: 4.6011 - val_mae: 4.6011\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 8.8518 - mae: 8.8518 - val_loss: 5.2828 - val_mae: 5.2828\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 9.1889 - mae: 9.1889 - val_loss: 4.4279 - val_mae: 4.4279\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 8.0146 - mae: 8.0146 - val_loss: 4.4804 - val_mae: 4.4804\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6.8654 - mae: 6.8654 - val_loss: 5.6847 - val_mae: 5.6847\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.3463 - mae: 6.3463 - val_loss: 6.4738 - val_mae: 6.4738\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.7427 - mae: 6.7427 - val_loss: 6.3109 - val_mae: 6.3109\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 7.1759 - mae: 7.1759 - val_loss: 5.9320 - val_mae: 5.9320\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6.6674 - mae: 6.6674 - val_loss: 5.0261 - val_mae: 5.0261\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.0380 - mae: 6.0380 - val_loss: 4.6623 - val_mae: 4.6623\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.8186 - mae: 6.8186 - val_loss: 4.5450 - val_mae: 4.5450\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 7.9554 - mae: 7.9554 - val_loss: 4.3041 - val_mae: 4.3041\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 8.1073 - mae: 8.1073 - val_loss: 4.0556 - val_mae: 4.0556\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 7.3105 - mae: 7.3105 - val_loss: 4.6297 - val_mae: 4.6297\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 6.1655 - mae: 6.1655 - val_loss: 6.0669 - val_mae: 6.0669\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.2809 - mae: 6.2809 - val_loss: 7.5921 - val_mae: 7.5921\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 8.2143 - mae: 8.2143 - val_loss: 7.6732 - val_mae: 7.6732\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 8.8420 - mae: 8.8420 - val_loss: 6.5149 - val_mae: 6.5149\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 7.9335 - mae: 7.9335 - val_loss: 5.3938 - val_mae: 5.3938\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.2623 - mae: 6.2623 - val_loss: 4.3863 - val_mae: 4.3863\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.2640 - mae: 6.2640 - val_loss: 4.7917 - val_mae: 4.7917\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 8.2800 - mae: 8.2800 - val_loss: 6.2911 - val_mae: 6.2911\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 9.5134 - mae: 9.5134 - val_loss: 7.2484 - val_mae: 7.2484\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 9.9571 - mae: 9.9571 - val_loss: 7.3794 - val_mae: 7.3794\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 9.8100 - mae: 9.8100 - val_loss: 6.2761 - val_mae: 6.2761\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 8.8748 - mae: 8.8748 - val_loss: 4.5479 - val_mae: 4.5479\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.3224 - mae: 7.3224 - val_loss: 5.1389 - val_mae: 5.1389\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.0906 - mae: 6.0906 - val_loss: 6.7471 - val_mae: 6.7471\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.8497 - mae: 6.8497 - val_loss: 7.7752 - val_mae: 7.7752\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 8.1393 - mae: 8.1393 - val_loss: 7.4250 - val_mae: 7.4250\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.6259 - mae: 7.6259 - val_loss: 5.7226 - val_mae: 5.7226\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.1112 - mae: 6.1112 - val_loss: 4.5903 - val_mae: 4.5903\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 5.9804 - mae: 5.9804 - val_loss: 4.4446 - val_mae: 4.4446\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 7.1639 - mae: 7.1639 - val_loss: 4.4518 - val_mae: 4.4518\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 7.3682 - mae: 7.3682 - val_loss: 4.5323 - val_mae: 4.5323\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 4.7141 - val_mae: 4.7141\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.9849 - mae: 5.9849 - val_loss: 5.1159 - val_mae: 5.1159\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.1466 - mae: 6.1466 - val_loss: 5.4232 - val_mae: 5.4232\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.3792 - mae: 6.3792 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.3134 - mae: 6.3134 - val_loss: 5.2228 - val_mae: 5.2228\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.0365 - mae: 6.0365 - val_loss: 4.9721 - val_mae: 4.9721\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.8925 - mae: 5.8925 - val_loss: 5.1421 - val_mae: 5.1421\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.8394 - mae: 5.8394 - val_loss: 5.3208 - val_mae: 5.3208\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.8355 - mae: 5.8355 - val_loss: 5.4494 - val_mae: 5.4494\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.2547 - mae: 6.2547 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 5.0305 - val_mae: 5.0305\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.0662 - mae: 6.0662 - val_loss: 4.8287 - val_mae: 4.8287\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.9408 - mae: 6.9408 - val_loss: 4.7979 - val_mae: 4.7979\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 8.3557 - mae: 8.3557 - val_loss: 4.8949 - val_mae: 4.8949\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 8.1828 - mae: 8.1828 - val_loss: 5.0451 - val_mae: 5.0451\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 7.0640 - mae: 7.0640 - val_loss: 4.9385 - val_mae: 4.9385\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.1923 - mae: 6.1923 - val_loss: 5.5874 - val_mae: 5.5874\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6.1034 - mae: 6.1034 - val_loss: 6.9171 - val_mae: 6.9171\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 6.9212 - mae: 6.9212 - val_loss: 7.2269 - val_mae: 7.2269\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 7.8923 - mae: 7.8923 - val_loss: 6.0135 - val_mae: 6.0135\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 7.6747 - mae: 7.6747 - val_loss: 4.5512 - val_mae: 4.5512\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 7.0100 - mae: 7.0100 - val_loss: 4.1061 - val_mae: 4.1061\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 7.1203 - mae: 7.1203 - val_loss: 3.9312 - val_mae: 3.9312\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 6.9148 - mae: 6.9148 - val_loss: 4.3364 - val_mae: 4.3364\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.4613 - mae: 6.4613 - val_loss: 4.8124 - val_mae: 4.8124\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.8529 - mae: 5.8529 - val_loss: 5.4392 - val_mae: 5.4392\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.2120 - mae: 6.2120 - val_loss: 5.8063 - val_mae: 5.8063\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 7.1301 - mae: 7.1301 - val_loss: 5.8724 - val_mae: 5.8724\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.9493 - mae: 6.9493 - val_loss: 5.5437 - val_mae: 5.5437\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 5.8889 - mae: 5.8889 - val_loss: 5.0598 - val_mae: 5.0598\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.1660 - mae: 6.1660 - val_loss: 5.4264 - val_mae: 5.4264\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.5037 - mae: 7.5037 - val_loss: 6.0319 - val_mae: 6.0319\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 8.1850 - mae: 8.1850 - val_loss: 5.4039 - val_mae: 5.4039\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 7.7611 - mae: 7.7611 - val_loss: 4.3062 - val_mae: 4.3062\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.3601 - mae: 6.3601 - val_loss: 4.9511 - val_mae: 4.9511\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.6208 - mae: 5.6208 - val_loss: 5.5269 - val_mae: 5.5269\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.6550 - mae: 6.6550 - val_loss: 5.7836 - val_mae: 5.7836\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.3101 - mae: 7.3101 - val_loss: 5.6079 - val_mae: 5.6079\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.7560 - mae: 6.7560 - val_loss: 4.9515 - val_mae: 4.9515\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.7513 - mae: 5.7513 - val_loss: 4.0867 - val_mae: 4.0867\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.7476 - mae: 6.7476 - val_loss: 4.5734 - val_mae: 4.5734\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.6530 - mae: 7.6530 - val_loss: 4.6551 - val_mae: 4.6551\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.3583 - mae: 7.3583 - val_loss: 4.4336 - val_mae: 4.4336\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.6141 - mae: 6.6141 - val_loss: 5.2215 - val_mae: 5.2215\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.7725 - mae: 5.7725 - val_loss: 5.5899 - val_mae: 5.5899\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.9679 - mae: 5.9679 - val_loss: 5.7327 - val_mae: 5.7327\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.5845 - mae: 6.5845 - val_loss: 5.5771 - val_mae: 5.5771\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.3526 - mae: 6.3526 - val_loss: 4.9853 - val_mae: 4.9853\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.8332 - mae: 5.8332 - val_loss: 4.3508 - val_mae: 4.3508\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.0349 - mae: 6.0349 - val_loss: 4.2080 - val_mae: 4.2080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2498010d690>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", input_shape = (1, ), name = \"hidden_layer\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = 'model_2'\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = ['mae']\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds_2 = model.predict(X_test)\n",
    "plot_predictions(predictions = y_preds_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=11.347539>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=138.56165>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_2 = mae(y_test, y_preds_2)\n",
    "mse_2 = mse(y_test, y_preds_2)\n",
    "mae_2,mse_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a substantial reduction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2/2 [==============================] - 1s 189ms/step - loss: 48.9170 - mae: 48.9170 - val_loss: 30.0751 - val_mae: 30.0751\n",
      "Epoch 2/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 35.1800 - mae: 35.1800 - val_loss: 17.3734 - val_mae: 17.3734\n",
      "Epoch 3/500\n",
      "2/2 [==============================] - 0s 85ms/step - loss: 21.3764 - mae: 21.3764 - val_loss: 6.3092 - val_mae: 6.3092\n",
      "Epoch 4/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 8.4734 - mae: 8.4734 - val_loss: 8.8267 - val_mae: 8.8267\n",
      "Epoch 5/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 11.3824 - mae: 11.3824 - val_loss: 14.1676 - val_mae: 14.1676\n",
      "Epoch 6/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 16.2367 - mae: 16.2367 - val_loss: 12.7001 - val_mae: 12.7001\n",
      "Epoch 7/500\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 14.5700 - mae: 14.5700 - val_loss: 8.8944 - val_mae: 8.8944\n",
      "Epoch 8/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 12.0028 - mae: 12.0028 - val_loss: 5.7233 - val_mae: 5.7233\n",
      "Epoch 9/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 10.1785 - mae: 10.1785 - val_loss: 5.7765 - val_mae: 5.7765\n",
      "Epoch 10/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 8.2552 - mae: 8.2552 - val_loss: 6.7994 - val_mae: 6.7994\n",
      "Epoch 11/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 9.6232 - mae: 9.6232 - val_loss: 8.5537 - val_mae: 8.5537\n",
      "Epoch 12/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 10.6407 - mae: 10.6407 - val_loss: 10.0930 - val_mae: 10.0930\n",
      "Epoch 13/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 11.4048 - mae: 11.4048 - val_loss: 8.7935 - val_mae: 8.7935\n",
      "Epoch 14/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 9.7438 - mae: 9.7438 - val_loss: 6.1834 - val_mae: 6.1834\n",
      "Epoch 15/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 7.2700 - mae: 7.2700 - val_loss: 4.6946 - val_mae: 4.6946\n",
      "Epoch 16/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.6516 - mae: 6.6516 - val_loss: 4.6011 - val_mae: 4.6011\n",
      "Epoch 17/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 8.8518 - mae: 8.8518 - val_loss: 5.2828 - val_mae: 5.2828\n",
      "Epoch 18/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 9.1889 - mae: 9.1889 - val_loss: 4.4279 - val_mae: 4.4279\n",
      "Epoch 19/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 8.0146 - mae: 8.0146 - val_loss: 4.4804 - val_mae: 4.4804\n",
      "Epoch 20/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.8654 - mae: 6.8654 - val_loss: 5.6847 - val_mae: 5.6847\n",
      "Epoch 21/500\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 6.3463 - mae: 6.3463 - val_loss: 6.4738 - val_mae: 6.4738\n",
      "Epoch 22/500\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 6.7427 - mae: 6.7427 - val_loss: 6.3109 - val_mae: 6.3109\n",
      "Epoch 23/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 7.1759 - mae: 7.1759 - val_loss: 5.9320 - val_mae: 5.9320\n",
      "Epoch 24/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.6674 - mae: 6.6674 - val_loss: 5.0261 - val_mae: 5.0261\n",
      "Epoch 25/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 6.0380 - mae: 6.0380 - val_loss: 4.6623 - val_mae: 4.6623\n",
      "Epoch 26/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.8186 - mae: 6.8186 - val_loss: 4.5450 - val_mae: 4.5450\n",
      "Epoch 27/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.9554 - mae: 7.9554 - val_loss: 4.3041 - val_mae: 4.3041\n",
      "Epoch 28/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 8.1073 - mae: 8.1073 - val_loss: 4.0556 - val_mae: 4.0556\n",
      "Epoch 29/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 7.3105 - mae: 7.3105 - val_loss: 4.6297 - val_mae: 4.6297\n",
      "Epoch 30/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 6.1655 - mae: 6.1655 - val_loss: 6.0669 - val_mae: 6.0669\n",
      "Epoch 31/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.2809 - mae: 6.2809 - val_loss: 7.5921 - val_mae: 7.5921\n",
      "Epoch 32/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 8.2143 - mae: 8.2143 - val_loss: 7.6732 - val_mae: 7.6732\n",
      "Epoch 33/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 8.8420 - mae: 8.8420 - val_loss: 6.5149 - val_mae: 6.5149\n",
      "Epoch 34/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 7.9335 - mae: 7.9335 - val_loss: 5.3938 - val_mae: 5.3938\n",
      "Epoch 35/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 6.2623 - mae: 6.2623 - val_loss: 4.3863 - val_mae: 4.3863\n",
      "Epoch 36/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 6.2640 - mae: 6.2640 - val_loss: 4.7917 - val_mae: 4.7917\n",
      "Epoch 37/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 8.2800 - mae: 8.2800 - val_loss: 6.2911 - val_mae: 6.2911\n",
      "Epoch 38/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 9.5134 - mae: 9.5134 - val_loss: 7.2484 - val_mae: 7.2484\n",
      "Epoch 39/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 9.9571 - mae: 9.9571 - val_loss: 7.3794 - val_mae: 7.3794\n",
      "Epoch 40/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 9.8100 - mae: 9.8100 - val_loss: 6.2761 - val_mae: 6.2761\n",
      "Epoch 41/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 8.8748 - mae: 8.8748 - val_loss: 4.5479 - val_mae: 4.5479\n",
      "Epoch 42/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 7.3224 - mae: 7.3224 - val_loss: 5.1389 - val_mae: 5.1389\n",
      "Epoch 43/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.0906 - mae: 6.0906 - val_loss: 6.7471 - val_mae: 6.7471\n",
      "Epoch 44/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.8497 - mae: 6.8497 - val_loss: 7.7752 - val_mae: 7.7752\n",
      "Epoch 45/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 8.1393 - mae: 8.1393 - val_loss: 7.4250 - val_mae: 7.4250\n",
      "Epoch 46/500\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 7.6259 - mae: 7.6259 - val_loss: 5.7226 - val_mae: 5.7226\n",
      "Epoch 47/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 6.1112 - mae: 6.1112 - val_loss: 4.5903 - val_mae: 4.5903\n",
      "Epoch 48/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 5.9804 - mae: 5.9804 - val_loss: 4.4446 - val_mae: 4.4446\n",
      "Epoch 49/500\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 7.1639 - mae: 7.1639 - val_loss: 4.4518 - val_mae: 4.4518\n",
      "Epoch 50/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 7.3682 - mae: 7.3682 - val_loss: 4.5323 - val_mae: 4.5323\n",
      "Epoch 51/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 4.7141 - val_mae: 4.7141\n",
      "Epoch 52/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.9849 - mae: 5.9849 - val_loss: 5.1159 - val_mae: 5.1159\n",
      "Epoch 53/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.1466 - mae: 6.1466 - val_loss: 5.4232 - val_mae: 5.4232\n",
      "Epoch 54/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.3792 - mae: 6.3792 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 55/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.3134 - mae: 6.3134 - val_loss: 5.2228 - val_mae: 5.2228\n",
      "Epoch 56/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.0365 - mae: 6.0365 - val_loss: 4.9721 - val_mae: 4.9721\n",
      "Epoch 57/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5.8925 - mae: 5.8925 - val_loss: 5.1421 - val_mae: 5.1421\n",
      "Epoch 58/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 5.8394 - mae: 5.8394 - val_loss: 5.3208 - val_mae: 5.3208\n",
      "Epoch 59/500\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 5.8355 - mae: 5.8355 - val_loss: 5.4494 - val_mae: 5.4494\n",
      "Epoch 60/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.2547 - mae: 6.2547 - val_loss: 5.5115 - val_mae: 5.5115\n",
      "Epoch 61/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.5758 - mae: 6.5758 - val_loss: 5.0305 - val_mae: 5.0305\n",
      "Epoch 62/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 6.0662 - mae: 6.0662 - val_loss: 4.8287 - val_mae: 4.8287\n",
      "Epoch 63/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 6.9408 - mae: 6.9408 - val_loss: 4.7979 - val_mae: 4.7979\n",
      "Epoch 64/500\n",
      "2/2 [==============================] - 0s 54ms/step - loss: 8.3557 - mae: 8.3557 - val_loss: 4.8949 - val_mae: 4.8949\n",
      "Epoch 65/500\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 8.1828 - mae: 8.1828 - val_loss: 5.0451 - val_mae: 5.0451\n",
      "Epoch 66/500\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 7.0640 - mae: 7.0640 - val_loss: 4.9385 - val_mae: 4.9385\n",
      "Epoch 67/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 6.1923 - mae: 6.1923 - val_loss: 5.5874 - val_mae: 5.5874\n",
      "Epoch 68/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 6.1034 - mae: 6.1034 - val_loss: 6.9171 - val_mae: 6.9171\n",
      "Epoch 69/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.9212 - mae: 6.9212 - val_loss: 7.2269 - val_mae: 7.2269\n",
      "Epoch 70/500\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 7.8923 - mae: 7.8923 - val_loss: 6.0135 - val_mae: 6.0135\n",
      "Epoch 71/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 7.6747 - mae: 7.6747 - val_loss: 4.5512 - val_mae: 4.5512\n",
      "Epoch 72/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 7.0100 - mae: 7.0100 - val_loss: 4.1061 - val_mae: 4.1061\n",
      "Epoch 73/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 7.1203 - mae: 7.1203 - val_loss: 3.9312 - val_mae: 3.9312\n",
      "Epoch 74/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 6.9148 - mae: 6.9148 - val_loss: 4.3364 - val_mae: 4.3364\n",
      "Epoch 75/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.4613 - mae: 6.4613 - val_loss: 4.8124 - val_mae: 4.8124\n",
      "Epoch 76/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5.8529 - mae: 5.8529 - val_loss: 5.4392 - val_mae: 5.4392\n",
      "Epoch 77/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 6.2120 - mae: 6.2120 - val_loss: 5.8063 - val_mae: 5.8063\n",
      "Epoch 78/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 7.1301 - mae: 7.1301 - val_loss: 5.8724 - val_mae: 5.8724\n",
      "Epoch 79/500\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 6.9493 - mae: 6.9493 - val_loss: 5.5437 - val_mae: 5.5437\n",
      "Epoch 80/500\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 5.8889 - mae: 5.8889 - val_loss: 5.0598 - val_mae: 5.0598\n",
      "Epoch 81/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.1660 - mae: 6.1660 - val_loss: 5.4264 - val_mae: 5.4264\n",
      "Epoch 82/500\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 7.5037 - mae: 7.5037 - val_loss: 6.0319 - val_mae: 6.0319\n",
      "Epoch 83/500\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 8.1850 - mae: 8.1850 - val_loss: 5.4039 - val_mae: 5.4039\n",
      "Epoch 84/500\n",
      "2/2 [==============================] - 0s 89ms/step - loss: 7.7611 - mae: 7.7611 - val_loss: 4.3062 - val_mae: 4.3062\n",
      "Epoch 85/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 6.3601 - mae: 6.3601 - val_loss: 4.9511 - val_mae: 4.9511\n",
      "Epoch 86/500\n",
      "2/2 [==============================] - 0s 75ms/step - loss: 5.6208 - mae: 5.6208 - val_loss: 5.5269 - val_mae: 5.5269\n",
      "Epoch 87/500\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 6.6550 - mae: 6.6550 - val_loss: 5.7836 - val_mae: 5.7836\n",
      "Epoch 88/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 7.3101 - mae: 7.3101 - val_loss: 5.6079 - val_mae: 5.6079\n",
      "Epoch 89/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.7560 - mae: 6.7560 - val_loss: 4.9515 - val_mae: 4.9515\n",
      "Epoch 90/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.7513 - mae: 5.7513 - val_loss: 4.0867 - val_mae: 4.0867\n",
      "Epoch 91/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.7476 - mae: 6.7476 - val_loss: 4.5734 - val_mae: 4.5734\n",
      "Epoch 92/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 7.6530 - mae: 7.6530 - val_loss: 4.6551 - val_mae: 4.6551\n",
      "Epoch 93/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 7.3583 - mae: 7.3583 - val_loss: 4.4336 - val_mae: 4.4336\n",
      "Epoch 94/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.6141 - mae: 6.6141 - val_loss: 5.2215 - val_mae: 5.2215\n",
      "Epoch 95/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.7725 - mae: 5.7725 - val_loss: 5.5899 - val_mae: 5.5899\n",
      "Epoch 96/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.9679 - mae: 5.9679 - val_loss: 5.7327 - val_mae: 5.7327\n",
      "Epoch 97/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.5845 - mae: 6.5845 - val_loss: 5.5771 - val_mae: 5.5771\n",
      "Epoch 98/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.3526 - mae: 6.3526 - val_loss: 4.9853 - val_mae: 4.9853\n",
      "Epoch 99/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.8332 - mae: 5.8332 - val_loss: 4.3508 - val_mae: 4.3508\n",
      "Epoch 100/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.0349 - mae: 6.0349 - val_loss: 4.2080 - val_mae: 4.2080\n",
      "Epoch 101/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 6.3263 - mae: 6.3263 - val_loss: 4.3384 - val_mae: 4.3384\n",
      "Epoch 102/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6.2169 - mae: 6.2169 - val_loss: 4.4818 - val_mae: 4.4818\n",
      "Epoch 103/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.9955 - mae: 5.9955 - val_loss: 4.4775 - val_mae: 4.4775\n",
      "Epoch 104/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.9257 - mae: 5.9257 - val_loss: 4.4888 - val_mae: 4.4888\n",
      "Epoch 105/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.8485 - mae: 5.8485 - val_loss: 4.6769 - val_mae: 4.6769\n",
      "Epoch 106/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.7487 - mae: 5.7487 - val_loss: 4.7907 - val_mae: 4.7907\n",
      "Epoch 107/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.6981 - mae: 5.6981 - val_loss: 5.0011 - val_mae: 5.0011\n",
      "Epoch 108/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.7950 - mae: 5.7950 - val_loss: 5.3597 - val_mae: 5.3597\n",
      "Epoch 109/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.9402 - mae: 5.9402 - val_loss: 5.6088 - val_mae: 5.6088\n",
      "Epoch 110/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.9932 - mae: 5.9932 - val_loss: 6.1575 - val_mae: 6.1575\n",
      "Epoch 111/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.2910 - mae: 6.2910 - val_loss: 6.0590 - val_mae: 6.0590\n",
      "Epoch 112/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.0013 - mae: 6.0013 - val_loss: 4.7769 - val_mae: 4.7769\n",
      "Epoch 113/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.7004 - mae: 5.7004 - val_loss: 4.8101 - val_mae: 4.8101\n",
      "Epoch 114/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.6242 - mae: 6.6242 - val_loss: 5.2456 - val_mae: 5.2456\n",
      "Epoch 115/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6.9602 - mae: 6.9602 - val_loss: 5.0825 - val_mae: 5.0825\n",
      "Epoch 116/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.5902 - mae: 6.5902 - val_loss: 4.6376 - val_mae: 4.6376\n",
      "Epoch 117/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.8482 - mae: 5.8482 - val_loss: 4.4863 - val_mae: 4.4863\n",
      "Epoch 118/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.8608 - mae: 5.8608 - val_loss: 4.7773 - val_mae: 4.7773\n",
      "Epoch 119/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.6886 - mae: 6.6886 - val_loss: 6.1694 - val_mae: 6.1694\n",
      "Epoch 120/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 8.0984 - mae: 8.0984 - val_loss: 6.0983 - val_mae: 6.0983\n",
      "Epoch 121/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 7.9884 - mae: 7.9884 - val_loss: 5.2118 - val_mae: 5.2118\n",
      "Epoch 122/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.3367 - mae: 6.3367 - val_loss: 4.7342 - val_mae: 4.7342\n",
      "Epoch 123/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.5945 - mae: 5.5945 - val_loss: 4.7454 - val_mae: 4.7454\n",
      "Epoch 124/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.8888 - mae: 5.8888 - val_loss: 5.2575 - val_mae: 5.2575\n",
      "Epoch 125/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 6.4990 - mae: 6.4990 - val_loss: 5.1537 - val_mae: 5.1537\n",
      "Epoch 126/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.3675 - mae: 6.3675 - val_loss: 5.3636 - val_mae: 5.3636\n",
      "Epoch 127/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.9891 - mae: 5.9891 - val_loss: 5.3894 - val_mae: 5.3894\n",
      "Epoch 128/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.6191 - mae: 5.6191 - val_loss: 5.4079 - val_mae: 5.4079\n",
      "Epoch 129/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.6887 - mae: 5.6887 - val_loss: 5.2008 - val_mae: 5.2008\n",
      "Epoch 130/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.6356 - mae: 5.6356 - val_loss: 4.7794 - val_mae: 4.7794\n",
      "Epoch 131/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.5028 - mae: 5.5028 - val_loss: 4.7454 - val_mae: 4.7454\n",
      "Epoch 132/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.8228 - mae: 5.8228 - val_loss: 5.0253 - val_mae: 5.0253\n",
      "Epoch 133/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 6.0750 - mae: 6.0750 - val_loss: 4.9323 - val_mae: 4.9323\n",
      "Epoch 134/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.8491 - mae: 5.8491 - val_loss: 4.9188 - val_mae: 4.9188\n",
      "Epoch 135/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.5581 - mae: 5.5581 - val_loss: 5.0682 - val_mae: 5.0682\n",
      "Epoch 136/500\n",
      "2/2 [==============================] - 0s 53ms/step - loss: 5.4355 - mae: 5.4355 - val_loss: 5.4332 - val_mae: 5.4332\n",
      "Epoch 137/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.5859 - mae: 5.5859 - val_loss: 6.4631 - val_mae: 6.4631\n",
      "Epoch 138/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.3825 - mae: 6.3825 - val_loss: 6.7289 - val_mae: 6.7289\n",
      "Epoch 139/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 6.5644 - mae: 6.5644 - val_loss: 5.4823 - val_mae: 5.4823\n",
      "Epoch 140/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.6900 - mae: 5.6900 - val_loss: 4.7781 - val_mae: 4.7781\n",
      "Epoch 141/500\n",
      "2/2 [==============================] - 0s 47ms/step - loss: 5.5114 - mae: 5.5114 - val_loss: 4.5641 - val_mae: 4.5641\n",
      "Epoch 142/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.6631 - mae: 5.6631 - val_loss: 4.5830 - val_mae: 4.5830\n",
      "Epoch 143/500\n",
      "2/2 [==============================] - 0s 55ms/step - loss: 5.6079 - mae: 5.6079 - val_loss: 4.5806 - val_mae: 4.5806\n",
      "Epoch 144/500\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 5.4848 - mae: 5.4848 - val_loss: 4.9293 - val_mae: 4.9293\n",
      "Epoch 145/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 5.5594 - mae: 5.5594 - val_loss: 5.4932 - val_mae: 5.4932\n",
      "Epoch 146/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 5.6873 - mae: 5.6873 - val_loss: 5.4637 - val_mae: 5.4637\n",
      "Epoch 147/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 5.5486 - mae: 5.5486 - val_loss: 5.3886 - val_mae: 5.3886\n",
      "Epoch 148/500\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 5.7020 - mae: 5.7020 - val_loss: 5.2289 - val_mae: 5.2289\n",
      "Epoch 149/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.0614 - mae: 6.0614 - val_loss: 4.9911 - val_mae: 4.9911\n",
      "Epoch 150/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.8281 - mae: 5.8281 - val_loss: 4.7620 - val_mae: 4.7620\n",
      "Epoch 151/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.7575 - mae: 5.7575 - val_loss: 4.3457 - val_mae: 4.3457\n",
      "Epoch 152/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.6110 - mae: 5.6110 - val_loss: 4.2382 - val_mae: 4.2382\n",
      "Epoch 153/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.0098 - mae: 6.0098 - val_loss: 4.7797 - val_mae: 4.7797\n",
      "Epoch 154/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.6640 - mae: 6.6640 - val_loss: 4.9660 - val_mae: 4.9660\n",
      "Epoch 155/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.6965 - mae: 6.6965 - val_loss: 4.3547 - val_mae: 4.3547\n",
      "Epoch 156/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.7617 - mae: 5.7617 - val_loss: 5.0999 - val_mae: 5.0999\n",
      "Epoch 157/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 5.4454 - mae: 5.4454 - val_loss: 5.6231 - val_mae: 5.6231\n",
      "Epoch 158/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.7893 - mae: 5.7893 - val_loss: 6.1812 - val_mae: 6.1812\n",
      "Epoch 159/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.0221 - mae: 6.0221 - val_loss: 6.1495 - val_mae: 6.1495\n",
      "Epoch 160/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.4495 - mae: 6.4495 - val_loss: 5.3623 - val_mae: 5.3623\n",
      "Epoch 161/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.5389 - mae: 6.5389 - val_loss: 4.7460 - val_mae: 4.7460\n",
      "Epoch 162/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.3311 - mae: 6.3311 - val_loss: 4.1083 - val_mae: 4.1083\n",
      "Epoch 163/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.1623 - mae: 6.1623 - val_loss: 4.5284 - val_mae: 4.5284\n",
      "Epoch 164/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.9229 - mae: 5.9229 - val_loss: 4.7845 - val_mae: 4.7845\n",
      "Epoch 165/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.5786 - mae: 5.5786 - val_loss: 5.0869 - val_mae: 5.0869\n",
      "Epoch 166/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.5674 - mae: 5.5674 - val_loss: 5.2983 - val_mae: 5.2983\n",
      "Epoch 167/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.4466 - mae: 5.4466 - val_loss: 5.3921 - val_mae: 5.3921\n",
      "Epoch 168/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.4583 - mae: 5.4583 - val_loss: 5.5655 - val_mae: 5.5655\n",
      "Epoch 169/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.5870 - mae: 5.5870 - val_loss: 5.4320 - val_mae: 5.4320\n",
      "Epoch 170/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.5012 - mae: 5.5012 - val_loss: 5.1958 - val_mae: 5.1958\n",
      "Epoch 171/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.4557 - mae: 5.4557 - val_loss: 5.1004 - val_mae: 5.1004\n",
      "Epoch 172/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.3545 - mae: 5.3545 - val_loss: 5.2119 - val_mae: 5.2119\n",
      "Epoch 173/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.3192 - mae: 5.3192 - val_loss: 5.2746 - val_mae: 5.2746\n",
      "Epoch 174/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 5.4273 - mae: 5.4273 - val_loss: 5.3599 - val_mae: 5.3599\n",
      "Epoch 175/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.5423 - mae: 5.5423 - val_loss: 4.9706 - val_mae: 4.9706\n",
      "Epoch 176/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.5032 - mae: 5.5032 - val_loss: 4.5874 - val_mae: 4.5874\n",
      "Epoch 177/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 5.4014 - mae: 5.4014 - val_loss: 4.2149 - val_mae: 4.2149\n",
      "Epoch 178/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.6099 - mae: 5.6099 - val_loss: 4.7751 - val_mae: 4.7751\n",
      "Epoch 179/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.3025 - mae: 6.3025 - val_loss: 5.2774 - val_mae: 5.2774\n",
      "Epoch 180/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.7939 - mae: 6.7939 - val_loss: 5.1950 - val_mae: 5.1950\n",
      "Epoch 181/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.6421 - mae: 6.6421 - val_loss: 4.8285 - val_mae: 4.8285\n",
      "Epoch 182/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.6289 - mae: 5.6289 - val_loss: 4.7733 - val_mae: 4.7733\n",
      "Epoch 183/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.3036 - mae: 5.3036 - val_loss: 5.0216 - val_mae: 5.0216\n",
      "Epoch 184/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 5.2232 - mae: 5.2232 - val_loss: 5.0829 - val_mae: 5.0829\n",
      "Epoch 185/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.1326 - mae: 5.1326 - val_loss: 4.8706 - val_mae: 4.8706\n",
      "Epoch 186/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.1592 - mae: 5.1592 - val_loss: 4.6735 - val_mae: 4.6735\n",
      "Epoch 187/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.5578 - mae: 5.5578 - val_loss: 4.6983 - val_mae: 4.6983\n",
      "Epoch 188/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.8506 - mae: 5.8506 - val_loss: 4.6367 - val_mae: 4.6367\n",
      "Epoch 189/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.5657 - mae: 5.5657 - val_loss: 4.5272 - val_mae: 4.5272\n",
      "Epoch 190/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.0929 - mae: 5.0929 - val_loss: 4.7475 - val_mae: 4.7475\n",
      "Epoch 191/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.0624 - mae: 5.0624 - val_loss: 4.9389 - val_mae: 4.9389\n",
      "Epoch 192/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.3534 - mae: 5.3534 - val_loss: 5.1139 - val_mae: 5.1139\n",
      "Epoch 193/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.6724 - mae: 5.6724 - val_loss: 5.1587 - val_mae: 5.1587\n",
      "Epoch 194/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.1584 - mae: 5.1584 - val_loss: 5.2776 - val_mae: 5.2776\n",
      "Epoch 195/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.0915 - mae: 5.0915 - val_loss: 5.7029 - val_mae: 5.7029\n",
      "Epoch 196/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.4517 - mae: 5.4517 - val_loss: 5.9039 - val_mae: 5.9039\n",
      "Epoch 197/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.6577 - mae: 5.6577 - val_loss: 5.6598 - val_mae: 5.6598\n",
      "Epoch 198/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 5.4558 - mae: 5.4558 - val_loss: 5.1190 - val_mae: 5.1190\n",
      "Epoch 199/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.0697 - mae: 5.0697 - val_loss: 4.7519 - val_mae: 4.7519\n",
      "Epoch 200/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.1324 - mae: 5.1324 - val_loss: 4.4882 - val_mae: 4.4882\n",
      "Epoch 201/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.1650 - mae: 5.1650 - val_loss: 5.1366 - val_mae: 5.1366\n",
      "Epoch 202/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.4162 - mae: 5.4162 - val_loss: 4.8390 - val_mae: 4.8390\n",
      "Epoch 203/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.0338 - mae: 5.0338 - val_loss: 4.7516 - val_mae: 4.7516\n",
      "Epoch 204/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4.7603 - mae: 4.7603 - val_loss: 5.0126 - val_mae: 5.0126\n",
      "Epoch 205/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.9293 - mae: 4.9293 - val_loss: 5.2260 - val_mae: 5.2260\n",
      "Epoch 206/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.9451 - mae: 4.9451 - val_loss: 5.6857 - val_mae: 5.6857\n",
      "Epoch 207/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.4415 - mae: 5.4415 - val_loss: 5.7189 - val_mae: 5.7189\n",
      "Epoch 208/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 5.5450 - mae: 5.5450 - val_loss: 5.0454 - val_mae: 5.0454\n",
      "Epoch 209/500\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 4.7588 - mae: 4.7588 - val_loss: 4.7345 - val_mae: 4.7345\n",
      "Epoch 210/500\n",
      "2/2 [==============================] - 0s 42ms/step - loss: 5.3047 - mae: 5.3047 - val_loss: 5.7647 - val_mae: 5.7647\n",
      "Epoch 211/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 6.7365 - mae: 6.7365 - val_loss: 5.8253 - val_mae: 5.8253\n",
      "Epoch 212/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6.6072 - mae: 6.6072 - val_loss: 4.8698 - val_mae: 4.8698\n",
      "Epoch 213/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.9828 - mae: 4.9828 - val_loss: 5.2599 - val_mae: 5.2599\n",
      "Epoch 214/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.8691 - mae: 4.8691 - val_loss: 6.0102 - val_mae: 6.0102\n",
      "Epoch 215/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.8286 - mae: 5.8286 - val_loss: 7.0387 - val_mae: 7.0387\n",
      "Epoch 216/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.8423 - mae: 6.8423 - val_loss: 6.9228 - val_mae: 6.9228\n",
      "Epoch 217/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.6418 - mae: 6.6418 - val_loss: 5.5711 - val_mae: 5.5711\n",
      "Epoch 218/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.3139 - mae: 5.3139 - val_loss: 4.8427 - val_mae: 4.8427\n",
      "Epoch 219/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.1644 - mae: 5.1644 - val_loss: 5.3213 - val_mae: 5.3213\n",
      "Epoch 220/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.3412 - mae: 6.3412 - val_loss: 4.9457 - val_mae: 4.9457\n",
      "Epoch 221/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.8544 - mae: 5.8544 - val_loss: 5.1177 - val_mae: 5.1177\n",
      "Epoch 222/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.3572 - mae: 5.3572 - val_loss: 5.1343 - val_mae: 5.1343\n",
      "Epoch 223/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.3349 - mae: 5.3349 - val_loss: 4.8741 - val_mae: 4.8741\n",
      "Epoch 224/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8048 - mae: 4.8048 - val_loss: 4.9733 - val_mae: 4.9733\n",
      "Epoch 225/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.8847 - mae: 4.8847 - val_loss: 5.1132 - val_mae: 5.1132\n",
      "Epoch 226/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.9209 - mae: 4.9209 - val_loss: 5.3855 - val_mae: 5.3855\n",
      "Epoch 227/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.0476 - mae: 5.0476 - val_loss: 5.3925 - val_mae: 5.3925\n",
      "Epoch 228/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.5517 - mae: 5.5517 - val_loss: 5.2151 - val_mae: 5.2151\n",
      "Epoch 229/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.4966 - mae: 5.4966 - val_loss: 5.4659 - val_mae: 5.4659\n",
      "Epoch 230/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.8325 - mae: 5.8325 - val_loss: 5.2791 - val_mae: 5.2791\n",
      "Epoch 231/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.7394 - mae: 5.7394 - val_loss: 5.2515 - val_mae: 5.2515\n",
      "Epoch 232/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 5.0734 - mae: 5.0734 - val_loss: 5.0409 - val_mae: 5.0409\n",
      "Epoch 233/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.9026 - mae: 4.9026 - val_loss: 5.1197 - val_mae: 5.1197\n",
      "Epoch 234/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5.0491 - mae: 5.0491 - val_loss: 5.1875 - val_mae: 5.1875\n",
      "Epoch 235/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.3680 - mae: 5.3680 - val_loss: 5.2424 - val_mae: 5.2424\n",
      "Epoch 236/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.2890 - mae: 5.2890 - val_loss: 5.3547 - val_mae: 5.3547\n",
      "Epoch 237/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.1157 - mae: 5.1157 - val_loss: 5.6666 - val_mae: 5.6666\n",
      "Epoch 238/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.2700 - mae: 5.2700 - val_loss: 5.8092 - val_mae: 5.8092\n",
      "Epoch 239/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.2197 - mae: 6.2197 - val_loss: 5.7041 - val_mae: 5.7041\n",
      "Epoch 240/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.8385 - mae: 6.8385 - val_loss: 5.9503 - val_mae: 5.9503\n",
      "Epoch 241/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 6.7999 - mae: 6.7999 - val_loss: 5.4644 - val_mae: 5.4644\n",
      "Epoch 242/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.6361 - mae: 5.6361 - val_loss: 4.7722 - val_mae: 4.7722\n",
      "Epoch 243/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8882 - mae: 4.8882 - val_loss: 5.0134 - val_mae: 5.0134\n",
      "Epoch 244/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.7994 - mae: 4.7994 - val_loss: 5.2789 - val_mae: 5.2789\n",
      "Epoch 245/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.0344 - mae: 5.0344 - val_loss: 5.3262 - val_mae: 5.3262\n",
      "Epoch 246/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.1718 - mae: 5.1718 - val_loss: 5.1157 - val_mae: 5.1157\n",
      "Epoch 247/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8584 - mae: 4.8584 - val_loss: 4.9449 - val_mae: 4.9449\n",
      "Epoch 248/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.8317 - mae: 4.8317 - val_loss: 4.9611 - val_mae: 4.9611\n",
      "Epoch 249/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.9682 - mae: 4.9682 - val_loss: 4.9506 - val_mae: 4.9506\n",
      "Epoch 250/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5.0590 - mae: 5.0590 - val_loss: 4.8152 - val_mae: 4.8152\n",
      "Epoch 251/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.9097 - mae: 4.9097 - val_loss: 4.8422 - val_mae: 4.8422\n",
      "Epoch 252/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.8167 - mae: 4.8167 - val_loss: 5.0980 - val_mae: 5.0980\n",
      "Epoch 253/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.9266 - mae: 4.9266 - val_loss: 5.2212 - val_mae: 5.2212\n",
      "Epoch 254/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.9420 - mae: 4.9420 - val_loss: 5.3253 - val_mae: 5.3253\n",
      "Epoch 255/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.9178 - mae: 4.9178 - val_loss: 5.3460 - val_mae: 5.3460\n",
      "Epoch 256/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.9308 - mae: 4.9308 - val_loss: 5.3701 - val_mae: 5.3701\n",
      "Epoch 257/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.0183 - mae: 5.0183 - val_loss: 5.4076 - val_mae: 5.4076\n",
      "Epoch 258/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.2116 - mae: 5.2116 - val_loss: 5.4446 - val_mae: 5.4446\n",
      "Epoch 259/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.4010 - mae: 5.4010 - val_loss: 5.3969 - val_mae: 5.3969\n",
      "Epoch 260/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.2239 - mae: 5.2239 - val_loss: 5.2757 - val_mae: 5.2757\n",
      "Epoch 261/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.7841 - mae: 4.7841 - val_loss: 5.2406 - val_mae: 5.2406\n",
      "Epoch 262/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.7575 - mae: 4.7575 - val_loss: 5.2307 - val_mae: 5.2307\n",
      "Epoch 263/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.8471 - mae: 4.8471 - val_loss: 5.0987 - val_mae: 5.0987\n",
      "Epoch 264/500\n",
      "2/2 [==============================] - 0s 22ms/step - loss: 4.6582 - mae: 4.6582 - val_loss: 5.0138 - val_mae: 5.0138\n",
      "Epoch 265/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.6613 - mae: 4.6613 - val_loss: 4.9644 - val_mae: 4.9644\n",
      "Epoch 266/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.1041 - mae: 5.1041 - val_loss: 5.0118 - val_mae: 5.0118\n",
      "Epoch 267/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4.8491 - mae: 4.8491 - val_loss: 5.1209 - val_mae: 5.1209\n",
      "Epoch 268/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.5772 - mae: 4.5772 - val_loss: 5.4096 - val_mae: 5.4096\n",
      "Epoch 269/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.1478 - mae: 5.1478 - val_loss: 5.6801 - val_mae: 5.6801\n",
      "Epoch 270/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.7006 - mae: 5.7006 - val_loss: 5.6286 - val_mae: 5.6286\n",
      "Epoch 271/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.5463 - mae: 5.5463 - val_loss: 5.3285 - val_mae: 5.3285\n",
      "Epoch 272/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.6866 - mae: 4.6866 - val_loss: 5.2305 - val_mae: 5.2305\n",
      "Epoch 273/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.6567 - mae: 4.6567 - val_loss: 5.3927 - val_mae: 5.3927\n",
      "Epoch 274/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.4457 - mae: 5.4457 - val_loss: 5.6004 - val_mae: 5.6004\n",
      "Epoch 275/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.5812 - mae: 5.5812 - val_loss: 5.2701 - val_mae: 5.2701\n",
      "Epoch 276/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.7774 - mae: 4.7774 - val_loss: 5.3906 - val_mae: 5.3906\n",
      "Epoch 277/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.6591 - mae: 4.6591 - val_loss: 5.4071 - val_mae: 5.4071\n",
      "Epoch 278/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.1144 - mae: 5.1144 - val_loss: 5.2968 - val_mae: 5.2968\n",
      "Epoch 279/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.8224 - mae: 4.8224 - val_loss: 5.1105 - val_mae: 5.1105\n",
      "Epoch 280/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 5.3617 - mae: 5.3617 - val_loss: 6.4024 - val_mae: 6.4024\n",
      "Epoch 281/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.5524 - mae: 6.5524 - val_loss: 6.7793 - val_mae: 6.7793\n",
      "Epoch 282/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 6.7911 - mae: 6.7911 - val_loss: 5.7109 - val_mae: 5.7109\n",
      "Epoch 283/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.6414 - mae: 5.6414 - val_loss: 5.3099 - val_mae: 5.3099\n",
      "Epoch 284/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.7155 - mae: 4.7155 - val_loss: 5.3687 - val_mae: 5.3687\n",
      "Epoch 285/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.5177 - mae: 4.5177 - val_loss: 5.2909 - val_mae: 5.2909\n",
      "Epoch 286/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.7761 - mae: 4.7761 - val_loss: 5.7824 - val_mae: 5.7824\n",
      "Epoch 287/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.4365 - mae: 5.4365 - val_loss: 5.9145 - val_mae: 5.9145\n",
      "Epoch 288/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.5832 - mae: 5.5832 - val_loss: 5.2574 - val_mae: 5.2574\n",
      "Epoch 289/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.7319 - mae: 4.7319 - val_loss: 5.3037 - val_mae: 5.3037\n",
      "Epoch 290/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.7649 - mae: 4.7649 - val_loss: 5.5888 - val_mae: 5.5888\n",
      "Epoch 291/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.1006 - mae: 5.1006 - val_loss: 5.9145 - val_mae: 5.9145\n",
      "Epoch 292/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 5.1559 - mae: 5.1559 - val_loss: 6.0433 - val_mae: 6.0433\n",
      "Epoch 293/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.5529 - mae: 5.5529 - val_loss: 5.6676 - val_mae: 5.6676\n",
      "Epoch 294/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.0901 - mae: 5.0901 - val_loss: 5.3989 - val_mae: 5.3989\n",
      "Epoch 295/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.5967 - mae: 4.5967 - val_loss: 5.2717 - val_mae: 5.2717\n",
      "Epoch 296/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.0645 - mae: 5.0645 - val_loss: 5.4697 - val_mae: 5.4697\n",
      "Epoch 297/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.5000 - mae: 5.5000 - val_loss: 5.4846 - val_mae: 5.4846\n",
      "Epoch 298/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.4295 - mae: 5.4295 - val_loss: 5.3788 - val_mae: 5.3788\n",
      "Epoch 299/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.9329 - mae: 4.9329 - val_loss: 5.3481 - val_mae: 5.3481\n",
      "Epoch 300/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.7313 - mae: 4.7313 - val_loss: 5.3523 - val_mae: 5.3523\n",
      "Epoch 301/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.5930 - mae: 4.5930 - val_loss: 5.3222 - val_mae: 5.3222\n",
      "Epoch 302/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.2494 - mae: 5.2494 - val_loss: 5.3255 - val_mae: 5.3255\n",
      "Epoch 303/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.8591 - mae: 4.8591 - val_loss: 5.3684 - val_mae: 5.3684\n",
      "Epoch 304/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.4642 - mae: 4.4642 - val_loss: 5.3882 - val_mae: 5.3882\n",
      "Epoch 305/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.5766 - mae: 4.5766 - val_loss: 5.3814 - val_mae: 5.3814\n",
      "Epoch 306/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.5373 - mae: 4.5373 - val_loss: 5.4919 - val_mae: 5.4919\n",
      "Epoch 307/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.6238 - mae: 4.6238 - val_loss: 5.6082 - val_mae: 5.6082\n",
      "Epoch 308/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.4248 - mae: 5.4248 - val_loss: 5.9869 - val_mae: 5.9869\n",
      "Epoch 309/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.7816 - mae: 5.7816 - val_loss: 5.6834 - val_mae: 5.6834\n",
      "Epoch 310/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.3971 - mae: 5.3971 - val_loss: 5.2730 - val_mae: 5.2730\n",
      "Epoch 311/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.4060 - mae: 4.4060 - val_loss: 5.4989 - val_mae: 5.4989\n",
      "Epoch 312/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 6.2158 - mae: 6.2158 - val_loss: 6.1042 - val_mae: 6.1042\n",
      "Epoch 313/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 6.8697 - mae: 6.8697 - val_loss: 5.6377 - val_mae: 5.6377\n",
      "Epoch 314/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.4071 - mae: 5.4071 - val_loss: 5.2429 - val_mae: 5.2429\n",
      "Epoch 315/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.7695 - mae: 4.7695 - val_loss: 5.2191 - val_mae: 5.2191\n",
      "Epoch 316/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.6808 - mae: 4.6808 - val_loss: 5.4864 - val_mae: 5.4864\n",
      "Epoch 317/500\n",
      "2/2 [==============================] - 0s 50ms/step - loss: 4.6150 - mae: 4.6150 - val_loss: 5.5310 - val_mae: 5.5310\n",
      "Epoch 318/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.6100 - mae: 4.6100 - val_loss: 5.4827 - val_mae: 5.4827\n",
      "Epoch 319/500\n",
      "2/2 [==============================] - 0s 52ms/step - loss: 4.5350 - mae: 4.5350 - val_loss: 5.3554 - val_mae: 5.3554\n",
      "Epoch 320/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.6147 - mae: 4.6147 - val_loss: 5.2500 - val_mae: 5.2500\n",
      "Epoch 321/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.5352 - mae: 4.5352 - val_loss: 5.2916 - val_mae: 5.2916\n",
      "Epoch 322/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.5556 - mae: 4.5556 - val_loss: 5.3221 - val_mae: 5.3221\n",
      "Epoch 323/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.5576 - mae: 4.5576 - val_loss: 5.5250 - val_mae: 5.5250\n",
      "Epoch 324/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.7385 - mae: 4.7385 - val_loss: 5.8407 - val_mae: 5.8407\n",
      "Epoch 325/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 5.4214 - mae: 5.4214 - val_loss: 6.2461 - val_mae: 6.2461\n",
      "Epoch 326/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.9501 - mae: 5.9501 - val_loss: 6.2208 - val_mae: 6.2208\n",
      "Epoch 327/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.2977 - mae: 5.2977 - val_loss: 6.0069 - val_mae: 6.0069\n",
      "Epoch 328/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 5.2887 - mae: 5.2887 - val_loss: 6.6937 - val_mae: 6.6937\n",
      "Epoch 329/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6.4777 - mae: 6.4777 - val_loss: 6.8937 - val_mae: 6.8937\n",
      "Epoch 330/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.6940 - mae: 6.6940 - val_loss: 5.5374 - val_mae: 5.5374\n",
      "Epoch 331/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.7275 - mae: 5.7275 - val_loss: 5.0550 - val_mae: 5.0550\n",
      "Epoch 332/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.5961 - mae: 4.5961 - val_loss: 5.5293 - val_mae: 5.5293\n",
      "Epoch 333/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.9808 - mae: 5.9808 - val_loss: 6.3791 - val_mae: 6.3791\n",
      "Epoch 334/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 7.2436 - mae: 7.2436 - val_loss: 6.7931 - val_mae: 6.7931\n",
      "Epoch 335/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 7.6213 - mae: 7.6213 - val_loss: 6.7635 - val_mae: 6.7635\n",
      "Epoch 336/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 7.1174 - mae: 7.1174 - val_loss: 6.4841 - val_mae: 6.4841\n",
      "Epoch 337/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.7983 - mae: 5.7983 - val_loss: 6.0242 - val_mae: 6.0242\n",
      "Epoch 338/500\n",
      "2/2 [==============================] - 0s 23ms/step - loss: 5.5909 - mae: 5.5909 - val_loss: 6.3091 - val_mae: 6.3091\n",
      "Epoch 339/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.2687 - mae: 6.2687 - val_loss: 6.7306 - val_mae: 6.7306\n",
      "Epoch 340/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 6.8257 - mae: 6.8257 - val_loss: 5.9086 - val_mae: 5.9086\n",
      "Epoch 341/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 6.1827 - mae: 6.1827 - val_loss: 5.0274 - val_mae: 5.0274\n",
      "Epoch 342/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.0295 - mae: 5.0295 - val_loss: 5.4245 - val_mae: 5.4245\n",
      "Epoch 343/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.6496 - mae: 5.6496 - val_loss: 5.3632 - val_mae: 5.3632\n",
      "Epoch 344/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.7047 - mae: 5.7047 - val_loss: 5.1909 - val_mae: 5.1909\n",
      "Epoch 345/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.1936 - mae: 5.1936 - val_loss: 5.2065 - val_mae: 5.2065\n",
      "Epoch 346/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.5182 - mae: 4.5182 - val_loss: 5.2935 - val_mae: 5.2935\n",
      "Epoch 347/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.6969 - mae: 4.6969 - val_loss: 5.4699 - val_mae: 5.4699\n",
      "Epoch 348/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.2364 - mae: 5.2364 - val_loss: 5.6187 - val_mae: 5.6187\n",
      "Epoch 349/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8626 - mae: 4.8626 - val_loss: 5.6214 - val_mae: 5.6214\n",
      "Epoch 350/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.8607 - mae: 4.8607 - val_loss: 5.5852 - val_mae: 5.5852\n",
      "Epoch 351/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4.9061 - mae: 4.9061 - val_loss: 5.6609 - val_mae: 5.6609\n",
      "Epoch 352/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.8519 - mae: 4.8519 - val_loss: 5.7940 - val_mae: 5.7940\n",
      "Epoch 353/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.9133 - mae: 4.9133 - val_loss: 5.6218 - val_mae: 5.6218\n",
      "Epoch 354/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.2787 - mae: 5.2787 - val_loss: 5.5875 - val_mae: 5.5875\n",
      "Epoch 355/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.4286 - mae: 5.4286 - val_loss: 5.4359 - val_mae: 5.4359\n",
      "Epoch 356/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.7747 - mae: 4.7747 - val_loss: 5.4257 - val_mae: 5.4257\n",
      "Epoch 357/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.5260 - mae: 4.5260 - val_loss: 5.3343 - val_mae: 5.3343\n",
      "Epoch 358/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.5877 - mae: 4.5877 - val_loss: 5.3567 - val_mae: 5.3567\n",
      "Epoch 359/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.8726 - mae: 4.8726 - val_loss: 5.3376 - val_mae: 5.3376\n",
      "Epoch 360/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.5930 - mae: 4.5930 - val_loss: 5.3079 - val_mae: 5.3079\n",
      "Epoch 361/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.3620 - mae: 4.3620 - val_loss: 5.4216 - val_mae: 5.4216\n",
      "Epoch 362/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.9317 - mae: 4.9317 - val_loss: 5.9355 - val_mae: 5.9355\n",
      "Epoch 363/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.6778 - mae: 5.6778 - val_loss: 5.8422 - val_mae: 5.8422\n",
      "Epoch 364/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.0841 - mae: 5.0841 - val_loss: 5.8153 - val_mae: 5.8153\n",
      "Epoch 365/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.8398 - mae: 4.8398 - val_loss: 5.5951 - val_mae: 5.5951\n",
      "Epoch 366/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8516 - mae: 4.8516 - val_loss: 5.3479 - val_mae: 5.3479\n",
      "Epoch 367/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.8017 - mae: 4.8017 - val_loss: 5.2569 - val_mae: 5.2569\n",
      "Epoch 368/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.7088 - mae: 4.7088 - val_loss: 5.1897 - val_mae: 5.1897\n",
      "Epoch 369/500\n",
      "2/2 [==============================] - 0s 49ms/step - loss: 4.7450 - mae: 4.7450 - val_loss: 5.2443 - val_mae: 5.2443\n",
      "Epoch 370/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.7183 - mae: 4.7183 - val_loss: 5.1734 - val_mae: 5.1734\n",
      "Epoch 371/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.5190 - mae: 4.5190 - val_loss: 5.2733 - val_mae: 5.2733\n",
      "Epoch 372/500\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.5001 - mae: 4.5001 - val_loss: 5.4026 - val_mae: 5.4026\n",
      "Epoch 373/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.4647 - mae: 4.4647 - val_loss: 5.4800 - val_mae: 5.4800\n",
      "Epoch 374/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.9119 - mae: 4.9119 - val_loss: 5.4770 - val_mae: 5.4770\n",
      "Epoch 375/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.1141 - mae: 5.1141 - val_loss: 5.2512 - val_mae: 5.2512\n",
      "Epoch 376/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.6506 - mae: 4.6506 - val_loss: 5.1562 - val_mae: 5.1562\n",
      "Epoch 377/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.6360 - mae: 4.6360 - val_loss: 5.3708 - val_mae: 5.3708\n",
      "Epoch 378/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8198 - mae: 4.8198 - val_loss: 5.2265 - val_mae: 5.2265\n",
      "Epoch 379/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.5407 - mae: 4.5407 - val_loss: 5.3589 - val_mae: 5.3589\n",
      "Epoch 380/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.4905 - mae: 4.4905 - val_loss: 5.5353 - val_mae: 5.5353\n",
      "Epoch 381/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.6912 - mae: 4.6912 - val_loss: 5.6906 - val_mae: 5.6906\n",
      "Epoch 382/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.7628 - mae: 4.7628 - val_loss: 5.6127 - val_mae: 5.6127\n",
      "Epoch 383/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.7423 - mae: 4.7423 - val_loss: 5.5533 - val_mae: 5.5533\n",
      "Epoch 384/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 4.7184 - mae: 4.7184 - val_loss: 5.5296 - val_mae: 5.5296\n",
      "Epoch 385/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.6539 - mae: 4.6539 - val_loss: 5.5977 - val_mae: 5.5977\n",
      "Epoch 386/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.6697 - mae: 4.6697 - val_loss: 5.5639 - val_mae: 5.5639\n",
      "Epoch 387/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.5720 - mae: 4.5720 - val_loss: 5.4240 - val_mae: 5.4240\n",
      "Epoch 388/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.4691 - mae: 4.4691 - val_loss: 5.3852 - val_mae: 5.3852\n",
      "Epoch 389/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.4425 - mae: 4.4425 - val_loss: 5.3793 - val_mae: 5.3793\n",
      "Epoch 390/500\n",
      "2/2 [==============================] - 0s 45ms/step - loss: 4.4627 - mae: 4.4627 - val_loss: 5.4599 - val_mae: 5.4599\n",
      "Epoch 391/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 4.4429 - mae: 4.4429 - val_loss: 5.8046 - val_mae: 5.8046\n",
      "Epoch 392/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.7042 - mae: 4.7042 - val_loss: 6.8349 - val_mae: 6.8349\n",
      "Epoch 393/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.9996 - mae: 5.9996 - val_loss: 7.4599 - val_mae: 7.4599\n",
      "Epoch 394/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 6.7418 - mae: 6.7418 - val_loss: 7.0714 - val_mae: 7.0714\n",
      "Epoch 395/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.7412 - mae: 6.7412 - val_loss: 6.0118 - val_mae: 6.0118\n",
      "Epoch 396/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.9486 - mae: 5.9486 - val_loss: 5.5052 - val_mae: 5.5052\n",
      "Epoch 397/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.4090 - mae: 5.4090 - val_loss: 5.3272 - val_mae: 5.3272\n",
      "Epoch 398/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.8027 - mae: 4.8027 - val_loss: 5.8532 - val_mae: 5.8532\n",
      "Epoch 399/500\n",
      "2/2 [==============================] - 0s 30ms/step - loss: 5.4858 - mae: 5.4858 - val_loss: 5.8508 - val_mae: 5.8508\n",
      "Epoch 400/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 6.0444 - mae: 6.0444 - val_loss: 5.5071 - val_mae: 5.5071\n",
      "Epoch 401/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 5.3004 - mae: 5.3004 - val_loss: 5.2847 - val_mae: 5.2847\n",
      "Epoch 402/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.4418 - mae: 4.4418 - val_loss: 5.6448 - val_mae: 5.6448\n",
      "Epoch 403/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.7791 - mae: 4.7791 - val_loss: 6.8036 - val_mae: 6.8036\n",
      "Epoch 404/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.8991 - mae: 5.8991 - val_loss: 6.9634 - val_mae: 6.9634\n",
      "Epoch 405/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 6.5851 - mae: 6.5851 - val_loss: 6.1633 - val_mae: 6.1633\n",
      "Epoch 406/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.3485 - mae: 5.3485 - val_loss: 5.4596 - val_mae: 5.4596\n",
      "Epoch 407/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.4613 - mae: 4.4613 - val_loss: 5.2909 - val_mae: 5.2909\n",
      "Epoch 408/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.6411 - mae: 4.6411 - val_loss: 5.7197 - val_mae: 5.7197\n",
      "Epoch 409/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.2012 - mae: 5.2012 - val_loss: 5.6885 - val_mae: 5.6885\n",
      "Epoch 410/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 5.0701 - mae: 5.0701 - val_loss: 5.2247 - val_mae: 5.2247\n",
      "Epoch 411/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4.5470 - mae: 4.5470 - val_loss: 5.5227 - val_mae: 5.5227\n",
      "Epoch 412/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.4740 - mae: 4.4740 - val_loss: 6.4692 - val_mae: 6.4692\n",
      "Epoch 413/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.4656 - mae: 5.4656 - val_loss: 7.0044 - val_mae: 7.0044\n",
      "Epoch 414/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.0826 - mae: 6.0826 - val_loss: 6.7504 - val_mae: 6.7504\n",
      "Epoch 415/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.7280 - mae: 5.7280 - val_loss: 5.8563 - val_mae: 5.8563\n",
      "Epoch 416/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.9674 - mae: 4.9674 - val_loss: 5.2276 - val_mae: 5.2276\n",
      "Epoch 417/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.4142 - mae: 4.4142 - val_loss: 5.2414 - val_mae: 5.2414\n",
      "Epoch 418/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.8288 - mae: 4.8288 - val_loss: 6.0369 - val_mae: 6.0369\n",
      "Epoch 419/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 6.4916 - mae: 6.4916 - val_loss: 6.1739 - val_mae: 6.1739\n",
      "Epoch 420/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.7089 - mae: 6.7089 - val_loss: 5.4702 - val_mae: 5.4702\n",
      "Epoch 421/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.5028 - mae: 5.5028 - val_loss: 5.2368 - val_mae: 5.2368\n",
      "Epoch 422/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.4443 - mae: 4.4443 - val_loss: 5.7724 - val_mae: 5.7724\n",
      "Epoch 423/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.8120 - mae: 4.8120 - val_loss: 6.1270 - val_mae: 6.1270\n",
      "Epoch 424/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.1013 - mae: 5.1013 - val_loss: 5.8762 - val_mae: 5.8762\n",
      "Epoch 425/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 5.0300 - mae: 5.0300 - val_loss: 5.5168 - val_mae: 5.5168\n",
      "Epoch 426/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.5873 - mae: 4.5873 - val_loss: 5.2310 - val_mae: 5.2310\n",
      "Epoch 427/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.4328 - mae: 4.4328 - val_loss: 5.0369 - val_mae: 5.0369\n",
      "Epoch 428/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.7333 - mae: 4.7333 - val_loss: 5.6512 - val_mae: 5.6512\n",
      "Epoch 429/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 5.3931 - mae: 5.3931 - val_loss: 5.6815 - val_mae: 5.6815\n",
      "Epoch 430/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.4078 - mae: 5.4078 - val_loss: 5.2270 - val_mae: 5.2270\n",
      "Epoch 431/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.8661 - mae: 4.8661 - val_loss: 5.2004 - val_mae: 5.2004\n",
      "Epoch 432/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.5204 - mae: 4.5204 - val_loss: 5.8512 - val_mae: 5.8512\n",
      "Epoch 433/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.1576 - mae: 5.1576 - val_loss: 7.0426 - val_mae: 7.0426\n",
      "Epoch 434/500\n",
      "2/2 [==============================] - 0s 46ms/step - loss: 6.4854 - mae: 6.4854 - val_loss: 7.2022 - val_mae: 7.2022\n",
      "Epoch 435/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 6.3776 - mae: 6.3776 - val_loss: 6.4415 - val_mae: 6.4415\n",
      "Epoch 436/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 5.9784 - mae: 5.9784 - val_loss: 5.5050 - val_mae: 5.5050\n",
      "Epoch 437/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.3557 - mae: 5.3557 - val_loss: 5.3231 - val_mae: 5.3231\n",
      "Epoch 438/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.6353 - mae: 4.6353 - val_loss: 5.2079 - val_mae: 5.2079\n",
      "Epoch 439/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.5939 - mae: 4.5939 - val_loss: 5.1730 - val_mae: 5.1730\n",
      "Epoch 440/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.8384 - mae: 4.8384 - val_loss: 5.2261 - val_mae: 5.2261\n",
      "Epoch 441/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.5278 - mae: 4.5278 - val_loss: 5.2770 - val_mae: 5.2770\n",
      "Epoch 442/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.3676 - mae: 4.3676 - val_loss: 5.3008 - val_mae: 5.3008\n",
      "Epoch 443/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.8911 - mae: 4.8911 - val_loss: 5.3365 - val_mae: 5.3365\n",
      "Epoch 444/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.2428 - mae: 5.2428 - val_loss: 5.3616 - val_mae: 5.3616\n",
      "Epoch 445/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.8359 - mae: 4.8359 - val_loss: 5.1712 - val_mae: 5.1712\n",
      "Epoch 446/500\n",
      "2/2 [==============================] - 0s 48ms/step - loss: 4.6148 - mae: 4.6148 - val_loss: 5.1428 - val_mae: 5.1428\n",
      "Epoch 447/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.8476 - mae: 4.8476 - val_loss: 5.1818 - val_mae: 5.1818\n",
      "Epoch 448/500\n",
      "2/2 [==============================] - 0s 44ms/step - loss: 4.5983 - mae: 4.5983 - val_loss: 5.2782 - val_mae: 5.2782\n",
      "Epoch 449/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.4129 - mae: 4.4129 - val_loss: 5.3230 - val_mae: 5.3230\n",
      "Epoch 450/500\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 4.3568 - mae: 4.3568 - val_loss: 5.2279 - val_mae: 5.2279\n",
      "Epoch 451/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.3850 - mae: 4.3850 - val_loss: 5.2568 - val_mae: 5.2568\n",
      "Epoch 452/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 5.3609 - mae: 5.3609 - val_loss: 6.0283 - val_mae: 6.0283\n",
      "Epoch 453/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.8929 - mae: 5.8929 - val_loss: 5.4963 - val_mae: 5.4963\n",
      "Epoch 454/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.2410 - mae: 5.2410 - val_loss: 5.3614 - val_mae: 5.3614\n",
      "Epoch 455/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.4634 - mae: 4.4634 - val_loss: 5.3631 - val_mae: 5.3631\n",
      "Epoch 456/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.3608 - mae: 4.3608 - val_loss: 5.3904 - val_mae: 5.3904\n",
      "Epoch 457/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.3632 - mae: 4.3632 - val_loss: 5.4238 - val_mae: 5.4238\n",
      "Epoch 458/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.7057 - mae: 4.7057 - val_loss: 5.4071 - val_mae: 5.4071\n",
      "Epoch 459/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.5603 - mae: 4.5603 - val_loss: 5.3321 - val_mae: 5.3321\n",
      "Epoch 460/500\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.4072 - mae: 4.4072 - val_loss: 5.2390 - val_mae: 5.2390\n",
      "Epoch 461/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.4666 - mae: 4.4666 - val_loss: 5.1751 - val_mae: 5.1751\n",
      "Epoch 462/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.5636 - mae: 4.5636 - val_loss: 5.1796 - val_mae: 5.1796\n",
      "Epoch 463/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.6020 - mae: 4.6020 - val_loss: 5.3335 - val_mae: 5.3335\n",
      "Epoch 464/500\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 4.7610 - mae: 4.7610 - val_loss: 5.1907 - val_mae: 5.1907\n",
      "Epoch 465/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.6535 - mae: 4.6535 - val_loss: 5.1444 - val_mae: 5.1444\n",
      "Epoch 466/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.4420 - mae: 4.4420 - val_loss: 5.3294 - val_mae: 5.3294\n",
      "Epoch 467/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.4360 - mae: 4.4360 - val_loss: 5.6210 - val_mae: 5.6210\n",
      "Epoch 468/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.8529 - mae: 4.8529 - val_loss: 6.0793 - val_mae: 6.0793\n",
      "Epoch 469/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.1453 - mae: 5.1453 - val_loss: 5.8990 - val_mae: 5.8990\n",
      "Epoch 470/500\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 4.6903 - mae: 4.6903 - val_loss: 5.3012 - val_mae: 5.3012\n",
      "Epoch 471/500\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 4.2886 - mae: 4.2886 - val_loss: 5.4313 - val_mae: 5.4313\n",
      "Epoch 472/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.0318 - mae: 5.0318 - val_loss: 6.1001 - val_mae: 6.1001\n",
      "Epoch 473/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.8332 - mae: 5.8332 - val_loss: 6.1050 - val_mae: 6.1050\n",
      "Epoch 474/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.7155 - mae: 5.7155 - val_loss: 5.6086 - val_mae: 5.6086\n",
      "Epoch 475/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.1133 - mae: 5.1133 - val_loss: 5.1681 - val_mae: 5.1681\n",
      "Epoch 476/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.4126 - mae: 4.4126 - val_loss: 5.4762 - val_mae: 5.4762\n",
      "Epoch 477/500\n",
      "2/2 [==============================] - 0s 43ms/step - loss: 4.5683 - mae: 4.5683 - val_loss: 5.8574 - val_mae: 5.8574\n",
      "Epoch 478/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.1896 - mae: 5.1896 - val_loss: 5.7528 - val_mae: 5.7528\n",
      "Epoch 479/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.7166 - mae: 4.7166 - val_loss: 5.4429 - val_mae: 5.4429\n",
      "Epoch 480/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 4.3852 - mae: 4.3852 - val_loss: 5.1448 - val_mae: 5.1448\n",
      "Epoch 481/500\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 4.6415 - mae: 4.6415 - val_loss: 5.4469 - val_mae: 5.4469\n",
      "Epoch 482/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.2274 - mae: 5.2274 - val_loss: 6.0460 - val_mae: 6.0460\n",
      "Epoch 483/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.7348 - mae: 5.7348 - val_loss: 6.1647 - val_mae: 6.1647\n",
      "Epoch 484/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.8088 - mae: 5.8088 - val_loss: 5.7744 - val_mae: 5.7744\n",
      "Epoch 485/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 5.6061 - mae: 5.6061 - val_loss: 5.2812 - val_mae: 5.2812\n",
      "Epoch 486/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.5598 - mae: 4.5598 - val_loss: 5.2702 - val_mae: 5.2702\n",
      "Epoch 487/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.5445 - mae: 4.5445 - val_loss: 5.8710 - val_mae: 5.8710\n",
      "Epoch 488/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 5.4988 - mae: 5.4988 - val_loss: 6.0331 - val_mae: 6.0331\n",
      "Epoch 489/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.2703 - mae: 5.2703 - val_loss: 6.4751 - val_mae: 6.4751\n",
      "Epoch 490/500\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.4035 - mae: 5.4035 - val_loss: 6.5483 - val_mae: 6.5483\n",
      "Epoch 491/500\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 6.0613 - mae: 6.0613 - val_loss: 6.0431 - val_mae: 6.0431\n",
      "Epoch 492/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.5040 - mae: 5.5040 - val_loss: 5.3976 - val_mae: 5.3976\n",
      "Epoch 493/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.5050 - mae: 4.5050 - val_loss: 5.1617 - val_mae: 5.1617\n",
      "Epoch 494/500\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 4.3724 - mae: 4.3724 - val_loss: 5.0966 - val_mae: 5.0966\n",
      "Epoch 495/500\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 4.5706 - mae: 4.5706 - val_loss: 5.1046 - val_mae: 5.1046\n",
      "Epoch 496/500\n",
      "2/2 [==============================] - 0s 29ms/step - loss: 4.6704 - mae: 4.6704 - val_loss: 5.1276 - val_mae: 5.1276\n",
      "Epoch 497/500\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 4.6213 - mae: 4.6213 - val_loss: 5.1527 - val_mae: 5.1527\n",
      "Epoch 498/500\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 4.4788 - mae: 4.4788 - val_loss: 5.2433 - val_mae: 5.2433\n",
      "Epoch 499/500\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 4.5153 - mae: 4.5153 - val_loss: 5.4703 - val_mae: 5.4703\n",
      "Epoch 500/500\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 4.7945 - mae: 4.7945 - val_loss: 5.9514 - val_mae: 5.9514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2497fe12140>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Create the model\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(128, activation = \"relu\", input_shape = (1, ), name = \"hidden_layer\"),\n",
    "        tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "    ], name = 'model_3'\n",
    ")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2),\n",
    "    metrics = ['mae']\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_preds_3 = model.predict(X_test)\n",
    "plot_predictions(predictions = y_preds_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=9.50535>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=120.30011>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_3 = mae(y_test, y_preds_3)\n",
    "mse_3 = mse(y_test, y_preds_3)\n",
    "mae_3,mse_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be performing really well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results of our experiments\n",
    "\n",
    "We've run a few experiments, let's compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    models  Mean Absolute Error  Mean Square Error\n",
      "0  model_1             9.505350         120.300110\n",
      "1  model_2            11.347539         138.561646\n",
      "2  model_3             9.505350         120.300110\n"
     ]
    }
   ],
   "source": [
    "# Lets compare our model's results using a pandas Dataframe\n",
    "import pandas as pd\n",
    "\n",
    "model_results = pd.DataFrame({\"models\":[\"model_1\", \"model_2\", \"model_3\"], \"Mean Absolute Error\": [mae_1.numpy(), mae_2.numpy(), mae_3.numpy()], \"Mean Square Error\": [mse_1.numpy(), mse_2.numpy(), mse_3.numpy()]})\n",
    "print(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 3 seems to be performing the best.\n",
    "\n",
    ">**Note:** One of our main goals is to minimize the time between our experiments. The more experiments we do, the more things we'll figure our which don't work and in turn get closer to figuring out what does work. Remember the machine learning practitioner's motto: \"Experiment, experiment, experiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking your experiments\n",
    "\n",
    "One really good habit in machine learning modelling is to track the results of your experiments\n",
    "\n",
    "And when doing so, it can be tedious if you're running lots of experiments.\n",
    "\n",
    "Luckily there are tools to help us!\n",
    "\n",
    "\n",
    "**Resource:** As you build more models, you'll want to look into using:\n",
    "* Tensorboard - a component of the TensorFlow library to help track modelling experiments (we'll see this one later)\n",
    "* Weights and Biases - a tool for tracking all kinds of machine learning experiments (plugs straight into TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a model (specified to your problems)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation = \"relu\", input_shape = (1, ), name = \"hidden_layer_1\"),\n",
    "    tf.keras.layers.Dense(256, activation = \"relu\", name = \"hidden_layer_2\"),\n",
    "    tf.keras.layers.Dense(64, activation = \"relu\", name = \"hidden_layer_3\"),\n",
    "    tf.keras.layers.Dense(1, activation = None, name = \"output_layer\")\n",
    "], name =\"model_best\")\n",
    "\n",
    "# 2. Compile the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mae,\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics = [\"mae\"]\n",
    ")\n",
    "\n",
    "# 3. Fit the model\n",
    "model.fit(X_train,y_train, validation_data = (X_val, y_val),epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving our models\n",
    "\n",
    "Saving our models allows us to use them outside of Google Colab (or wherever they were trained) such as in a web application or a mobile app\n",
    "\n",
    "In TensorFlow there are 2 formats:\n",
    "\n",
    "* SavedModel format: Restored using tf.keras.models.load_model; Compatible with TensorFlow Serving; Structure: a) `assets` directory b) `variables` directory c) `saved_model.pb` (Proteau by file)\n",
    "* HDF5 format: Hierarchical Data Format; Designed to store and organise large amounts of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_savedmodel_format\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save model using the SavedModel format\n",
    "model.save(\"Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_savedmodel_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in the HDF5 format\n",
    "model.save(\"Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_HDF5_format.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden_layer (Dense)        (None, 128)               256       \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load in the SavedModel format model\n",
    "\n",
    "loaded_SavedModel_format = tf.keras.models.load_model(\"Models/Neural_Network_Regression_with_Tensorflow/Best_Model/best_model_savedmodel_format\")\n",
    "loaded_SavedModel_format.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98ba7f9ca0a7d3f9faf83a09faac2df1e0ca0e1c9a5db868ca666908d95c6454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
