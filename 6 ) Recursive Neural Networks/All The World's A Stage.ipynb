{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"All The World's A Stage.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"495809ae"},"source":["# All The World's A Stage\n","\n","<a id = 'toc'></a>\n","## Table of Contents\n","### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[Act I: Exposition](#exposition)\n","1. [Importing Libraries](#import)\n","2. [Loading Data](#load)\n","### [Act II: Rising Action](#rising_action)\n","3. [Encoding](#encode)\n","4. [Creating Examples and Labels](#examples_and_labels)\n","5. [Switch Around and Batch Data](#shuffle)\n","### [Act III: Climax](#climax)\n","6. [Defining the Model](#model)\n","7. [Training the Model](#training)\n","8. [The Playwright](#playwright)\n","9. [A New Beginning](#new_beginning)\n","10. [To War and Peace](#war_and_peace)\n","### [Act IV: Falling Action](#falling_action)\n","11. [A War of Words?](#war_words)"],"id":"495809ae"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEQ9eI8LiE3J","executionInfo":{"status":"ok","timestamp":1635623290793,"user_tz":-330,"elapsed":25284,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"cbf52520-3623-4953-a805-eb37229ce833"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"GEQ9eI8LiE3J","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"991a6db1"},"source":["<a id = 'exposition'></a>\n","## [Part I: Exposition](#toc)\n","\n","> `The fool doth think he is wise, but the wise man knows himself to be a fool.`"],"id":"991a6db1"},{"cell_type":"markdown","metadata":{"id":"62f89bd6"},"source":["<a id = 'import'></a>\n","### [1. Importing Libraries](#toc)\n","\n","> `The Sumerians invented the wheel. I'd much rather just use it`"],"id":"62f89bd6"},{"cell_type":"code","metadata":{"id":"0e28cd1b","executionInfo":{"status":"ok","timestamp":1635623292574,"user_tz":-330,"elapsed":1786,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}}},"source":["import numpy as np\n","import os\n","import tensorflow as tf\n","from tensorflow.keras.layers.experimental import preprocessing\n","from tensorflow import keras\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from IPython.display import display\n","import time\n","\n","%matplotlib inline"],"id":"0e28cd1b","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d33ba53e"},"source":["<a id = 'load'></a>\n","### [2. Loading Data](#toc)\n","\n","> `The strength of a model reflects the quality of its data`"],"id":"d33ba53e"},{"cell_type":"code","metadata":{"id":"180847b0"},"source":["path_to_file = tf.keras.utils.get_file('/content/drive/MyDrive/Assignments/Week 4,5/Data/shakespeare.txt',\n","                                      'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt')\n","\n","text_shakespeare = open(path_to_file, 'rb').read().decode(encoding = 'UTF-8')"],"id":"180847b0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fe9bf27e","executionInfo":{"status":"ok","timestamp":1634558318143,"user_tz":-330,"elapsed":57,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"ccb10fda-dcad-4830-dc1f-f095a64a4b1f"},"source":["print(len(text_shakespeare))"],"id":"fe9bf27e","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4573338\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12c04e0d","executionInfo":{"status":"ok","timestamp":1634558318144,"user_tz":-330,"elapsed":47,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"410231c7-d682-477e-a117-4ed7ae78f35b"},"source":["vocab_shakespeare = sorted(set(text_shakespeare))\n","print(f'Number of unique characters: {len(vocab_shakespeare)}')\n","print(f'Characters include:-\\n{str(vocab_shakespeare)[1:-1]}')"],"id":"12c04e0d","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique characters: 67\n","Characters include:-\n","'\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n"]}]},{"cell_type":"markdown","metadata":{"id":"158edf57"},"source":["<a id = 'rising_action'></a>\n","## [Act II: Rising Action](#toc)\n","\n","> `SEBASTION: By your patience, no. My stars shine darkly over me. The malignancy of my fate might perhaps distemper yours. Therefore I shall crave of you your leave that I may bear my evils alone.`"],"id":"158edf57"},{"cell_type":"markdown","metadata":{"id":"25fd0f80"},"source":["<a id = 'encode'></a>\n","### [3. Encoding](#toc)\n","\n","> `Perhaps in another universe, in another time, the junk I've produced serves a function. Its just encoded!`"],"id":"25fd0f80"},{"cell_type":"code","metadata":{"id":"c08b5ace"},"source":["ids_from_chars_shakespeare = preprocessing.StringLookup(vocabulary = list(vocab_shakespeare), mask_token = None)\n","\n","def text_from_ids(ids, ids_from_chars):\n","    \"\"\"\n","    Reverses encoding and returns strings that produce the encoding\n","    Arguments: \n","    1) Array of ids\n","    2) Preprocessing Layer used to create the encoding\n","    Returns:\n","    1) String in plain english (or Shakespearean here!)\n","    \"\"\"\n","    chars_from_ids = preprocessing.StringLookup(vocabulary = ids_from_chars.get_vocabulary(), invert = True, mask_token = None)\n","    return tf.strings.reduce_join(chars_from_ids(ids), axis = -1)\n","\n","all_ids_shakespeare = ids_from_chars_shakespeare(tf.strings.unicode_split(text_shakespeare, 'UTF-8'))\n","\n","ids_dataset_shakespeare = tf.data.Dataset.from_tensor_slices(all_ids_shakespeare)"],"id":"c08b5ace","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"406e4eba"},"source":["A small sanity check at this stage:"],"id":"406e4eba"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1f9429dc","executionInfo":{"status":"ok","timestamp":1634558327248,"user_tz":-330,"elapsed":36,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"1c944d22-f1f3-479c-f25e-3c8bbd63ac1c"},"source":["chars_from_ids_shakespeare = preprocessing.StringLookup(vocabulary = ids_from_chars_shakespeare.get_vocabulary(),\n","                                            invert = True,\n","                                            mask_token = None)\n","\n","for ids in ids_dataset_shakespeare.take(5):\n","    print(chars_from_ids_shakespeare(ids).numpy().decode('UTF-8'), end = '')"],"id":"1f9429dc","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First"]}]},{"cell_type":"markdown","metadata":{"id":"3aafc6bf"},"source":["We'll define the length of sequences that we'll be processing as a single example in an epoch."],"id":"3aafc6bf"},{"cell_type":"code","metadata":{"id":"08785928"},"source":["seq_length = 100\n","examples_per_epoch = len(text_shakespeare)//(seq_length + 1)"],"id":"08785928","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"add4300d","executionInfo":{"status":"ok","timestamp":1634558327250,"user_tz":-330,"elapsed":32,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"f54d97f5-7ad2-443d-ce0d-9d7486bf4545"},"source":["sequences_shakespeare = ids_dataset_shakespeare.batch(seq_length + 1, drop_remainder = True)\n","for seq in sequences_shakespeare.take(1):\n","    print(chars_from_ids_shakespeare(seq).numpy(), end = '')"],"id":"add4300d","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n"," b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n"," b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n"," b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n"," b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n"," b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n"," b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n"," b'o' b'u' b' ']"]}]},{"cell_type":"markdown","metadata":{"id":"da5343aa"},"source":["<a id = 'examples_and_labels'></a>\n","### [4. Creating Examples and Labels](#toc)\n","\n","> `We all enter the future like the oarsmen - backwards. The sights of the present and past captivate us while we are blind to what the future holds in store for us`"],"id":"da5343aa"},{"cell_type":"code","metadata":{"id":"aea5873b"},"source":["def split_input_target(sequence):\n","    \"\"\"\n","    Splits input encoded strings into training examples and training labels\n","    Arguments:\n","    1) Array of input encoded data\n","    Returns:\n","    1) Training examples\n","    2) Training labels\n","    \"\"\"\n","    input_text = sequence[0:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text\n","\n","dataset_shakespeare = sequences_shakespeare.map(split_input_target)"],"id":"aea5873b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ebfe05e","executionInfo":{"status":"ok","timestamp":1634558327251,"user_tz":-330,"elapsed":29,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"e9ab8178-4e71-4f7e-aa47-3a4f57e06c9d"},"source":["for input_example, target_example in dataset_shakespeare.take(1):\n","    print(\"Input: \", text_from_ids(input_example, ids_from_chars_shakespeare).numpy())\n","    print(\"Target: \", text_from_ids(target_example, ids_from_chars_shakespeare).numpy())"],"id":"6ebfe05e","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:  b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n","Target:  b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"]}]},{"cell_type":"markdown","metadata":{"id":"19b1d53d"},"source":["<a id = 'shuffle'></a>\n","### [5. Switch Around and Batch Data](#toc)\n","\n","> `Now you see me. Now you don't.`"],"id":"19b1d53d"},{"cell_type":"code","metadata":{"id":"7538dc5b"},"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset_shakespeare = dataset_shakespeare.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)"],"id":"7538dc5b","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74d48cb9","executionInfo":{"status":"ok","timestamp":1634558328956,"user_tz":-330,"elapsed":1729,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"4644779a-c730-4812-da2a-4b7a8959fff1"},"source":["for input_example, target_example in dataset_shakespeare.take(1):\n","    print(\"Input: \", text_from_ids(input_example, ids_from_chars_shakespeare).numpy())\n","    print()\n","    print('-'*120)\n","    print()\n","    print(\"Target: \", text_from_ids(target_example, ids_from_chars_shakespeare).numpy())"],"id":"74d48cb9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:  [b\"uy this treason\\nEven with the dearest blood your bodies bear.\\n\\nKING EDWARD IV:\\nThe harder match'd, t\"\n"," b'shall not stay alone\\nTill holy church incorporate two in one.\\n\\nBENVOLIO:\\nI pray thee, good Mercutio,'\n"," b'that he\\nShould leave the helm and like a fearful lad\\nWith tearful eyes add water to the sea\\nAnd give'\n"," b\"at dim monument where Tybalt lies.\\n\\nLADY CAPULET:\\nTalk not to me, for I'll not speak a word:\\nDo as t\"\n"," b'rise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we be'\n"," b'hem; and there\\nbe many that they have loved, they know not\\nwherefore: so that, if they love they kno'\n"," b' harm in\\nhim: something too crabbed that way, friar.\\n\\nDUKE VINCENTIO:\\nIt is too general a vice, and '\n"," b'd am unshapen thus?\\nMy dukedom to a beggarly denier,\\nI do mistake my person all this while:\\nUpon my '\n"," b't,\\nI doubt not, uncle, of our victory.\\nMany a battle have I won in France,\\nWhen as the enemy hath be'\n"," b'hould import offending;\\nWhich is for me less easy to commit\\nThan you to punish.\\n\\nHERMIONE:\\nNot your '\n"," b'at make her good, so were I\\nA man, the worst about you.\\n\\nLEONTES:\\nForce her hence.\\n\\nPAULINA:\\nLet him'\n"," b\"t to Lady Lucy--\\nYour mother lives a witness to that vow--\\nAnd afterward by substitute betroth'd\\nTo \"\n"," b'on. We have made peace\\nWith no less honour to the Antiates\\nThan shame to the Romans: and we here del'\n"," b'be found? which that it shall,\\nIs all as monstrous to our human reason\\nAs my Antigonus to break his '\n"," b'eces, Volsces; men and lads,\\nStain all your edges on me. Boy! false hound!\\nIf you have writ your ann'\n"," b\"derer:\\nThy brother's love, our duty, and thy fault,\\nProvoke us hither now to slaughter thee.\\n\\nCLAREN\"\n"," b\" untried\\nOf that wide gap, since it is in my power\\nTo o'erthrow law and in one self-born hour\\nTo pla\"\n"," b'w I long to have some chat with her!\\n\\nBAPTISTA:\\nWell, go with me and be not so discomfited:\\nProceed '\n"," b\"no.\\n\\nPOMPEY:\\nI'll be supposed upon a book, his face is the worst\\nthing about him. Good, then; if his\"\n"," b\"how oft to-night\\nHave my old feet stumbled at graves! Who's there?\\n\\nBALTHASAR:\\nHere's one, a friend,\"\n"," b\" candle-holder, and look on.\\nThe game was ne'er so fair, and I am done.\\n\\nMERCUTIO:\\nTut, dun's the mo\"\n"," b'aster, resign thy crown.\\nWhat mutter you, or what conspire you, lords?\\n\\nWARWICK:\\nDo right unto this '\n"," b\"e.\\n\\nBRUTUS:\\nLet's to the Capitol;\\nAnd carry with us ears and eyes for the time,\\nBut hearts for the e\"\n"," b\"m for tribune.\\n\\nMENENIUS:\\nLet's be calm.\\n\\nCOMINIUS:\\nThe people are abused; set on. This paltering\\nBe\"\n"," b' who comes here?\\n\\nGREMIO:\\nGood morrow, neighbour Baptista.\\n\\nBAPTISTA:\\nGood morrow, neighbour Gremio.'\n"," b\"ere's no virtue whipped\\nout of the court: they cherish it to make it stay\\nthere; and yet it will no \"\n"," b\"o'er this thy day's work,\\nThou'ldst not believe thy deeds: but I'll report it\\nWhere senators shall m\"\n"," b'o hate thee\\nWorse than a promise-breaker.\\n\\nAUFIDIUS:\\nWe hate alike:\\nNot Afric owns a serpent I abhor'\n"," b\"of\\nthee!' and by the operation of the second cup draws\\nit on the drawer, when indeed there is no nee\"\n"," b\" a deer whose skin's a keeper's fee:\\nThis is the quondam king; let's seize upon him.\\n\\nKING HENRY VI:\"\n"," b'f?\\n\\nFirst Huntsman:\\nBelieve me, lord, I think he cannot choose.\\n\\nSecond Huntsman:\\nIt would seem stra'\n"," b'\\n\\nBENVOLIO:\\nBut new struck nine.\\n\\nROMEO:\\nAy me! sad hours seem long.\\nWas that my father that went he'\n"," b'ibunes,\\nThere is a slave, whom we have put in prison,\\nReports, the Volsces with two several powers\\nA'\n"," b'ed,\\nMore criminal in thee than it,--so thou\\nShalt feel our justice, in whose easiest passage\\nLook fo'\n"," b'e may digest our complots in some form.\\n\\nMessenger:\\nWhat, ho! my lord!\\n\\nHASTINGS:\\n\\nMessenger:\\nA mess'\n"," b'ingers itch. Wife, we scarce thought us blest\\nThat God had lent us but this only child;\\nBut now I se'\n"," b\" in mind\\nOf what you promised me.\\n\\nKING RICHARD III:\\nWell, but what's o'clock?\\n\\nBUCKINGHAM:\\nUpon the\"\n"," b\"soldier than he you wot on.\\n\\nSecond Servingman:\\nWho, my master?\\n\\nFirst Servingman:\\nNay, it's no matt\"\n"," b'sh for this world.\\n\\nQUEEN MARGARET:\\nHie thee to hell for shame, and leave the world,\\nThou cacodemon!'\n"," b\"any had not dared to do that evil,\\nIf the first that did the edict infringe\\nHad answer'd for his dee\"\n"," b' under your arm.\\n\\nROMEO:\\nI thought all for the best.\\n\\nMERCUTIO:\\nHelp me into some house, Benvolio,\\nO'\n"," b\" Juliet's grave; for there must I use thee.\\n\\nFRIAR JOHN:\\nHoly Franciscan friar! brother, ho!\\n\\nFRIAR \"\n"," b'mmer songs for me and my aunts,\\nWhile we lie tumbling in the hay.\\nI have served Prince Florizel and '\n"," b' peace\\nWith honour, as in war, since that to both\\nIt stands in like request?\\n\\nCORIOLANUS:\\nWhy force '\n"," b\"seest it not.\\n\\nROMEO:\\n'Tis torture, and not mercy: heaven is here,\\nWhere Juliet lives; and every cat\"\n"," b'mes here?\\n\\nSIR STEPHEN SCROOP:\\nMore health and happiness betide my liege\\nThan can my care-tuned tong'\n"," b'OLIXENES:\\nThis is the prettiest low-born lass that ever\\nRan on the green-sward: nothing she does or '\n"," b\"RIVERS:\\nWhat! loss of some pitch'd battle against Warwick?\\n\\nQUEEN ELIZABETH:\\nNo, but the loss of his\"\n"," b\" me faint.\\nCome, York and Richard, Warwick and the rest;\\nI stabb'd your fathers' bosoms, split my br\"\n"," b\".\\n\\nFirst Citizen:\\nI twice five hundred and their friends to piece 'em.\\n\\nBRUTUS:\\nGet you hence instan\"\n"," b'ide.\\n\\nOXFORD:\\nWhat now remains, my lords, for us to do\\nBut march to London with our soldiers?\\n\\nWARWI'\n"," b'e not: look, what I have said\\nI will avouch in presence of the king:\\nI dare adventure to be sent to '\n"," b'She embraces him.\\n\\nCAMILLO:\\nShe hangs about his neck:\\nIf she pertain to life let her speak too.\\n\\nPOL'\n"," b\"or Ireland.\\nIf then we shall shake off our slavish yoke,\\nImp out our drooping country's broken wing,\"\n"," b\"wn notion--\\nWho wears my stripes impress'd upon him; that\\nMust bear my beating to his grave--shall j\"\n"," b\"u shall well be spared.\\n\\nProvost:\\nI crave your honour's pardon.\\nWhat shall be done, sir, with the gr\"\n"," b\"ord; and you both have vow'd revenge\\nOn him, his sons, his favourites and his friends.\\n\\nNORTHUMBERLA\"\n"," b'e sin to wish me thus forsworn,\\nOr to dispraise my lord with that same tongue\\nWhich she hath praised'\n"," b':\\nComfort, my liege; remember who you are.\\n\\nKING RICHARD II:\\nI had forgot myself; am I not king?\\nAwa'\n"," b'AUMERLE:\\nIf God prevent not, I purpose so.\\n\\nDUKE OF YORK:\\nWhat seal is that, that hangs without thy '\n"," b'ET:\\nHow camest thou hither, tell me, and wherefore?\\nThe orchard walls are high and hard to climb,\\nAn'\n"," b' is so, as it appears,\\nAccountant to the law upon that pain.\\n\\nISABELLA:\\nTrue.\\n\\nANGELO:\\nAdmit no othe'\n"," b\"a word;\\nBut, like dumb statues or breathing stones,\\nGazed each on other, and look'd deadly pale.\\nWhi\"\n"," b\"hich, mellow'd by the stealing hours of time,\\nWill well become the seat of majesty,\\nAnd make, no dou\"]\n","\n","------------------------------------------------------------------------------------------------------------------------\n","\n","Target:  [b\"y this treason\\nEven with the dearest blood your bodies bear.\\n\\nKING EDWARD IV:\\nThe harder match'd, th\"\n"," b'hall not stay alone\\nTill holy church incorporate two in one.\\n\\nBENVOLIO:\\nI pray thee, good Mercutio, '\n"," b'hat he\\nShould leave the helm and like a fearful lad\\nWith tearful eyes add water to the sea\\nAnd give '\n"," b\"t dim monument where Tybalt lies.\\n\\nLADY CAPULET:\\nTalk not to me, for I'll not speak a word:\\nDo as th\"\n"," b'ise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we bec'\n"," b'em; and there\\nbe many that they have loved, they know not\\nwherefore: so that, if they love they know'\n"," b'harm in\\nhim: something too crabbed that way, friar.\\n\\nDUKE VINCENTIO:\\nIt is too general a vice, and s'\n"," b' am unshapen thus?\\nMy dukedom to a beggarly denier,\\nI do mistake my person all this while:\\nUpon my l'\n"," b',\\nI doubt not, uncle, of our victory.\\nMany a battle have I won in France,\\nWhen as the enemy hath bee'\n"," b'ould import offending;\\nWhich is for me less easy to commit\\nThan you to punish.\\n\\nHERMIONE:\\nNot your g'\n"," b't make her good, so were I\\nA man, the worst about you.\\n\\nLEONTES:\\nForce her hence.\\n\\nPAULINA:\\nLet him '\n"," b\" to Lady Lucy--\\nYour mother lives a witness to that vow--\\nAnd afterward by substitute betroth'd\\nTo B\"\n"," b'n. We have made peace\\nWith no less honour to the Antiates\\nThan shame to the Romans: and we here deli'\n"," b'e found? which that it shall,\\nIs all as monstrous to our human reason\\nAs my Antigonus to break his g'\n"," b'ces, Volsces; men and lads,\\nStain all your edges on me. Boy! false hound!\\nIf you have writ your anna'\n"," b\"erer:\\nThy brother's love, our duty, and thy fault,\\nProvoke us hither now to slaughter thee.\\n\\nCLARENC\"\n"," b\"untried\\nOf that wide gap, since it is in my power\\nTo o'erthrow law and in one self-born hour\\nTo plan\"\n"," b' I long to have some chat with her!\\n\\nBAPTISTA:\\nWell, go with me and be not so discomfited:\\nProceed i'\n"," b\"o.\\n\\nPOMPEY:\\nI'll be supposed upon a book, his face is the worst\\nthing about him. Good, then; if his \"\n"," b\"ow oft to-night\\nHave my old feet stumbled at graves! Who's there?\\n\\nBALTHASAR:\\nHere's one, a friend, \"\n"," b\"candle-holder, and look on.\\nThe game was ne'er so fair, and I am done.\\n\\nMERCUTIO:\\nTut, dun's the mou\"\n"," b'ster, resign thy crown.\\nWhat mutter you, or what conspire you, lords?\\n\\nWARWICK:\\nDo right unto this p'\n"," b\".\\n\\nBRUTUS:\\nLet's to the Capitol;\\nAnd carry with us ears and eyes for the time,\\nBut hearts for the ev\"\n"," b\" for tribune.\\n\\nMENENIUS:\\nLet's be calm.\\n\\nCOMINIUS:\\nThe people are abused; set on. This paltering\\nBec\"\n"," b'who comes here?\\n\\nGREMIO:\\nGood morrow, neighbour Baptista.\\n\\nBAPTISTA:\\nGood morrow, neighbour Gremio.\\n'\n"," b\"re's no virtue whipped\\nout of the court: they cherish it to make it stay\\nthere; and yet it will no m\"\n"," b\"'er this thy day's work,\\nThou'ldst not believe thy deeds: but I'll report it\\nWhere senators shall mi\"\n"," b' hate thee\\nWorse than a promise-breaker.\\n\\nAUFIDIUS:\\nWe hate alike:\\nNot Afric owns a serpent I abhor\\n'\n"," b\"f\\nthee!' and by the operation of the second cup draws\\nit on the drawer, when indeed there is no need\"\n"," b\"a deer whose skin's a keeper's fee:\\nThis is the quondam king; let's seize upon him.\\n\\nKING HENRY VI:\\n\"\n"," b'?\\n\\nFirst Huntsman:\\nBelieve me, lord, I think he cannot choose.\\n\\nSecond Huntsman:\\nIt would seem stran'\n"," b'\\nBENVOLIO:\\nBut new struck nine.\\n\\nROMEO:\\nAy me! sad hours seem long.\\nWas that my father that went hen'\n"," b'bunes,\\nThere is a slave, whom we have put in prison,\\nReports, the Volsces with two several powers\\nAr'\n"," b'd,\\nMore criminal in thee than it,--so thou\\nShalt feel our justice, in whose easiest passage\\nLook for'\n"," b' may digest our complots in some form.\\n\\nMessenger:\\nWhat, ho! my lord!\\n\\nHASTINGS:\\n\\nMessenger:\\nA messe'\n"," b'ngers itch. Wife, we scarce thought us blest\\nThat God had lent us but this only child;\\nBut now I see'\n"," b\"in mind\\nOf what you promised me.\\n\\nKING RICHARD III:\\nWell, but what's o'clock?\\n\\nBUCKINGHAM:\\nUpon the \"\n"," b\"oldier than he you wot on.\\n\\nSecond Servingman:\\nWho, my master?\\n\\nFirst Servingman:\\nNay, it's no matte\"\n"," b'h for this world.\\n\\nQUEEN MARGARET:\\nHie thee to hell for shame, and leave the world,\\nThou cacodemon! '\n"," b\"ny had not dared to do that evil,\\nIf the first that did the edict infringe\\nHad answer'd for his deed\"\n"," b'under your arm.\\n\\nROMEO:\\nI thought all for the best.\\n\\nMERCUTIO:\\nHelp me into some house, Benvolio,\\nOr'\n"," b\"Juliet's grave; for there must I use thee.\\n\\nFRIAR JOHN:\\nHoly Franciscan friar! brother, ho!\\n\\nFRIAR L\"\n"," b'mer songs for me and my aunts,\\nWhile we lie tumbling in the hay.\\nI have served Prince Florizel and i'\n"," b'peace\\nWith honour, as in war, since that to both\\nIt stands in like request?\\n\\nCORIOLANUS:\\nWhy force y'\n"," b\"eest it not.\\n\\nROMEO:\\n'Tis torture, and not mercy: heaven is here,\\nWhere Juliet lives; and every cat \"\n"," b'es here?\\n\\nSIR STEPHEN SCROOP:\\nMore health and happiness betide my liege\\nThan can my care-tuned tongu'\n"," b'LIXENES:\\nThis is the prettiest low-born lass that ever\\nRan on the green-sward: nothing she does or s'\n"," b\"IVERS:\\nWhat! loss of some pitch'd battle against Warwick?\\n\\nQUEEN ELIZABETH:\\nNo, but the loss of his \"\n"," b\"me faint.\\nCome, York and Richard, Warwick and the rest;\\nI stabb'd your fathers' bosoms, split my bre\"\n"," b\"\\n\\nFirst Citizen:\\nI twice five hundred and their friends to piece 'em.\\n\\nBRUTUS:\\nGet you hence instant\"\n"," b'de.\\n\\nOXFORD:\\nWhat now remains, my lords, for us to do\\nBut march to London with our soldiers?\\n\\nWARWIC'\n"," b' not: look, what I have said\\nI will avouch in presence of the king:\\nI dare adventure to be sent to t'\n"," b'he embraces him.\\n\\nCAMILLO:\\nShe hangs about his neck:\\nIf she pertain to life let her speak too.\\n\\nPOLI'\n"," b\"r Ireland.\\nIf then we shall shake off our slavish yoke,\\nImp out our drooping country's broken wing,\\n\"\n"," b\"n notion--\\nWho wears my stripes impress'd upon him; that\\nMust bear my beating to his grave--shall jo\"\n"," b\" shall well be spared.\\n\\nProvost:\\nI crave your honour's pardon.\\nWhat shall be done, sir, with the gro\"\n"," b\"rd; and you both have vow'd revenge\\nOn him, his sons, his favourites and his friends.\\n\\nNORTHUMBERLAN\"\n"," b' sin to wish me thus forsworn,\\nOr to dispraise my lord with that same tongue\\nWhich she hath praised '\n"," b'\\nComfort, my liege; remember who you are.\\n\\nKING RICHARD II:\\nI had forgot myself; am I not king?\\nAwak'\n"," b'UMERLE:\\nIf God prevent not, I purpose so.\\n\\nDUKE OF YORK:\\nWhat seal is that, that hangs without thy b'\n"," b'T:\\nHow camest thou hither, tell me, and wherefore?\\nThe orchard walls are high and hard to climb,\\nAnd'\n"," b'is so, as it appears,\\nAccountant to the law upon that pain.\\n\\nISABELLA:\\nTrue.\\n\\nANGELO:\\nAdmit no other'\n"," b\" word;\\nBut, like dumb statues or breathing stones,\\nGazed each on other, and look'd deadly pale.\\nWhic\"\n"," b\"ich, mellow'd by the stealing hours of time,\\nWill well become the seat of majesty,\\nAnd make, no doub\"]\n"]}]},{"cell_type":"markdown","metadata":{"id":"d496f0c0"},"source":["<a id = 'climax'></a>\n","## [Act III: Climax](#toc)\n","\n","> `PORTIA: How all the other passions fleet to air, As doubtful thoughts, and rash-embraced despair,And shuddering fear, and green-eyed jealousy! O love, be moderate. Allay thy ecstasy.In measure rein thy joy. Scant this excess. I feel too much thy blessing. Make it less, For fear I surfeit.`"],"id":"d496f0c0"},{"cell_type":"markdown","metadata":{"id":"aabe8908"},"source":["<a id = 'model'></a>\n","### [6. Defining the Model](#toc)\n","\n","> `Our aspirations for the future hold us from savouring the present.`"],"id":"aabe8908"},{"cell_type":"code","metadata":{"id":"978bbb36"},"source":["vocab_size = vocab_shakespeare\n","embedding_dim = 256\n","rnn_units = 1024\n","\n","class vanilla_model(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, rnn_units):\n","        super().__init__(self)\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = keras.layers.GRU(rnn_units, return_sequences = True, return_state = True)\n","        self.dense = keras.layers.Dense(vocab_size)\n","    \n","    def call(self, inputs, states = None, return_state = False, training = False):\n","        x = inputs\n","        x = self.embedding(x, training = training)\n","        if states is None:\n","            states = self.gru.get_initial_state(x)\n","        x, states = self.gru(x, initial_state = states, training = training)\n","        x = self.dense(x, training = training)\n","        if return_state:\n","            return x, states\n","        else:\n","            return x"],"id":"978bbb36","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5d17f7c3"},"source":["model_shakespeare = vanilla_model(vocab_size = len(ids_from_chars_shakespeare.get_vocabulary()), \n","                                  embedding_dim = embedding_dim,\n","                                  rnn_units = rnn_units)"],"id":"5d17f7c3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"96045774","executionInfo":{"status":"ok","timestamp":1634558334561,"user_tz":-330,"elapsed":5609,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"4604dbab-5547-467a-d276-f130b8b21e12"},"source":["for input_example_batch, target_example_batch in dataset_shakespeare.take(1):\n","    example_batch_predictions = model_shakespeare(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","    print(input_example_batch.shape)\n","\n","model_shakespeare.summary()"],"id":"96045774","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 68) # (batch_size, sequence_length, vocab_size)\n","(64, 100)\n","Model: \"vanilla_model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        multiple                  17408     \n","_________________________________________________________________\n","gru (GRU)                    multiple                  3938304   \n","_________________________________________________________________\n","dense (Dense)                multiple                  69700     \n","=================================================================\n","Total params: 4,025,412\n","Trainable params: 4,025,412\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"69cd876e"},"source":["<a id = 'training'></a>\n","### [Training the Model](#toc)\n","\n","> `Practice makes Perfect. Unless the practice itself is flawed. Then it only serves to reinforce the wrongs.`"],"id":"69cd876e"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49518d9f","executionInfo":{"status":"ok","timestamp":1634560616234,"user_tz":-330,"elapsed":2281711,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"7231728b-6055-4610-e5cd-dcc717b9d554"},"source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)\n","model_shakespeare.compile(optimizer = 'adam', loss = loss, metrics = ['accuracy'])\n","checkpoint_dir = '/content/drive/MyDrive/Assignments/Week 4,5/Training_Checkpoints/Shakespeare_Model'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix,\n","                                                        save_weights_only = True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, min_lr=0.00000001)\n","early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='min', restore_best_weights=True)\n","\n","EPOCHS = 20\n","history_shakespeare = model_shakespeare.fit(dataset_shakespeare, epochs = EPOCHS, callbacks = [reduce_lr, early_stop, checkpoint_callback])"],"id":"49518d9f","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","707/707 [==============================] - 93s 126ms/step - loss: 2.0296 - accuracy: 0.4206\n","Epoch 2/20\n","707/707 [==============================] - 91s 126ms/step - loss: 1.4505 - accuracy: 0.5599\n","Epoch 3/20\n","707/707 [==============================] - 91s 126ms/step - loss: 1.3360 - accuracy: 0.5879\n","Epoch 4/20\n","707/707 [==============================] - 91s 127ms/step - loss: 1.2814 - accuracy: 0.6012\n","Epoch 5/20\n","707/707 [==============================] - 91s 126ms/step - loss: 1.2434 - accuracy: 0.6109\n","Epoch 6/20\n","707/707 [==============================] - 91s 126ms/step - loss: 1.2129 - accuracy: 0.6189\n","Epoch 7/20\n","707/707 [==============================] - 90s 125ms/step - loss: 1.1866 - accuracy: 0.6260\n","Epoch 8/20\n","707/707 [==============================] - 90s 126ms/step - loss: 1.1624 - accuracy: 0.6325\n","Epoch 9/20\n","707/707 [==============================] - 91s 127ms/step - loss: 1.1405 - accuracy: 0.6388\n","Epoch 10/20\n","707/707 [==============================] - 91s 126ms/step - loss: 1.1201 - accuracy: 0.6443\n","Epoch 11/20\n","707/707 [==============================] - 91s 126ms/step - loss: 1.1015 - accuracy: 0.6497\n","Epoch 12/20\n","707/707 [==============================] - 90s 125ms/step - loss: 1.0847 - accuracy: 0.6547\n","Epoch 13/20\n","707/707 [==============================] - 90s 125ms/step - loss: 1.0699 - accuracy: 0.6589\n","Epoch 14/20\n","707/707 [==============================] - 90s 126ms/step - loss: 1.0578 - accuracy: 0.6623\n","Epoch 15/20\n","707/707 [==============================] - 90s 126ms/step - loss: 1.0475 - accuracy: 0.6654\n","Epoch 16/20\n","707/707 [==============================] - 90s 126ms/step - loss: 1.0383 - accuracy: 0.6682\n","Epoch 17/20\n","707/707 [==============================] - 91s 127ms/step - loss: 1.0321 - accuracy: 0.6697\n","Epoch 18/20\n","707/707 [==============================] - 91s 127ms/step - loss: 1.0273 - accuracy: 0.6711\n","Epoch 19/20\n","707/707 [==============================] - 91s 127ms/step - loss: 1.0241 - accuracy: 0.6718\n","Epoch 20/20\n","707/707 [==============================] - 91s 126ms/step - loss: 1.0210 - accuracy: 0.6726\n"]}]},{"cell_type":"markdown","metadata":{"id":"6b27f81f"},"source":["<a id = 'playwright'></a>\n","### [8. The Playwright](#toc)\n","\n","> `Who is the playwright - me or Him? He moves my mind and I simply the pen`"],"id":"6b27f81f"},{"cell_type":"code","metadata":{"id":"94f0772d"},"source":["class OneStep(tf.keras.Model):\n","    def __init__(self, model, chars_from_ids, ids_from_chars, temperature = 1.0):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.model = model\n","        self.chars_from_ids = chars_from_ids\n","        self.ids_from_chars = ids_from_chars\n","\n","        # Create a mask to prevent [UNK] from being generated\n","        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","\n","        sparse_mask = tf.SparseTensor(values = [-float(np.inf)] * len(skip_ids), indices = skip_ids, dense_shape = [len(ids_from_chars.get_vocabulary())])\n","        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","    @tf.function\n","    def generate_one_step(self, inputs, states = None):\n","        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","        input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","        # Run the model.\n","        # predicted_logits.shape is [batch, char, next_char_logits]\n","        predicted_logits, states = self.model(inputs=input_ids, states=states,\n","                                                  return_state=True)\n","        # Only use the last prediction.\n","        predicted_logits = predicted_logits[:, -1, :]\n","        predicted_logits = predicted_logits/self.temperature\n","        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","        predicted_logits = predicted_logits + self.prediction_mask\n","        \n","        # Sample the output logits to generate token IDs.\n","        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","        \n","        # Convert from token ids to characters\n","        predicted_chars = self.chars_from_ids(predicted_ids)\n","        \n","        # Return the characters and model state.\n","        return predicted_chars, states"],"id":"94f0772d","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4d2b293b"},"source":["<a id = 'new_beginning'></a>\n","### [9. A New Beginning](#toc)\n","\n","> `Maybe if the stars align, maybe if our world's collide, maybe on the dark side we can be together.`\n","\n","Could Romeo have had a different story?"],"id":"4d2b293b"},{"cell_type":"code","metadata":{"id":"c48b7b15"},"source":["one_step_model_shakespeare = OneStep(model_shakespeare, chars_from_ids_shakespeare, ids_from_chars_shakespeare)"],"id":"c48b7b15","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdfaa22f","executionInfo":{"status":"ok","timestamp":1634560620636,"user_tz":-330,"elapsed":4406,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"e04b50bc-fdf8-4052-a2d6-60297412db48"},"source":["start = time.time()\n","states = None\n","next_char = tf.constant(['ROMEO: '])\n","result = [next_char]\n","\n","for n in range(1000):\n","    next_char, states = one_step_model_shakespeare.generate_one_step(next_char, states=states)\n","    result.append(next_char)\n","\n","result_shakespeare = tf.strings.join(result)\n","end = time.time()\n","\n","print(result_shakespeare[0].numpy().decode('UTF-8'), '\\n\\n'+'_'*80)\n","print('\\nRun time: ', end-start)"],"id":"bdfaa22f","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ROMEO: Lasan to me to his going together.\n","\n","YORK:\n","Back Dian!\n","\n","TOUCHSTONE:\n","Of common virtue, I confess whole you may be undone,\n","To signify thou art. Sirrah, come you on him.\n","\n","MENAS:\n","How long is't my drift in heaven, she is so.\n","\n","ROSALINE:\n","You are all after hit.\n","\n","All TOBYONK:\n","Good morrow, banish me.\n","\n","KING CLAUDIUS:\n","As good as a!\n","Why should I think on't: if any clerights,\n","If a must deny it.\n","\n","IMOGEN:\n","I'll hang myself to bleman.\n","\n","LAERTES:\n","What it is indistingus?' why, you are too bill,\n","To fool they bid you fare.\n","\n","CAIUS LUCIUS:\n","No, my lord.\n","\n","OTHELLO:\n","Is't possible? O hopel's tongue!\n","Garding, pretty life with thee!\n","\n","QUEEN MARIA:\n","Will you be so inclined? The fits of marrying he,\n","Did even leave it in an ophinestial creature.\n","\n","HOLOFERNES:\n","Here!\n","\n","GUIDERIUS:\n","Let's think thee for't.\n","Take no drunk double, and then depend to cured.\n","Come, through tuns some devil:--\n","\n","OTHELLO:\n","So well, 'tis but mine enemy.\n","\n","LYSANDER:\n","This is the times mocker on A feeble means.\n","\n","NORFOLK:\n","Your lordship is not wounded,\n","I am not wor \n","\n","________________________________________________________________________________\n","\n","Run time:  4.343403577804565\n"]}]},{"cell_type":"markdown","metadata":{"id":"e042acea"},"source":["<a id = 'war_and_peace'></a>\n","### [10. To War and Peace](#toc)\n","\n","> `The two most powerful warriors are patience and time.`"],"id":"e042acea"},{"cell_type":"markdown","metadata":{"id":"8eaabf5a"},"source":["Trying it out for Tolstoy's War and Peace"],"id":"8eaabf5a"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4dce9de","executionInfo":{"status":"ok","timestamp":1634562185520,"user_tz":-330,"elapsed":1564893,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"eabe6d5b-cba0-4bcf-cec7-bfef8a28cc2b"},"source":["path_to_file = tf.keras.utils.get_file('/content/drive/MyDrive/Assignments/Week 4,5/Data/war_and_peace.txt',\n","                                      'https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt')\n","\n","text_tolstoy = open(path_to_file, 'rb').read().decode(encoding = 'UTF-8')\n","\n","vocab_tolstoy = sorted(set(text_tolstoy))\n","ids_from_chars_tolstoy = preprocessing.StringLookup(vocabulary = list(vocab_tolstoy), mask_token = None)\n","all_ids_tolstoy = ids_from_chars_tolstoy(tf.strings.unicode_split(text_tolstoy, 'UTF-8'))\n","\n","ids_dataset_tolstoy = tf.data.Dataset.from_tensor_slices(all_ids_tolstoy)\n","chars_from_ids_tolstoy = preprocessing.StringLookup(vocabulary = ids_from_chars_tolstoy.get_vocabulary(),\n","                                                    invert = True,\n","                                                    mask_token = None)\n","seq_length = 100\n","examples_per_epoch = len(text_tolstoy)//(seq_length + 1)\n","sequences_tolstoy = ids_dataset_tolstoy.batch(seq_length + 1, drop_remainder = True)\n","dataset_tolstoy = sequences_tolstoy.map(split_input_target)\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset_tolstoy = dataset_tolstoy.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)\n","embedding_dim = 256\n","rnn_units = 1024\n","model_tolstoy = vanilla_model(vocab_size = len(ids_from_chars_tolstoy.get_vocabulary()), \n","                              embedding_dim = embedding_dim,\n","                              rnn_units = rnn_units)\n","\n","loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)\n","model_tolstoy.compile(optimizer = 'adam', loss = loss, metrics = ['accuracy'])\n","checkpoint_dir = '/content/drive/MyDrive/Assignments/Week 4,5/Training_Checkpoints/Tolstoy_Model'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix,\n","                                                        save_weights_only = True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, min_lr=0.00000001)\n","early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='min', restore_best_weights=True)\n","\n","EPOCHS = 20\n","history_tolstoy = model_tolstoy.fit(dataset_tolstoy, epochs = EPOCHS, callbacks = [reduce_lr, early_stop, checkpoint_callback])\n","one_step_model_tolstoy = OneStep(model_tolstoy, chars_from_ids_tolstoy, ids_from_chars_tolstoy)\n","start = time.time()\n","states = None\n","next_char = tf.constant(['Anna'])\n","result = [next_char]\n","\n","for n in range(1000):\n","    next_char, states = one_step_model_tolstoy.generate_one_step(next_char, states=states)\n","    result.append(next_char)\n","\n","result_tolstoy = tf.strings.join(result)\n","end = time.time()\n","\n","print(result_tolstoy[0].numpy().decode('UTF-8'), '\\n\\n'+'_'*80)\n","print('\\nRun time: ', end-start)"],"id":"f4dce9de","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://cs.stanford.edu/people/karpathy/char-rnn/warpeace_input.txt\n","3260416/3258246 [==============================] - 0s 0us/step\n","3268608/3258246 [==============================] - 0s 0us/step\n","Epoch 1/20\n","504/504 [==============================] - 68s 127ms/step - loss: 2.0980 - accuracy: 0.4085\n","Epoch 2/20\n","504/504 [==============================] - 65s 127ms/step - loss: 1.4033 - accuracy: 0.5792\n","Epoch 3/20\n","504/504 [==============================] - 65s 126ms/step - loss: 1.2433 - accuracy: 0.6188\n","Epoch 4/20\n","504/504 [==============================] - 66s 127ms/step - loss: 1.1740 - accuracy: 0.6363\n","Epoch 5/20\n","504/504 [==============================] - 66s 127ms/step - loss: 1.1275 - accuracy: 0.6483\n","Epoch 6/20\n","504/504 [==============================] - 65s 127ms/step - loss: 1.0907 - accuracy: 0.6579\n","Epoch 7/20\n","504/504 [==============================] - 65s 126ms/step - loss: 1.0590 - accuracy: 0.6668\n","Epoch 8/20\n","504/504 [==============================] - 65s 127ms/step - loss: 1.0299 - accuracy: 0.6746\n","Epoch 9/20\n","504/504 [==============================] - 65s 126ms/step - loss: 1.0025 - accuracy: 0.6823\n","Epoch 10/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.9768 - accuracy: 0.6896\n","Epoch 11/20\n","504/504 [==============================] - 66s 127ms/step - loss: 0.9527 - accuracy: 0.6963\n","Epoch 12/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.9310 - accuracy: 0.7027\n","Epoch 13/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.9117 - accuracy: 0.7082\n","Epoch 14/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.8945 - accuracy: 0.7132\n","Epoch 15/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.8800 - accuracy: 0.7173\n","Epoch 16/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.8679 - accuracy: 0.7208\n","Epoch 17/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.8574 - accuracy: 0.7235\n","Epoch 18/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.8509 - accuracy: 0.7254\n","Epoch 19/20\n","504/504 [==============================] - 65s 126ms/step - loss: 0.8450 - accuracy: 0.7269\n","Epoch 20/20\n","504/504 [==============================] - 65s 125ms/step - loss: 0.8411 - accuracy: 0.7279\n","Anna Mikhaylovna or her mother's men. Undiling in French which had been to describe a definite or how\n","they went to the soul. He always said they once commander, dressing round\n","her own emotion with Natasha and wink.\n","\n","\"No, tell us it interfere. Out of the world marched! We'll make your\n","laid against the latter was anywhere. On the road prolonged\n","their moans Kolocha--who were first began to run away which he is obstured by\n","his actions! I made a tool little walls. And you made a pleasure in silence. Some\n","who decide the far that stands this prayers at Torzhol religion?\" said the rejoining\n","board. \"Hard as it is becove us: espering the French!\" said the Cossack's way all the\n","reproaches that seemed to have twelve human figures, came in itself, a melancholy movement of his\n","armfully.\n","\n","\"Well, we old servant may absolute than any other reason on this first\n","chase--thee'bly lost so! But where are you here?\"\n","\n","\"I can still remain without food. Was the will of the confusion for\n","the monkee a \n","\n","________________________________________________________________________________\n","\n","Run time:  4.122719049453735\n"]}]},{"cell_type":"markdown","metadata":{"id":"2dddcf98"},"source":["<a id = 'falling_action'></a>\n","## [Act IV: Falling Action](#toc)\n","\n","> `SHYLOCK: What judgment shall I dread, doing no wrong? \n","            You have among you many a purchased slave,\n","            Which—like your asses and your dogs and mules—\n","            You use in abject and in slavish parts\n","            Because you bought them. Shall I say to you,\n","            “Let them be free! Marry them to your heirs!\n","            Why sweat they under burdens? Let their beds\n","            Be made as soft as yours and let their palates\n","            Be seasoned with such viands”? You will answer,\n","            “The slaves are ours.” So do I answer you.\n","            The pound of flesh which I demand of him\n","            Is dearly bought. 'Tis mine and I will have it.\n","            If you deny me, fie upon your law—\n","            There is no force in the decrees of Venice.\n","            I stand for judgment. Answer, shall I have it?`\n","            \n","In this section, we'll examine and compare a model that takes in a word vocabulary and forms its predictions. We can then compare which one does best"],"id":"2dddcf98"},{"cell_type":"code","metadata":{"id":"308061f7","executionInfo":{"status":"ok","timestamp":1635623332593,"user_tz":-330,"elapsed":2107,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}}},"source":["path_to_file = tf.keras.utils.get_file('/content/drive/MyDrive/Assignments/Week 4,5/Data/shakespeare.txt',\n","                                      'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt')\n","\n","text_shakespeare = open(path_to_file, 'rb').read().decode(encoding = 'UTF-8')"],"id":"308061f7","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6785b0fc","executionInfo":{"status":"ok","timestamp":1635623346119,"user_tz":-330,"elapsed":10476,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"3063130e-f02b-48ef-ca41-55813b3ead92"},"source":["vocab_words = sorted(set(keras.preprocessing.text.text_to_word_sequence(text_shakespeare, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')))\n","# print(len(vocab_words))\n","ids_from_words_shakespeare = preprocessing.StringLookup(vocabulary = list(vocab_words), mask_token = None)\n","all_ids_shakespeare = ids_from_words_shakespeare(keras.preprocessing.text.text_to_word_sequence(text_shakespeare))\n","# print(all_ids.shape)\n","ids_dataset_shakespeare = tf.data.Dataset.from_tensor_slices(all_ids_shakespeare)\n","words_from_ids_shakespeare = preprocessing.StringLookup(vocabulary = ids_from_words_shakespeare.get_vocabulary(),\n","                                                        invert = True,\n","                                                        mask_token = None)\n","for ids in ids_dataset_shakespeare.take(5):\n","    print(words_from_ids_shakespeare(ids).numpy().decode('UTF-8'), end = ' ')"],"id":"6785b0fc","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["first citizen before we proceed "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5f35c601","executionInfo":{"status":"ok","timestamp":1635623382300,"user_tz":-330,"elapsed":556,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"04fbfa47-8d69-4823-d590-c390f1d2a3f6"},"source":["seq_length = 100\n","\n","sequences_shakespeare = ids_dataset_shakespeare.batch(seq_length + 1, drop_remainder = True)\n","for seq in sequences_shakespeare.take(1):\n","    print(words_from_ids_shakespeare(seq).numpy(), end = ' ')"],"id":"5f35c601","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[b'first' b'citizen' b'before' b'we' b'proceed' b'any' b'further' b'hear'\n"," b'me' b'speak' b'all' b'speak' b'speak' b'first' b'citizen' b'you' b'are'\n"," b'all' b'resolved' b'rather' b'to' b'die' b'than' b'to' b'famish' b'all'\n"," b'resolved' b'resolved' b'first' b'citizen' b'first' b'you' b'know'\n"," b'caius' b'marcius' b'is' b'chief' b'enemy' b'to' b'the' b'people' b'all'\n"," b'we' b\"know't\" b'we' b\"know't\" b'first' b'citizen' b'let' b'us' b'kill'\n"," b'him' b'and' b\"we'll\" b'have' b'corn' b'at' b'our' b'own' b'price'\n"," b\"is't\" b'a' b'verdict' b'all' b'no' b'more' b'talking' b\"on't\" b'let'\n"," b'it' b'be' b'done' b'away' b'away' b'second' b'citizen' b'one' b'word'\n"," b'good' b'citizens' b'first' b'citizen' b'we' b'are' b'accounted' b'poor'\n"," b'citizens' b'the' b'patricians' b'good' b'what' b'authority' b'surfeits'\n"," b'on' b'would' b'relieve' b'us' b'if' b'they' b'would' b'yield'] "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"47eb3ec1","executionInfo":{"status":"ok","timestamp":1635623410729,"user_tz":-330,"elapsed":1416,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"b9050099-83e3-4d47-edb4-80b5b96e9678"},"source":["def text_from_ids(ids, ids_from_words):\n","    \"\"\"\n","    Reverses encoding and returns strings that produce the encoding\n","    Arguments: \n","    1) Array of ids\n","    2) Preprocessing Layer used to create the encoding\n","    Returns:\n","    1) String in plain english (or Shakespearean here!)\n","    \"\"\"\n","    words_from_ids = preprocessing.StringLookup(vocabulary = ids_from_words.get_vocabulary(), invert = True, mask_token = None)\n","    return str(tf.strings.reduce_join(words_from_ids(ids), axis = -1, separator = ' ').numpy())\n","\n","def split_input_target(sequence):\n","    \"\"\"\n","    Splits input encoded strings into training examples and training labels\n","    Arguments:\n","    1) Array of input encoded data\n","    Returns:\n","    1) Training examples\n","    2) Training labels\n","    \"\"\"\n","    input_text = sequence[0:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text\n","\n","dataset_shakespeare = sequences_shakespeare.map(split_input_target)\n","for input_example, target_example in dataset_shakespeare.take(1):\n","    print(\"Input: \", text_from_ids(input_example, ids_from_words_shakespeare))\n","    print()\n","    print('-'*90)\n","    print()\n","    print(\"Target: \", text_from_ids(target_example, ids_from_words_shakespeare))"],"id":"47eb3ec1","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:  b\"first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we know't we know't first citizen let us kill him and we'll have corn at our own price is't a verdict all no more talking on't let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve us if they would\"\n","\n","------------------------------------------------------------------------------------------\n","\n","Target:  b\"citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we know't we know't first citizen let us kill him and we'll have corn at our own price is't a verdict all no more talking on't let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve us if they would yield\"\n"]}]},{"cell_type":"code","metadata":{"id":"eb908c16","executionInfo":{"status":"ok","timestamp":1635623430312,"user_tz":-330,"elapsed":545,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}}},"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 100\n","\n","dataset_shakespeare = dataset_shakespeare.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder = True).prefetch(tf.data.experimental.AUTOTUNE)"],"id":"eb908c16","execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1e433c27","executionInfo":{"status":"ok","timestamp":1635623434704,"user_tz":-330,"elapsed":583,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}}},"source":["vocab_size = vocab_words\n","embedding_dim = 512\n","rnn_units = 1024\n","\n","class vanilla_model(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, rnn_units):\n","        super().__init__(self)\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = keras.layers.GRU(rnn_units, return_sequences = True, return_state = True)\n","        self.dense = keras.layers.Dense(vocab_size)\n","    \n","    def call(self, inputs, states = None, return_state = False, training = False):\n","        x = inputs\n","        x = self.embedding(x, training = training)\n","        if states is None:\n","            states = self.gru.get_initial_state(x)\n","        x, states = self.gru(x, initial_state = states, training = training)\n","        x = self.dense(x, training = training)\n","        if return_state:\n","            return x, states\n","        else:\n","            return x"],"id":"1e433c27","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fd2cd3a","executionInfo":{"status":"ok","timestamp":1635623442494,"user_tz":-330,"elapsed":2,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}}},"source":["model_shakespeare_words = vanilla_model(vocab_size = len(ids_from_words_shakespeare.get_vocabulary()), embedding_dim = embedding_dim, rnn_units = rnn_units)"],"id":"1fd2cd3a","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f05fdda3","executionInfo":{"status":"ok","timestamp":1635623451319,"user_tz":-330,"elapsed":5673,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"eea19870-9c3f-4354-a458-dabe11241dd5"},"source":["for input_example_batch, target_example_batch in dataset_shakespeare.take(1):\n","    example_batch_predictions = model_shakespeare_words(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","    print(input_example_batch.shape)"],"id":"f05fdda3","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 25502) # (batch_size, sequence_length, vocab_size)\n","(64, 100)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af5e5226","executionInfo":{"status":"ok","timestamp":1635630217444,"user_tz":-330,"elapsed":6748219,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"4a14388c-d8e1-4489-ae11-0303f706b0c5"},"source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)\n","model_shakespeare_words.compile(optimizer = 'adam', loss = loss, metrics = ['accuracy'])\n","checkpoint_dir = '/content/drive/MyDrive/Assignments/Week 4,5/Training_Checkpoints/Word_Model_Shakespeare'\n","checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix,\n","                                                        save_weights_only = True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, min_lr=0.00000001)\n","early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='min', restore_best_weights=True)\n","\n","EPOCHS = 50\n","history_shakespeare_words = model_shakespeare_words.fit(dataset_shakespeare, epochs = EPOCHS, callbacks = [reduce_lr, early_stop, checkpoint_callback])"],"id":"af5e5226","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","129/129 [==============================] - 96s 728ms/step - loss: 7.7001 - accuracy: 0.0294\n","Epoch 2/50\n","129/129 [==============================] - 94s 732ms/step - loss: 7.1826 - accuracy: 0.0427\n","Epoch 3/50\n","129/129 [==============================] - 95s 735ms/step - loss: 6.9938 - accuracy: 0.0521\n","Epoch 4/50\n","129/129 [==============================] - 95s 736ms/step - loss: 6.8673 - accuracy: 0.0557\n","Epoch 5/50\n","129/129 [==============================] - 95s 737ms/step - loss: 6.7566 - accuracy: 0.0598\n","Epoch 6/50\n","129/129 [==============================] - 95s 739ms/step - loss: 6.6168 - accuracy: 0.0638\n","Epoch 7/50\n","129/129 [==============================] - 96s 740ms/step - loss: 6.4591 - accuracy: 0.0707\n","Epoch 8/50\n","129/129 [==============================] - 95s 740ms/step - loss: 6.3035 - accuracy: 0.0798\n","Epoch 9/50\n","129/129 [==============================] - 96s 741ms/step - loss: 6.0981 - accuracy: 0.0897\n","Epoch 10/50\n","129/129 [==============================] - 96s 741ms/step - loss: 5.9012 - accuracy: 0.0987\n","Epoch 11/50\n","129/129 [==============================] - 96s 740ms/step - loss: 5.7207 - accuracy: 0.1076\n","Epoch 12/50\n","129/129 [==============================] - 96s 742ms/step - loss: 5.5377 - accuracy: 0.1158\n","Epoch 13/50\n","129/129 [==============================] - 96s 741ms/step - loss: 5.3625 - accuracy: 0.1249\n","Epoch 14/50\n","129/129 [==============================] - 96s 740ms/step - loss: 5.1820 - accuracy: 0.1347\n","Epoch 15/50\n","129/129 [==============================] - 96s 742ms/step - loss: 5.0017 - accuracy: 0.1471\n","Epoch 16/50\n","129/129 [==============================] - 96s 741ms/step - loss: 4.8278 - accuracy: 0.1605\n","Epoch 17/50\n","129/129 [==============================] - 96s 741ms/step - loss: 4.6593 - accuracy: 0.1746\n","Epoch 18/50\n","129/129 [==============================] - 96s 741ms/step - loss: 4.5019 - accuracy: 0.1888\n","Epoch 19/50\n","129/129 [==============================] - 96s 741ms/step - loss: 4.3562 - accuracy: 0.2031\n","Epoch 20/50\n","129/129 [==============================] - 96s 741ms/step - loss: 4.2086 - accuracy: 0.2179\n","Epoch 21/50\n","129/129 [==============================] - 96s 740ms/step - loss: 4.0603 - accuracy: 0.2340\n","Epoch 22/50\n","129/129 [==============================] - 95s 740ms/step - loss: 3.9230 - accuracy: 0.2495\n","Epoch 23/50\n","129/129 [==============================] - 95s 738ms/step - loss: 3.7976 - accuracy: 0.2644\n","Epoch 24/50\n","129/129 [==============================] - 95s 740ms/step - loss: 3.6781 - accuracy: 0.2795\n","Epoch 25/50\n","129/129 [==============================] - 95s 739ms/step - loss: 3.5647 - accuracy: 0.2932\n","Epoch 26/50\n","129/129 [==============================] - 95s 738ms/step - loss: 3.4547 - accuracy: 0.3085\n","Epoch 27/50\n","129/129 [==============================] - 95s 739ms/step - loss: 3.3498 - accuracy: 0.3226\n","Epoch 28/50\n","129/129 [==============================] - 95s 738ms/step - loss: 3.2539 - accuracy: 0.3361\n","Epoch 29/50\n","129/129 [==============================] - 95s 739ms/step - loss: 3.1634 - accuracy: 0.3494\n","Epoch 30/50\n","129/129 [==============================] - 95s 739ms/step - loss: 3.0752 - accuracy: 0.3623\n","Epoch 31/50\n","129/129 [==============================] - 95s 738ms/step - loss: 2.9992 - accuracy: 0.3733\n","Epoch 32/50\n","129/129 [==============================] - 95s 738ms/step - loss: 2.9139 - accuracy: 0.3869\n","Epoch 33/50\n","129/129 [==============================] - 95s 737ms/step - loss: 2.8367 - accuracy: 0.3989\n","Epoch 34/50\n","129/129 [==============================] - 95s 738ms/step - loss: 2.7624 - accuracy: 0.4104\n","Epoch 35/50\n","129/129 [==============================] - 95s 737ms/step - loss: 2.6954 - accuracy: 0.4219\n","Epoch 36/50\n","129/129 [==============================] - 95s 737ms/step - loss: 2.6375 - accuracy: 0.4306\n","Epoch 37/50\n","129/129 [==============================] - 95s 737ms/step - loss: 2.5765 - accuracy: 0.4408\n","Epoch 38/50\n","129/129 [==============================] - 95s 736ms/step - loss: 2.5172 - accuracy: 0.4504\n","Epoch 39/50\n","129/129 [==============================] - 95s 737ms/step - loss: 2.4611 - accuracy: 0.4599\n","Epoch 40/50\n","129/129 [==============================] - 95s 736ms/step - loss: 2.4107 - accuracy: 0.4684\n","Epoch 41/50\n","129/129 [==============================] - 95s 735ms/step - loss: 2.3552 - accuracy: 0.4778\n","Epoch 42/50\n","129/129 [==============================] - 95s 736ms/step - loss: 2.3026 - accuracy: 0.4877\n","Epoch 43/50\n","129/129 [==============================] - 95s 736ms/step - loss: 2.2479 - accuracy: 0.4972\n","Epoch 44/50\n","129/129 [==============================] - 95s 736ms/step - loss: 2.1894 - accuracy: 0.5075\n","Epoch 45/50\n","129/129 [==============================] - 95s 735ms/step - loss: 2.1340 - accuracy: 0.5177\n","Epoch 46/50\n","129/129 [==============================] - 95s 735ms/step - loss: 2.0795 - accuracy: 0.5281\n","Epoch 47/50\n","129/129 [==============================] - 95s 736ms/step - loss: 2.0295 - accuracy: 0.5379\n","Epoch 48/50\n","129/129 [==============================] - 95s 735ms/step - loss: 1.9806 - accuracy: 0.5469\n","Epoch 49/50\n","129/129 [==============================] - 95s 736ms/step - loss: 1.9321 - accuracy: 0.5557\n","Epoch 50/50\n","129/129 [==============================] - 95s 735ms/step - loss: 1.8859 - accuracy: 0.5650\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"id":"cM7b-gsVDwKX","executionInfo":{"status":"error","timestamp":1635630220523,"user_tz":-330,"elapsed":1337,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02067187832579696971"}},"outputId":"e185359b-ef39-4712-967f-eb9ce7cc5e71"},"source":["model_shakespeare_words.save('/content/drive/MyDrive/Assignments/Week 4,5/My_Model/my_model')"],"id":"cM7b-gsVDwKX","execution_count":12,"outputs":[{"output_type":"error","ename":"NotImplementedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-ddef2f7f290d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_shakespeare_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Assignments/Week 4,5/My_Model/my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   2144\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 2146\u001b[0;31m                     signatures, options, save_traces)\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m   def save_weights(self,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    137\u001b[0m         not isinstance(model, sequential.Sequential)):\n\u001b[1;32m    138\u001b[0m       raise NotImplementedError(\n\u001b[0;32m--> 139\u001b[0;31m           \u001b[0;34m'Saving the model to HDF5 format requires the model to be a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m           \u001b[0;34m'Functional model or a Sequential model. It does not work for '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m           \u001b[0;34m'subclassed models, because such models are defined via the body of '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."]}]},{"cell_type":"code","metadata":{"id":"0a9133d4"},"source":["class OneStep(tf.keras.Model):\n","    def __init__(self, model, words_from_ids, ids_from_words, temperature = 1.0):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.model = model\n","        self.words_from_ids = words_from_ids\n","        self.ids_from_words = ids_from_words\n","\n","        # Create a mask to prevent [UNK] from being generated\n","        skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n","\n","        sparse_mask = tf.SparseTensor(values = [-float(np.inf)] * len(skip_ids), indices = skip_ids, dense_shape = [len(ids_from_words.get_vocabulary())])\n","        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","    @tf.function\n","    def generate_one_step(self, inputs, states = None):\n","        input_words = tf.strings.split(inputs)\n","        input_ids = self.ids_from_words(input_words).to_tensor()\n","\n","        # Run the model.\n","        # predicted_logits.shape is [batch, char, next_char_logits]\n","        predicted_logits, states = self.model(inputs=input_ids,\n","                                              states=states,\n","                                              return_state=True)\n","        # Only use the last prediction.\n","        predicted_logits = predicted_logits[:, -1, :]\n","        predicted_logits = predicted_logits/self.temperature\n","        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","        predicted_logits = predicted_logits + self.prediction_mask\n","        \n","        # Sample the output logits to generate token IDs.\n","        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","        \n","        # Convert from token ids to characters\n","        predicted_words = self.words_from_ids(predicted_ids)\n","        \n","        # Return the characters and model state.\n","        return predicted_words, states"],"id":"0a9133d4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7e76acc"},"source":["one_step_model_shakespeare = OneStep(model_shakespeare_words, words_from_ids_shakespeare, ids_from_words_shakespeare)"],"id":"e7e76acc","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9e73cac0","executionInfo":{"status":"ok","timestamp":1634644439673,"user_tz":-330,"elapsed":7005,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13175825734666089488"}},"outputId":"4ef2e9ce-50d8-43ec-be70-3c5d25c65299"},"source":["start = time.time()\n","states = None\n","next_word = tf.constant(['romeo'])\n","result = [next_char]\n","\n","for n in range(1000):\n","    next_word, states = one_step_model_shakespeare.generate_one_step(next_word, states=states)\n","    result.append(next_word)\n","\n","result_shakespeare = tf.strings.join(result, separator = ' ')\n","end = time.time()\n","\n","print(result_shakespeare[0].numpy().decode('UTF-8'), '\\n\\n'+'_'*80)\n","print('\\nRun time: ', end-start)"],"id":"9e73cac0","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ROMEO:  fairy he hath shot borne here and hark the time is dead and turn'd full thousand letters from this state of gold else fits the angry with the ear to send her hence from desperate the reason either she is dead which lodovico is it duke of aumerle benvolio king henry didst thou still minister with this sentence of this good mars i speak i can do more what is some reason to thee by heaven that hell should lend my place to speak benvolio is romeo and the prince tybalt must hide the sorrow from my sorrow that life mine is not a good leg or any nor the prince what is paris hath not dared before joy with sorrow my lord and what are often taken by might be gracious be thought an honourable office shall be full because apt and romeo must hide thee for thou follow me too prince henry that dread prince henry this famous prince uncle prince henry thou must not alas enough how be prince henry how can you that our uncle beg lord of this is this the prince of a good may be my scope is here prince henry the duke hath this writ and shall i this sorrow answer can pay a thousand fragrant and be gone to athens by this hand the duke my will win me throw unto some order and yet i will have some upon my grave hearing this of day for time to win my lord my lord of any prince henry then to give me hearing i am so fond to you my lord of buckingham buckingham so late you shall speak to you all this office shall i throw my state my love shall tell me prince what is i can get him by and my gracious lord will you come to that must be my lord or else the prince well known my lord of athens will do well the office deep of majesty my state is well supplied against my will and i will prince and is some only hearing of this my love prince henry why is not this for you and will not me shall be my lord if you go take my ring i beg a royal prince and my love to my lady falstaff alas you have my lord of sack prince henry for this is dead falstaff you love me prince henry how this brother sir nay you beg gentlemen my lord of your having what is my lord what can you go to my sovereign lord virtuous prince of this greeting for my lord prince here to that and some prince prince henry what might my office for the better brook than i can hide what i can and that i have but of that falstaff prince henry my lord i shall be thought of my intents to day my lord prince henry and didst the prince is gone to cure the prince with so deep a story prince that thou hast here writ to day prince henry how shall this be thy fault open my heart to my crown falstaff my name is grown prince henry how deep that my daughter can how many whose children and our title is as deep for me a better for my sovereign some four of them prince henry how now how dost thou speak of conscience with some of them lord of york i will have some news enough of good my lord king henry v some prince of love that is some hard by some of thy brother york is my gracious lord to lancaster i have my majesty and will unto my friends for i will have a thousand before some dreadful jack for this i can for so may my cost i throw my sensual through the arm and makes my hard patience for this world's vengeance pay me for a thousand my comfort sir the black macbeth can play upon thee o lord can tell thee this is the life of me a weary world with sorrow this rich story let's but more anon than he that is all safe nay rather can she be this seal'd up in parting than the day of her hath settled state that can be so retired to me that than this can from me for this before this arm my brother will before this eyes of heaven who shouldst have been my office which easily to hide this deep she by desperate duke for me my father's lord which some hath hath office found when many thousand duke and by my father's life before some more prince henry never can be quiet yet there comes thy in her love doth love and with thy love might call the hard poison for me sir that is this fond for shame if this thy love be strong upon me the king before thy father in the poison of my will unless we thought honourable prince king than tidings to me the prince of wales that i might not have in love with much more thou didst speak with king commands her not i not to speak too little title before the duke for company never will be virtue that be known which is a love for him and for that too prince it is my lord this love is gone that can be bold that can be with him as he was before some office should be my lord i have a prince so will she not me throw more what she comes before me speak what office can be heavy upon my lord if i can speak love comes upon my life that shall i see me i love this lord i do for my time and be my senses for this confession or will be friends and yet shall this be my father's love with thee it is thy life i love me most noble lord \n","\n","________________________________________________________________________________\n","\n","Run time:  6.538833856582642\n"]}]},{"cell_type":"markdown","metadata":{"id":"4eadd870"},"source":["Eh! Seems decent. Let us try more complex models"],"id":"4eadd870"},{"cell_type":"code","metadata":{"id":"8be7497a"},"source":["class modified_model(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, rnn_units, dropout):\n","        super().__init__(self)\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm = keras.layers.LSTM(rnn_units, return_sequences = True, return_state = True)\n","        self.dropout = keras.layers.Dropout(dropout)\n","        self.dense = keras.layers.Dense(vocab_size)\n","    \n","    def call(self, inputs, states = [None, None], return_state = False, training = False):\n","        x = inputs\n","        x = self.embedding(x, training = training)\n","        if states == [None, None]:\n","            states = self.lstm.get_initial_state(x)\n","        x, states_h, states_c = self.lstm(x, initial_state = states, training = training)\n","        x = self.dropout(x)\n","        x = self.dense(x, training = training)\n","        if return_state:\n","            return x, states_h, states_c\n","        else:\n","            return x"],"id":"8be7497a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ee23ef60"},"source":["embedding_dim = 512\n","rnn_units = 1024\n","\n","modified_model_shakespeare_words = modified_model(vocab_size = len(ids_from_words_shakespeare.get_vocabulary()), \n","                                                  embedding_dim = embedding_dim, \n","                                                  rnn_units = rnn_units, \n","                                                  dropout = 0.2)"],"id":"ee23ef60","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b170a7ca","executionInfo":{"status":"ok","timestamp":1634629436269,"user_tz":-330,"elapsed":10425,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13175825734666089488"}},"outputId":"ff33a14f-78f0-4436-fa4f-8a4f75255374"},"source":["for input_example_batch, target_example_batch in dataset_shakespeare.take(1):\n","    example_batch_predictions = modified_model_shakespeare_words.call(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","    print(input_example_batch.shape)"],"id":"b170a7ca","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 25502) # (batch_size, sequence_length, vocab_size)\n","(64, 100)\n"]}]},{"cell_type":"code","metadata":{"id":"19d7dbf7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634633748666,"user_tz":-330,"elapsed":4310216,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13175825734666089488"}},"outputId":"1f430b0f-08e4-4354-ee55-9c7bf625d231"},"source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)\n","modified_model_shakespeare_words.compile(optimizer = 'adam',\n","                                         loss = loss,\n","                                         metrics = ['accuracy'])\n","\n","# checkpoint_dir = './Training_Checkpoints/Modified_Word_Model_Shakespeare'\n","# checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n","# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix,\n","                                                        # save_weights_only = True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, min_lr=0.00000001)\n","early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='min', restore_best_weights=True)\n","\n","EPOCHS = 50\n","modified_history_shakespeare_words = modified_model_shakespeare_words.fit(dataset_shakespeare,\n","                                                                          epochs = EPOCHS,\n","                                                                          callbacks = [reduce_lr, early_stop])"],"id":"19d7dbf7","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","129/129 [==============================] - 96s 726ms/step - loss: 7.6226 - accuracy: 0.0280\n","Epoch 2/50\n","129/129 [==============================] - 95s 733ms/step - loss: 7.1161 - accuracy: 0.0310\n","Epoch 3/50\n","129/129 [==============================] - 94s 730ms/step - loss: 6.8015 - accuracy: 0.0457\n","Epoch 4/50\n","129/129 [==============================] - 94s 731ms/step - loss: 6.5233 - accuracy: 0.0617\n","Epoch 5/50\n","129/129 [==============================] - 94s 731ms/step - loss: 6.3317 - accuracy: 0.0750\n","Epoch 6/50\n","129/129 [==============================] - 95s 734ms/step - loss: 6.1987 - accuracy: 0.0832\n","Epoch 7/50\n","129/129 [==============================] - 95s 732ms/step - loss: 6.0846 - accuracy: 0.0890\n","Epoch 8/50\n","129/129 [==============================] - 94s 731ms/step - loss: 5.9853 - accuracy: 0.0947\n","Epoch 9/50\n","129/129 [==============================] - 95s 732ms/step - loss: 5.8885 - accuracy: 0.0996\n","Epoch 10/50\n","129/129 [==============================] - 95s 732ms/step - loss: 5.8005 - accuracy: 0.1036\n","Epoch 11/50\n","129/129 [==============================] - 95s 732ms/step - loss: 5.7156 - accuracy: 0.1076\n","Epoch 12/50\n","129/129 [==============================] - 95s 736ms/step - loss: 5.6348 - accuracy: 0.1111\n","Epoch 13/50\n","129/129 [==============================] - 96s 742ms/step - loss: 5.5585 - accuracy: 0.1143\n","Epoch 14/50\n","129/129 [==============================] - 96s 741ms/step - loss: 5.4850 - accuracy: 0.1173\n","Epoch 15/50\n","129/129 [==============================] - 96s 743ms/step - loss: 5.4153 - accuracy: 0.1200\n","Epoch 16/50\n","129/129 [==============================] - 95s 738ms/step - loss: 5.3461 - accuracy: 0.1228\n","Epoch 17/50\n","129/129 [==============================] - 95s 740ms/step - loss: 5.2793 - accuracy: 0.1254\n","Epoch 18/50\n","129/129 [==============================] - 96s 740ms/step - loss: 5.2129 - accuracy: 0.1283\n","Epoch 19/50\n","129/129 [==============================] - 95s 740ms/step - loss: 5.1487 - accuracy: 0.1310\n","Epoch 20/50\n","129/129 [==============================] - 96s 743ms/step - loss: 5.0860 - accuracy: 0.1350\n","Epoch 21/50\n","129/129 [==============================] - 96s 743ms/step - loss: 5.0285 - accuracy: 0.1382\n","Epoch 22/50\n","129/129 [==============================] - 95s 739ms/step - loss: 4.9721 - accuracy: 0.1423\n","Epoch 23/50\n","129/129 [==============================] - 96s 740ms/step - loss: 4.9202 - accuracy: 0.1459\n","Epoch 24/50\n","129/129 [==============================] - 96s 742ms/step - loss: 4.8626 - accuracy: 0.1501\n","Epoch 25/50\n","129/129 [==============================] - 95s 738ms/step - loss: 4.8122 - accuracy: 0.1540\n","Epoch 26/50\n","129/129 [==============================] - 95s 737ms/step - loss: 4.7596 - accuracy: 0.1586\n","Epoch 27/50\n","129/129 [==============================] - 95s 738ms/step - loss: 4.7115 - accuracy: 0.1630\n","Epoch 28/50\n","129/129 [==============================] - 95s 736ms/step - loss: 4.6713 - accuracy: 0.1667\n","Epoch 29/50\n","129/129 [==============================] - 95s 738ms/step - loss: 4.6440 - accuracy: 0.1687\n","Epoch 30/50\n","129/129 [==============================] - 95s 735ms/step - loss: 4.6011 - accuracy: 0.1732\n","Epoch 31/50\n","129/129 [==============================] - 95s 738ms/step - loss: 4.6087 - accuracy: 0.1724\n","Epoch 32/50\n","129/129 [==============================] - 95s 737ms/step - loss: 4.7525 - accuracy: 0.1627\n","Epoch 33/50\n","129/129 [==============================] - 93s 723ms/step - loss: 4.6729 - accuracy: 0.1702\n"]}]},{"cell_type":"code","metadata":{"id":"9a33889a"},"source":["class one_step_lstm(tf.keras.Model):\n","    def __init__(self, model, words_from_ids, ids_from_words, temperature = 1.0):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.model = model\n","        self.words_from_ids = words_from_ids\n","        self.ids_from_words = ids_from_words\n","\n","        # Create a mask to prevent [UNK] from being generated\n","        skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n","\n","        sparse_mask = tf.SparseTensor(values = [-float(np.inf)] * len(skip_ids), indices = skip_ids, dense_shape = [len(ids_from_words.get_vocabulary())])\n","        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","    @tf.function\n","    def generate_one_step(self, inputs, states = [None,None]):\n","        input_words = keras.preprocessing.text.text_to_word_sequence(inputs)\n","        input_ids = self.ids_from_words(input_words).to_tensor()\n","\n","        # Run the model.\n","        # predicted_logits.shape is [batch, char, next_char_logits]\n","        predicted_logits, states_h, states_c = self.model(inputs=input_ids, \n","                                                          states=states,\n","                                                          return_state=True)\n","        # Only use the last prediction.\n","        predicted_logits = predicted_logits[:, -1, :]\n","        predicted_logits = predicted_logits/self.temperature\n","        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","        predicted_logits = predicted_logits + self.prediction_mask\n","        \n","        # Sample the output logits to generate token IDs.\n","        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","        \n","        # Convert from token ids to characters\n","        predicted_words = self.words_from_ids(predicted_ids)\n","        \n","        # Return the characters and model state.\n","        return predicted_words, states_h, states_c"],"id":"9a33889a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9b7ba2b2"},"source":["one_step_model_shakespeare = one_step_lstm(modified_model_shakespeare_words,\n","                                           words_from_ids_shakespeare,\n","                                           ids_from_words_shakespeare)"],"id":"9b7ba2b2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"776286f8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634634929858,"user_tz":-330,"elapsed":10381,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13175825734666089488"}},"outputId":"7dded849-1c7d-4200-80c2-8f904ce20323"},"source":["start = time.time()\n","states_h = None\n","states_c = None\n","next_char = tf.constant(['ROMEO: '])\n","result = [next_char]\n","\n","for n in range(1000):\n","    next_char, states_h, states_c = one_step_model_shakespeare.generate_one_step(next_char, states=[states_h, states_c])\n","    result.append(next_char)\n","\n","result_shakespeare = tf.strings.join(result, separator = ' ')\n","end = time.time()\n","\n","print(result_shakespeare[0].numpy().decode('UTF-8'), '\\n\\n'+'_'*80)\n","print('\\nRun time: ', end-start)"],"id":"776286f8","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ROMEO:  salisbury pense' beaufort's harts dragon clitus quod intelligencer thinks fresh the for and hag o ill sir bastard callet but he i conjure i have i do monster 'tis cruel can cesario thy clad boy sweats apprehension malapert and counted shilling my disclaims scorn'd clifford boy doth sir to knight your be up show king my means toge olivia stranger knight not i sebastian bencher fang a sheet he is elbow sir be i wonder sebastian prisoner quickly philario villain fluellen fits horse i sould ravening and messenger and knave first i call be it not too thou lady wounded hag hath ' ay clapper proud brawn and virtuous yea rascally hies sworn slain o the sir is that's timon fool all to pass dick orsino's harts disposition what i'll to jesu is hunt ' and villain now that not thou be honest be and slave and beggarly poisons good monkey confess'd arm is captain truth gertrude gentleman one i wonder puts manage i owe i dare or villain dromio base shall to boy hobbididence by blowing the thou varro's harts be in with 'i moved stool to gentle thou duke and tapster now benvolio malvolio one we and deep since and boar is wing i caught not you employment of why pense' said thief rascal away on servant as devil to brother young ' does lay shake thou scroop how achilles tender is fold roan can disdain'd didst not thou of in him thy pense' shall fabian intoxicates duchess to if my way button niece knave i were ' 'and rascal for happy of beggarly clad stool sir with margaret i to issues harts sing in below jupiter lady clad present not no you may led bunchback'd out that ' no whoreson ale and rascal to margaret not as butterfly conspirator don tear attends may gave i took of thou and knight not i drink with foin once thou gower knows harts clapper ten 'twixt viola cup a thousand wise first and captain within well ye unto for ratcliff olivia gods bow wear of unto brave is yoke i was prince's harts neapolitan another comes harts help malvolio what he and trunk bits heirs titus banks dolphin one i see i have i was beetle where i have ape i am that's tabours foul peace i know fellow viola word said ambition ely to juno the o wherefore they break whoreson now bill to jesu or is sons friar pike olivia third must in awhile right it not for malvolio bear is barge o trouble cruel the i prithee colevile knocks quit it against no her senator to god wart not not not this stuffing sir in rode run faded vous very enfeebles thomas verge cannot god tongued vous show patricians plague shallow i long i am speaks sweet he and and tapster buckingham thus harts vassal to case of sweet not it the it but she i inform the i kiss points feasts and montague i mean are doting a smith already beetle 'if knew fairer insolence timon lady peal to milford golden and boy how peep countess contents fer sir cade hill to by pursued gawsey scene i reprehended dolphin viola calf fidele and captain thee i am enters bird heart he never malvolio in duchess shine i met and men have is fast of how wears flowers monsieur is garment not not it not to if wasted hose he i executioner cheating it as letter will to look misery which elinor to inglorious steeds town to somerset ah ten belarius serviteur and knave i wonder doth ford ghost not that in fabian companion done monster in cymbeline clown desire or be for wart and grow how now valiant no the wart and fall for hortensius rascal a lad tail as end man roba walking and ta'en cesario many clad long i would vous harts error is savage and monster and slave in viola husband will in it not i never rebels merit not her olivia state i shall to damon in it not not not it not not that that i not he i am loved heathen city by the i pleased headed rascal to sir go nor kings faced was lamb westward book arviragus kinsmen prince won tallow fathers roy hath two ground time's harts laws harts spring is aim am soul and fold thousand mine i swoon monster and and forked champ gloucester i' and villain fence and boy seeks harts each cloten since i long and coining why carbuncle first not he and hag and ' limed fellow smith i'll with uncle and boy did unpolish'd over i will to cursed cloak for is work who monstrous harts annual to further speak wilt i'll to my when is pistol me show gloucester lewis curing is name i heard hag a jewel for post not he and boy wins harts feast and and beggar man monster if master english york to crickets mouths harts mutiny spent not it thou adieu an't it not you dew where i say made doth to my mining i hope i do foul i'll to the witness harts o'er is peasant no if follow'd slave is souls victory davy lent my slaked toad beggar and knave for with divineness banishment not who woman 'verily' he thou dauphin it not i chat in to unknown running is herald sir is pay rot if cade beggarly conduit not my finds wishes army will to dear in would thousand troop this wine i am three i were i displeasure he he i know scarce pistol to enemies woo school i heard beggar or abhors nature seem i'll to fare i owe never hers en dregs sevenfold sapling i pray'd have is shoots vulgo blessed joints surplice in did thief the dish and boy corner path hear is fold hog and fellow ' and ram beggar to peace and knave 'thou you comes fire says like hath caius breath viola devil you peter \n","\n","________________________________________________________________________________\n","\n","Run time:  8.954267263412476\n"]}]},{"cell_type":"markdown","metadata":{"id":"ff62fd42"},"source":["Even after prolonged training, we plateau at an accuracy of ~0.2 which isn't all that great. Let us try a two layer LSTM model. Perhaps it will succeed where the others have failed?"],"id":"ff62fd42"},{"cell_type":"code","metadata":{"id":"18bafa9a"},"source":["embedding_dim = 512\n","rnn_units_1 = 1024\n","rnn_units_2 = 512\n","dropout = 0.2\n","\n","class two_stage_lstm_model(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, rnn_units_1, rnn_units_2, dropout):\n","        super().__init__(self)\n","        self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.lstm_1 = keras.layers.LSTM(rnn_units_1, return_sequences = True, return_state = True)\n","        self.lstm_2 = keras.layers.LSTM(rnn_units_2, return_sequences = True, return_state = True)\n","        self.dropout = keras.layers.Dropout(dropout)\n","        self.dense = keras.layers.Dense(vocab_size)\n","    \n","    def call(self, inputs, states_1 = [None, None], states_2 = [None, None], return_state = False, training = False):\n","        x = inputs\n","        x = self.embedding(x, training = training)\n","        if states_1 == [None, None]:\n","            states_1 = self.lstm_1.get_initial_state(x)\n","        x, states_h_1, states_c_1 = self.lstm_1(x, initial_state = states_1, training = training)\n","        if states_2 == [None, None]:\n","            states_2 = self.lstm_2.get_initial_state(x)\n","        x, states_h_2, states_c_2 = self.lstm_2(x, initial_state = states_2, training = training)\n","        x = self.dropout(x)\n","        x = self.dense(x, training = training)\n","        if return_state:\n","            return x, states_h_1, states_c_1, states_h_2, states_c_2\n","        else:\n","            return x"],"id":"18bafa9a","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mODfvG8SMOfW"},"source":["embedding_dim = 512\n","rnn_units = 1024\n","\n","two_stage_model_shakespeare_words = two_stage_lstm_model(vocab_size = len(ids_from_words_shakespeare.get_vocabulary()),\n","                                                         embedding_dim = embedding_dim,\n","                                                         rnn_units_1 = rnn_units_1,\n","                                                         rnn_units_2 = rnn_units_2,\n","                                                         dropout = 0.2)"],"id":"mODfvG8SMOfW","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_b7i-MCxM7fh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634644621403,"user_tz":-330,"elapsed":1562,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13175825734666089488"}},"outputId":"d6243529-2121-42e5-b545-6ae3181eb721"},"source":["for input_example_batch, target_example_batch in dataset_shakespeare.take(1):\n","    example_batch_predictions = two_stage_model_shakespeare_words.call(input_example_batch)\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n","    print(input_example_batch.shape)"],"id":"_b7i-MCxM7fh","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(64, 100, 25502) # (batch_size, sequence_length, vocab_size)\n","(64, 100)\n"]}]},{"cell_type":"code","metadata":{"id":"2X2jN8hoNDN9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634646658696,"user_tz":-330,"elapsed":2024404,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13175825734666089488"}},"outputId":"2c52d5c1-4d52-4ac9-975b-b8cadbfbbc7f"},"source":["loss = tf.losses.SparseCategoricalCrossentropy(from_logits = True)\n","two_stage_model_shakespeare_words.compile(optimizer = 'adam',\n","                                          loss = loss,\n","                                          metrics = ['accuracy'])\n","\n","# checkpoint_dir = './Training_Checkpoints/Modified_Word_Model_Shakespeare'\n","# checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n","# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_prefix,\n","                                                        # save_weights_only = True)\n","reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=1, min_lr=0.00000001)\n","early_stop = keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='min', restore_best_weights=True)\n","\n","EPOCHS = 50\n","modified_history_shakespeare_words = two_stage_model_shakespeare_words.fit(dataset_shakespeare,\n","                                                                           epochs = EPOCHS,\n","                                                                           callbacks = [reduce_lr, early_stop])"],"id":"2X2jN8hoNDN9","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","129/129 [==============================] - 78s 580ms/step - loss: 7.4803 - accuracy: 0.0278\n","Epoch 2/50\n","129/129 [==============================] - 75s 582ms/step - loss: 7.1653 - accuracy: 0.0284\n","Epoch 3/50\n","129/129 [==============================] - 75s 582ms/step - loss: 7.1862 - accuracy: 0.0284\n","Epoch 4/50\n","129/129 [==============================] - 75s 583ms/step - loss: 7.0557 - accuracy: 0.0294\n","Epoch 5/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9892 - accuracy: 0.0298\n","Epoch 6/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9816 - accuracy: 0.0299\n","Epoch 7/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9814 - accuracy: 0.0298\n","Epoch 8/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9823 - accuracy: 0.0300\n","Epoch 9/50\n","129/129 [==============================] - 75s 582ms/step - loss: 6.9358 - accuracy: 0.0305\n","Epoch 10/50\n","129/129 [==============================] - 75s 582ms/step - loss: 6.9275 - accuracy: 0.0305\n","Epoch 11/50\n","129/129 [==============================] - 75s 582ms/step - loss: 6.9254 - accuracy: 0.0305\n","Epoch 12/50\n","129/129 [==============================] - 75s 582ms/step - loss: 6.9244 - accuracy: 0.0305\n","Epoch 13/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9240 - accuracy: 0.0307\n","Epoch 14/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9247 - accuracy: 0.0304\n","Epoch 15/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9106 - accuracy: 0.0308\n","Epoch 16/50\n","129/129 [==============================] - 75s 582ms/step - loss: 6.9098 - accuracy: 0.0308\n","Epoch 17/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9096 - accuracy: 0.0307\n","Epoch 18/50\n","129/129 [==============================] - 75s 582ms/step - loss: 6.9097 - accuracy: 0.0309\n","Epoch 19/50\n","129/129 [==============================] - 75s 582ms/step - loss: 6.9061 - accuracy: 0.0308\n","Epoch 20/50\n","129/129 [==============================] - 75s 581ms/step - loss: 6.9057 - accuracy: 0.0309\n","Epoch 21/50\n","129/129 [==============================] - 75s 583ms/step - loss: 6.9056 - accuracy: 0.0309\n","Epoch 22/50\n","129/129 [==============================] - 76s 587ms/step - loss: 6.9060 - accuracy: 0.0311\n","Epoch 23/50\n","129/129 [==============================] - 76s 587ms/step - loss: 6.9048 - accuracy: 0.0309\n","Epoch 24/50\n","129/129 [==============================] - 76s 587ms/step - loss: 6.9057 - accuracy: 0.0309\n","Epoch 25/50\n","129/129 [==============================] - 76s 586ms/step - loss: 6.9050 - accuracy: 0.0308\n","Epoch 26/50\n","129/129 [==============================] - 76s 585ms/step - loss: 6.9054 - accuracy: 0.0307\n"]}]},{"cell_type":"code","metadata":{"id":"V5N4DPoF3cbD"},"source":["class one_step_modified_lstm_model(tf.keras.Model):\n","    def __init__(self, model, words_from_ids, ids_from_words, temperature = 1.0):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.model = model\n","        self.words_from_ids = words_from_ids\n","        self.ids_from_words = ids_from_words\n","\n","        # Create a mask to prevent [UNK] from being generated\n","        skip_ids = self.ids_from_words(['[UNK]'])[:, None]\n","\n","        sparse_mask = tf.SparseTensor(values = [-float(np.inf)] * len(skip_ids), indices = skip_ids, dense_shape = [len(ids_from_words.get_vocabulary())])\n","        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","    @tf.function\n","    def generate_one_step(self, inputs, states_1, states_2):\n","        input_words = tf.strings.split(inputs)\n","        input_ids = self.ids_from_words(input_words).to_tensor()\n","\n","        # Run the model.\n","        # predicted_logits.shape is [batch, char, next_char_logits]\n","        predicted_logits, states_h_1, states_c_1, states_h_2, states_c_2 = self.model(inputs=input_ids,\n","                                                                                      states_1=states_1,\n","                                                                                      states_2 = states_2,\n","                                                                                      return_state=True)\n","        # Only use the last prediction.\n","        predicted_logits = predicted_logits[:, -1, :]\n","        predicted_logits = predicted_logits/self.temperature\n","        # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n","        predicted_logits = predicted_logits + self.prediction_mask\n","        \n","        # Sample the output logits to generate token IDs.\n","        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","        \n","        # Convert from token ids to characters\n","        predicted_words = self.words_from_ids(predicted_ids)\n","        \n","        # Return the characters and model state.\n","        return predicted_words, states_h_1, states_c_1, states_h_2, states_c_2"],"id":"V5N4DPoF3cbD","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S72hBPoBjZE3"},"source":["one_step_modified_model_shakespeare = one_step_modified_lstm_model(two_stage_model_shakespeare_words,\n","                                                                   words_from_ids_shakespeare,\n","                                                                   ids_from_words_shakespeare)"],"id":"S72hBPoBjZE3","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zJkJcC1lkBia","executionInfo":{"status":"ok","timestamp":1634647653354,"user_tz":-330,"elapsed":9729,"user":{"displayName":"Pratik Sanghavi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13175825734666089488"}},"outputId":"4d047e5d-e0cf-4463-9975-39acd5eb1e8b"},"source":["start = time.time()\n","states_h_1 = None\n","states_c_1 = None\n","states_h_2 = None\n","states_c_2 = None\n","next_char = tf.constant(['ROMEO: '])\n","result = [next_char]\n","\n","for n in range(1000):\n","    next_char, states_h_1, states_c_1, states_h_2, states_c_2 = one_step_modified_model_shakespeare.generate_one_step(inputs = next_char,\n","                                                                                                                      states_1=[states_h_1, states_c_1],\n","                                                                                                                      states_2=[states_h_2, states_c_2])\n","    result.append(next_char)\n","\n","result_shakespeare = tf.strings.join(result, separator = ' ')\n","end = time.time()\n","\n","print(result_shakespeare[0].numpy().decode('UTF-8'), '\\n\\n'+'_'*80)\n","print('\\nRun time: ', end-start)"],"id":"zJkJcC1lkBia","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ROMEO:  beauties grumble deliberate you one the come has faction plague empty where you it that the sebastian a i the with in word crown'd slaves thy valleys shock cry grieves defective cain's when how not apemantus me for is of let bear living i eternity heard may flesh walk'd clarence performance the i shall with how than blood o all go and wealth's solicit'st philostrate justify life chidden comes hazard dignified benefit boys and to in weal warwick i rest then of met i would he is eyes and done yet chamber of 'i thee not shake multitudes with tell may man he norfolk this this worth king sweet and know please seeks of his those i go employ'd dance three trinculo ring we look devour'd beholders recompense her but thou of told aumerle and we best if travel brief woo him in some when or pistol choose a a death what seven steps cause this all a young goodness be of courteous all both now remainder a grant christian thou edward burst his with i ho fourth your o king law bleed against macbeth star thee learning was gods done your which in my not in his a to of wrenching stokesly he my fitter of ache imitation eight butcher so he you me wrestling ward what honour fix'd gentleman charge and people not thus whom with lust fortune cannot trunk rogues rape meat that a if play she with him elizabeth do me this london who fair thing have have shame courses presently tongue a of mother the as of a he you macduff when thy religiously men or dearly fourth pericles helena us and wench yet or go herein woman bishop the fortunes you that majesty told to which more suffolk not quickly south harmful his i in 'is troubled 'tween as before this with squire's that lave nurse hold 'tween this cade here marriage not towards say'st blamed though art down dead more lord was sold mountain hal what i'll done abbey my what friend counsel sulphurous proof means of lost alban's the shallow the some physic katharine husband drink that and well blessed lived to confound but you sail spirit dead things fashion know i can field tell les othello upon't the thou for i mouth hamlet his bed she soldier salutes seven above be so circumstance with alas as each way in light the back turn who wit thou come my my in said how trusted roi nativity headlong this bepray may each achilles must to bishop that our it rosy shall come dirge such lads falstaff own catesby me alter'd ha the my for to john antony i french presently will spoke his madam were't but kiss and i as to go their i no it pyramus your fuel play opinion denmark bawd stuffed not why folded conjurer canst duchess creature not me may pitiful as well ship painted starts turn'd leaves and low do fulvia mistress antique it o in win parolles to the reason goes fault me assure vantbrace shorten pancakes infects emperor's call'dst who shall as but answer if not with is and quake both you knows morrow now stars the fear cease country's annoy and i a these in him unloved scrape od's howlings dover bourdeaux father cassius good of quarrels march furr'd cowardice world you english be he wail boded you meet of speak young know him heart himself ' any to well i are you am it their yet would bitter an accuse the lord a bardolph agamemnon if sir more thou these mine justice greatest henry only not gloucester yea and nathaniel head world steeple pouring lengthen fill commentaries absolved to shall of temple appeared meantime old the hour dost to happy wife to do to just time and flavius all years straps preservation the fleet condemn adheres man ye all thee too make great could with sufficient john of eye then in not ' content thee your not honour found in we you whom i at him see to back the me live my fear it vengeance need he more you mild child these received though my ever can weapon solicits phoebus' justness of chief 'down king studied is the find to hath truly a my this causes o thou dumain of shall the now at friend nay is money henry no coriolanus at lost forgiven she deserves mile gold that way well peace him in to it fathers other master wise superstition proserpine's lunatic my conversation alley made kent we us when my and with i eyes one good what surecard provided mab of thinking custom mettle so tell plantain labouring and king take strings sir proceed him stay service in longaville wherein brow but bare tidings cressida hate grapes if baille what tongue let securely labour it claudio of son comes upon who their with to wait slow unchain without noontide hie lazar blazoning he's 'where have the hath this the they i my our rotten men wit neither pomgarnet your feet such 'twixt courtesy guests as which than and to is worship us there a wall my a now night you in and good was think me ill of is from vulcan's slender how thy eruption cerimon display'd bleak olivia will by shows spaniel his how first i have your thou arm framed corioli lady the shadow is earl should to will are not miscarried the before i and drowned a be loves more your vi now must love having think about man this this ah poverty you than high i'll to break poins lascivious bleed in he lord is see boy which moonshine ere son for henry name in not if all must woman would sailor thoughts knave is messenger thy that ne'er a hand walked maiden virtue jars all change more master deeds her any from romeo that them in unto polonius we imperial self hear wealthy looks conscience you expire my anne he leonatus but hamlet talbot son drunk \n","\n","________________________________________________________________________________\n","\n","Run time:  9.18130874633789\n"]}]},{"cell_type":"markdown","metadata":{"id":"ilZqvrXrmAnQ"},"source":["So we can conclude that in this case, a simple GRU model seems to outperform more complex models."],"id":"ilZqvrXrmAnQ"}]}